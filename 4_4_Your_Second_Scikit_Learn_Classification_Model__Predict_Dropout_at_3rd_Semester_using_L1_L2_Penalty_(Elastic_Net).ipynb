{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksuaray/Suaray25/blob/main/4_4_Your_Second_Scikit_Learn_Classification_Model__Predict_Dropout_at_3rd_Semester_using_L1_L2_Penalty_(Elastic_Net).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ],
      "metadata": {
        "id": "Hd3lWeY4he0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the final Module of this course. You've made tremendous progress so far, evolving from using data to simply describe students and campuses, to now using data to make predictions and concrete decisions. The thing is, we've just scratched the surface on\n",
        "\n",
        "- the multitude of directions we could go in interrogating our data to mine rich insights, and\n",
        "\n",
        "- the many machine learning models we could use to improve results.\n",
        "\n",
        "In this module, we'll see some ways our previous findings can deepen the questions we ask of our data, and learn two widely used enhancements to our previously applied models that will serve as the perfect segue into more advanced ML modeling."
      ],
      "metadata": {
        "id": "qypvhzGihVVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Higher Education Challenge: Enhanced Models for Deeper Questions\n",
        "\n",
        "Higher Education is full of highly skilled professionals who are driven to create environments and provide resourses to see all students succeed. Shontelle and Antonio exemplify this sentiment, and are excited about the role their new found experience with descriptive and predictive modeling can bring to their advising and policy making. They catch up over coffee to hear about the impact their statistical and machine learning models are having on their service of students.\n",
        "\n",
        "In their meeting of the minds, two pressing considerations arise. Antonio remembers a report from the Educational Opportunity Program that indicated that first generation students were not being retained at the university at the same rate as continuing generation students. Even more troubling was that it seemed the effects differed based on Race/Ethnicity. After learning about Shontelle's model, he wondered if it could be used to make predictoins to investigate this unsettling trend.\n",
        "\n",
        "Shontelle also had a suggestion for Antonio. She was impressed with how GPA predictions could be generated before the semester even began. But she also noted that if a student never steps foot on campus in that semester due to not being retained, the GPA prediction is useless. She wonders if they can use the skills learned in the certificate program to somehow use both models together.\n",
        "\n",
        "Data exploration leads to insight. But that insight leads to further interrogation, enhancing the quality of the investigation. And just as their questions are becoming more rich and nuanced, Shontelle and Antonio would like to learn more machine learning models to address these tasks.\n",
        "\n",
        "In this module we continue to cover Supervised Machine Learning. For continuous labels, we've employed Multiple Linear Regression (MLR), our Baseline Statistical Model, and for qualitative labels we utilized Logistic Regression. We saw that these models did a good job of generalizing what we learned from the training set to the test set. The question at hand: \"Is it possible to make a good thing better?\". By strategically modifying our objective function, the solutions to the MLR and logistic models evolve from the previously seen outcomes to one of three possibilities: LASSO, Rigde Regression, or Elastic Net - based on how the modification is made. In this notebook, we'll see how just four Python libraries - **Pandas**, **NumPy**, **Plotly** and **Scikit Learn** - are all you need to implement the entire Machine Learning cycle with these new methods for Higher Ed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_xabtTyJhWX_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a82b3bff-d738-453e-99ca-21e0073c6b98",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "DlSJC9VFgyk3"
      },
      "source": [
        "#2.3. Get Your Data Machine Learning Ready for Classification: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "79fc8205-a9ba-4f88-966d-da5cb44897f8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "LtgI6Ia2gyk4"
      },
      "source": [
        "##Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3a77ef1a-b114-490a-9353-fd9fbd40f41d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "g5nsYOz0gyk5"
      },
      "source": [
        "###Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "463c2878-cb35-424f-b46a-f7214c57fe33",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j4icR3fjgyk5"
      },
      "source": [
        "In Course 1, Module 3: *Magic Pandas Library: Mastering Higher Education Data Preparation and Analysis*, we learned how to merge data that originated from multiple sources accross campus. The High School, Enrollment, Admissions, Course and Completion datasets all provide valuable information to assist us in our effort to predict student metrics in future semesters. As you recall, we've selected a subset of the variables from these data to include in the modeling phase. These include:\n",
        "1. Academic Performance Data\n",
        "\n",
        "      - Available at time of admission: high school GPAs\n",
        "\n",
        "      - Available at time of modeling: units attempted, completed and DFW, and available postsecondary GPAs  \n",
        "2. Demographic Data\n",
        "      - Gender, ethnicity, first gen status\n",
        "\n",
        "3. The target variable, **SEM_2_STATUS**, a qualitative variable coded as follows:\n",
        "\n",
        "| Code | Meaning |\n",
        "|---|---|\n",
        "|C |Continuing |\n",
        "|NR |Not Registered |\n",
        "|GD |Graduated |\n",
        "\n",
        "\n",
        "Let's load the necessary Python libraries to import the data and start to process it for analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f709203d-5747-43aa-beb9-90ba96d7a5f6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7KD9znbkgyk6"
      },
      "outputs": [],
      "source": [
        "#Libraries for importing and preparing data\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d58ff1fa-7bc8-42ca-8840-047fe1c0c73a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "NjcqHx6cgyk7"
      },
      "source": [
        "Before we actually import the data into this notebook, it is important that we start things off by attending to a consideration that will affect a large number of cells in this notebook. We'll be executing quite a few commands that insert randomnes into the process, which will result in different answers every time we (and you) run this code. We can ensure *reproducibility* by setting a global seed for this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1c48cbc1-e84e-43e7-aea5-d66851592b51",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jx6gaas5gyk7"
      },
      "outputs": [],
      "source": [
        "rms = 34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "063da7bf-1f6f-461b-a465-274e22d72a2d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "yGvKFswAgyk8"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(rms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da95e43f-8fab-4c85-a982-e5fd9b780352",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6ZvdJmeCgyk8"
      },
      "source": [
        "Now let's import the **ml_data** data we've curated. Then, by typing the name we assign it, we can scope out the top and bottom 5 rows of the DataFrame and view its basic attributes in detail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9dd5eff7-f818-4626-9b96-9ec3160b6c17",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "C5OPgOrQgyk9",
        "outputId": "56388735-6444-46a0-ea3c-08cab7f43a66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_GPA</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_ATTEMPTED_3</th>\n",
              "      <th>UNITS_ATTEMPTED_4</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>UNITS_COMPLETED_3</th>\n",
              "      <th>UNITS_COMPLETED_4</th>\n",
              "      <th>DFW_UNITS_1</th>\n",
              "      <th>DFW_UNITS_2</th>\n",
              "      <th>DFW_UNITS_3</th>\n",
              "      <th>DFW_UNITS_4</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>GPA_3</th>\n",
              "      <th>GPA_4</th>\n",
              "      <th>CUM_GPA_1</th>\n",
              "      <th>CUM_GPA_2</th>\n",
              "      <th>CUM_GPA_3</th>\n",
              "      <th>CUM_GPA_4</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "      <th>SEM_3_STATUS</th>\n",
              "      <th>SEM_4_STATUS</th>\n",
              "      <th>SEM_5_STATUS</th>\n",
              "      <th>SEM_6_STATUS</th>\n",
              "      <th>SEM_7_STATUS</th>\n",
              "      <th>SEM_8_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JHPSY555D</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.83</td>\n",
              "      <td>3.97</td>\n",
              "      <td>4.09</td>\n",
              "      <td>Business</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9KC4NM2YV</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.77</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33M8O2J01</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.78</td>\n",
              "      <td>3.19</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AMX4WP4A0</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.02</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Science</td>\n",
              "      <td>13.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.615385</td>\n",
              "      <td>2.266667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.615385</td>\n",
              "      <td>2.428571</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R32ET2VTA</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.58</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16020</th>\n",
              "      <td>Q08DAI21M</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.31</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16021</th>\n",
              "      <td>IWN7KUUVJ</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.07</td>\n",
              "      <td>3.90</td>\n",
              "      <td>4.16</td>\n",
              "      <td>Arts</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16022</th>\n",
              "      <td>FODW9TFSK</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.49</td>\n",
              "      <td>3.34</td>\n",
              "      <td>3.73</td>\n",
              "      <td>Science</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16023</th>\n",
              "      <td>N7PLM1GIE</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.59</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.71</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.677419</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16024</th>\n",
              "      <td>G507U5A3M</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.80</td>\n",
              "      <td>4.29</td>\n",
              "      <td>3.97</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16025 rows × 37 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             SID     COHORT   RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  \\\n",
              "0      JHPSY555D  Fall 2023  Hispanic/Latino  Female                Unknown   \n",
              "1      9KC4NM2YV  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "2      33M8O2J01  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "3      AMX4WP4A0  Fall 2023  Hispanic/Latino  Female  Continuing Generation   \n",
              "4      R32ET2VTA  Fall 2023  Hispanic/Latino    Male  Continuing Generation   \n",
              "...          ...        ...              ...     ...                    ...   \n",
              "16020  Q08DAI21M  Fall 2021            Asian  Female  Continuing Generation   \n",
              "16021  IWN7KUUVJ  Fall 2021            White  Female  Continuing Generation   \n",
              "16022  FODW9TFSK  Fall 2021  Hispanic/Latino  Female                Unknown   \n",
              "16023  N7PLM1GIE  Fall 2021  Hispanic/Latino  Female  Continuing Generation   \n",
              "16024  G507U5A3M  Fall 2021          Unknown  Female  Continuing Generation   \n",
              "\n",
              "       HS_GPA  HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  \\\n",
              "0        3.83         3.97         4.09             Business   \n",
              "1        3.72         3.67         3.77                 Arts   \n",
              "2        2.97         2.78         3.19  University Programs   \n",
              "3        4.00         4.02         4.15              Science   \n",
              "4        3.33         3.08         3.58  University Programs   \n",
              "...       ...          ...          ...                  ...   \n",
              "16020    4.18         4.28         4.31                 Arts   \n",
              "16021    4.07         3.90         4.16                 Arts   \n",
              "16022    3.49         3.34         3.73              Science   \n",
              "16023    3.59         3.67         3.71  University Programs   \n",
              "16024    3.80         4.29         3.97  University Programs   \n",
              "\n",
              "       UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  UNITS_ATTEMPTED_3  \\\n",
              "0                   12.0                NaN                NaN   \n",
              "1                   15.0               12.0                NaN   \n",
              "2                   13.0                NaN                NaN   \n",
              "3                   13.0               15.0                NaN   \n",
              "4                    6.0                6.0                NaN   \n",
              "...                  ...                ...                ...   \n",
              "16020               15.0               16.0                NaN   \n",
              "16021               12.0                NaN                NaN   \n",
              "16022               10.0               13.0                NaN   \n",
              "16023               15.0               16.0                NaN   \n",
              "16024               12.0                NaN                NaN   \n",
              "\n",
              "       UNITS_ATTEMPTED_4  UNITS_COMPLETED_1  UNITS_COMPLETED_2  \\\n",
              "0                    NaN                6.0                NaN   \n",
              "1                    NaN                6.0                0.0   \n",
              "2                    NaN               10.0                NaN   \n",
              "3                    NaN               13.0               11.0   \n",
              "4                    NaN                3.0                3.0   \n",
              "...                  ...                ...                ...   \n",
              "16020                NaN               15.0               16.0   \n",
              "16021                NaN               12.0                NaN   \n",
              "16022                NaN                0.0                7.0   \n",
              "16023                NaN                9.0                0.0   \n",
              "16024                NaN                0.0                NaN   \n",
              "\n",
              "       UNITS_COMPLETED_3  UNITS_COMPLETED_4  DFW_UNITS_1  DFW_UNITS_2  \\\n",
              "0                    NaN                NaN          6.0          NaN   \n",
              "1                    NaN                NaN          9.0         12.0   \n",
              "2                    NaN                NaN          3.0          NaN   \n",
              "3                    NaN                NaN          0.0          4.0   \n",
              "4                    NaN                NaN          3.0          3.0   \n",
              "...                  ...                ...          ...          ...   \n",
              "16020                NaN                NaN          0.0          0.0   \n",
              "16021                NaN                NaN          0.0          NaN   \n",
              "16022                NaN                NaN         10.0          6.0   \n",
              "16023                NaN                NaN          6.0         16.0   \n",
              "16024                NaN                NaN         12.0          NaN   \n",
              "\n",
              "       DFW_UNITS_3  DFW_UNITS_4     GPA_1     GPA_2  GPA_3  GPA_4  CUM_GPA_1  \\\n",
              "0              NaN          NaN  1.666667       NaN    NaN    NaN   1.666667   \n",
              "1              NaN          NaN  2.000000  0.000000    NaN    NaN   2.000000   \n",
              "2              NaN          NaN  2.000000       NaN    NaN    NaN   2.000000   \n",
              "3              NaN          NaN  2.615385  2.266667    NaN    NaN   2.615385   \n",
              "4              NaN          NaN  1.000000  1.500000    NaN    NaN   1.000000   \n",
              "...            ...          ...       ...       ...    ...    ...        ...   \n",
              "16020          NaN          NaN  4.000000  4.000000    NaN    NaN   4.000000   \n",
              "16021          NaN          NaN  3.250000       NaN    NaN    NaN   3.250000   \n",
              "16022          NaN          NaN  0.600000  2.200000    NaN    NaN   0.600000   \n",
              "16023          NaN          NaN  1.400000  0.000000    NaN    NaN   1.400000   \n",
              "16024          NaN          NaN  0.000000       NaN    NaN    NaN   0.000000   \n",
              "\n",
              "       CUM_GPA_2  CUM_GPA_3  CUM_GPA_4 SEM_1_STATUS SEM_2_STATUS SEM_3_STATUS  \\\n",
              "0            NaN        NaN        NaN           NR           NR           NR   \n",
              "1       0.857143        NaN        NaN            C           NR           NR   \n",
              "2            NaN        NaN        NaN           NR           NR           NR   \n",
              "3       2.428571        NaN        NaN            C            C           NR   \n",
              "4       1.250000        NaN        NaN            C           NR           NR   \n",
              "...          ...        ...        ...          ...          ...          ...   \n",
              "16020   4.000000        NaN        NaN            C           NR           NR   \n",
              "16021        NaN        NaN        NaN           NR           NR           NR   \n",
              "16022   1.400000        NaN        NaN            C           NR           NR   \n",
              "16023   0.677419        NaN        NaN            C           NR           NR   \n",
              "16024        NaN        NaN        NaN           NR           NR           NR   \n",
              "\n",
              "      SEM_4_STATUS SEM_5_STATUS SEM_6_STATUS SEM_7_STATUS SEM_8_STATUS  \n",
              "0               NR           NR           NR           NR           NR  \n",
              "1               NR           NR           NR           NR           NR  \n",
              "2               NR           NR           NR           NR           NR  \n",
              "3               NR           NR           NR           NR           NR  \n",
              "4               NR           NR           NR           NR           NR  \n",
              "...            ...          ...          ...          ...          ...  \n",
              "16020           NR           NR           NR           NR           NR  \n",
              "16021           NR           NR           NR           NR           NR  \n",
              "16022           NR           NR           NR           NR           NR  \n",
              "16023           NR           NR           NR           NR           NR  \n",
              "16024           NR           NR           NR           NR           NR  \n",
              "\n",
              "[16025 rows x 37 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "source": [
        "#Let's ensure that we can view all columns of the dataframe, along with a head and tail look at the data\n",
        "pd.set_option('display.max_columns',None)\n",
        "\n",
        "retention = pd.read_csv('/Workspace/ira-ml-cert/data/ml_data.csv')\n",
        "\n",
        "retention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cc7e24ce-30f2-422f-833a-b3d128cad8bf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1yDw2rYIgyk9",
        "outputId": "1765be61-04de-4026-ac2b-5374ac8dcc16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_ATTEMPTED_3</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "      <th>SEM_3_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JHPSY555D</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33M8O2J01</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>FJ43I4I5Y</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>H4FBLM7EM</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>QQ9GAYUJV</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14412</th>\n",
              "      <td>BXXXVTVLJ</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14413</th>\n",
              "      <td>WPFBYCAF2</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14414</th>\n",
              "      <td>9DB0QFR11</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14415</th>\n",
              "      <td>F7Z5NC62T</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14416</th>\n",
              "      <td>KDYYXRZTL</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>206 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             SID     COHORT  UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  \\\n",
              "0      JHPSY555D  Fall 2023               12.0                NaN   \n",
              "2      33M8O2J01  Fall 2023               13.0                NaN   \n",
              "53     FJ43I4I5Y  Fall 2023                4.0                NaN   \n",
              "61     H4FBLM7EM  Fall 2023               16.0                NaN   \n",
              "108    QQ9GAYUJV  Fall 2023               10.0                NaN   \n",
              "...          ...        ...                ...                ...   \n",
              "14412  BXXXVTVLJ  Fall 2023               14.0                NaN   \n",
              "14413  WPFBYCAF2  Fall 2023               15.0                NaN   \n",
              "14414  9DB0QFR11  Fall 2023               14.0                NaN   \n",
              "14415  F7Z5NC62T  Fall 2023               12.0                NaN   \n",
              "14416  KDYYXRZTL  Fall 2023                6.0                NaN   \n",
              "\n",
              "       UNITS_ATTEMPTED_3 SEM_1_STATUS SEM_2_STATUS SEM_3_STATUS  \n",
              "0                    NaN           NR           NR           NR  \n",
              "2                    NaN           NR           NR           NR  \n",
              "53                   NaN           NR            C           NR  \n",
              "61                   NaN           NR           NR           NR  \n",
              "108                  NaN           NR           NR           NR  \n",
              "...                  ...          ...          ...          ...  \n",
              "14412                NaN           NR           NR           NR  \n",
              "14413                NaN           NR           NR           NR  \n",
              "14414                NaN           NR           NR           NR  \n",
              "14415                NaN           NR           NR           NR  \n",
              "14416                NaN           NR           NR           NR  \n",
              "\n",
              "[206 rows x 8 columns]"
            ]
          },
          "execution_count": 180,
          "metadata": {}
        }
      ],
      "source": [
        "retention[(retention['COHORT']=='Fall 2023') & (retention['SEM_1_STATUS']=='NR')][['SID','COHORT','UNITS_ATTEMPTED_1','UNITS_ATTEMPTED_2', 'UNITS_ATTEMPTED_3','SEM_1_STATUS','SEM_2_STATUS','SEM_3_STATUS']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3b565cc3-ea77-4dcd-a4ef-ee84702ff473",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QcIA1cxsgyk-"
      },
      "source": [
        "###Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0ab91d35-bc53-49df-8cbd-121761fc7ee7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "bQhXjv9Fgyk-"
      },
      "source": [
        "Data availability is a necessary condition for data analysis, but it is not sufficient. There are a number of modifications we need to make to the data to prepare it for machine learning. The process of preparing the data for exploration and modeling is known as **data wrangling**, and will be performed here.\n",
        "To answer Shontelle's question, we need to build a model using cohorts for which term 3 grade data has already been collected. Thus our response variable will be based on the SEM_2_STATUS variable. Let's dig deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "78516ccf-da33-438e-a37e-09fdc8de0aa1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "4SATrcNegyk-"
      },
      "source": [
        "#### Response Variable Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "87c098c7-e3f1-47a7-b3e2-92babbc002f7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "10fx1qS1gyk-"
      },
      "source": [
        "Recall that this DataFrame consists of three cohorts: Fall 2021, Fall 2022 and Fall 2023. The cohort sizes may be identified as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c8c71614-3064-4167-9bbc-23b150c26905",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "B-lzsZt9gyk_",
        "outputId": "7247966c-3e34-46b5-a211-45e1091e5d26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Fall 2023    5745\n",
              "Fall 2022    5380\n",
              "Fall 2021    4900\n",
              "Name: COHORT, dtype: int64"
            ]
          },
          "execution_count": 181,
          "metadata": {}
        }
      ],
      "source": [
        "pd.value_counts(retention['COHORT'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7a0df5e3-9040-4d89-8250-aa4d398d0d6c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "97-7GBurgyk_"
      },
      "source": [
        "The code below groups the retention DataFrame by 'COHORT' and 'SEM_2_STATUS' columns, counts the number of occurrences for each category, and resets the index, renaming the count column to 'COUNTS'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c2eb2de3-d6a8-4d10-b273-2febfc96e1fd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "maWgHKvjgyk_",
        "outputId": "f98f0c63-d154-4662-e9f0-0c0ad13ed370"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>COHORT</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "      <th>COUNTS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>C</td>\n",
              "      <td>4157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>NR</td>\n",
              "      <td>743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Fall 2022</td>\n",
              "      <td>C</td>\n",
              "      <td>4563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fall 2022</td>\n",
              "      <td>NR</td>\n",
              "      <td>817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>C</td>\n",
              "      <td>4978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>NR</td>\n",
              "      <td>767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      COHORT SEM_2_STATUS  COUNTS\n",
              "0  Fall 2021            C    4157\n",
              "1  Fall 2021           NR     743\n",
              "2  Fall 2022            C    4563\n",
              "3  Fall 2022           NR     817\n",
              "4  Fall 2023            C    4978\n",
              "5  Fall 2023           NR     767"
            ]
          },
          "execution_count": 182,
          "metadata": {}
        }
      ],
      "source": [
        "retention[['COHORT', 'SEM_2_STATUS']].groupby(['COHORT', 'SEM_2_STATUS']).size().reset_index(name='COUNTS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ab409209-83c4-4442-878c-d663edbb1f63",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "kIrEcDHwgyk_"
      },
      "source": [
        "We observe the distribution of 'SEM_3_STATUS' and determine that for Fall 2023 only the value 'NR' is observed. This means that the Fall 2023 cohort is \"unseasoned,\" meaning not enough time has elapsed since the Fall of 2023 for students to continue or graduate. This means that we should use the Fall 2021 and Fall 2022 cohorts to train our machine learning models. Then we can use the Fall 2023 cohort to make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a1e7a0d-b780-44ff-b7e3-6f41c7e44ed7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hsKg-LwZgyk_"
      },
      "source": [
        "Supervised learning cannot occur if the target is not observed. So let's determine whether any observations of the SEM_3_STATUS variable are missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b61363ac-1e2b-4800-8166-c09cf1d23742",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "vbRURxwMgylA",
        "outputId": "340056fa-613b-4030-a84b-4d25596fc811"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 183,
          "metadata": {}
        }
      ],
      "source": [
        "retention['SEM_2_STATUS'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "771b8e7d-ddc9-4eaa-9b02-9d607f016d6a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6ATz9sihgylA"
      },
      "source": [
        "No missing values, so we may proceed by investigating data quality issues in our DataFrame that could affect our analysis. These include\n",
        "\n",
        " - Rare classes in features\n",
        " - Noninformative features\n",
        " - Missing values in features\n",
        " - Class imbalance in target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fcb4d50e-392f-4eda-a668-cfe4b3d288d9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JNo5dzQAgylA"
      },
      "source": [
        "*Figure out where to move this paragraph*\n",
        "\n",
        "This is will be our primary dataframe for the ML cycle. It consists of 10,280 observations, uniquely identified by SID (as well as their row index from the original **fry** data frame), and 37 variables.\n",
        "\n",
        "Clearly the vast majority of students in our data persist through the third term. This is obviously a desired outcome, but the preponderance of retained students and scarcity of those who leave inherently makes our stated objective challenging. In the face of this challenge our task remains unchanged: use our classification model to reliably predict whether a student is a member of the minority class: those who leave the university for a variety of reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3f959f10-07f8-466f-ba7d-53300812d980",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "U7l-fbx5gylA",
        "outputId": "116d00c9-8658-4e64-a49e-dbe87f46778f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "C     13698\n",
              "NR     2327\n",
              "Name: SEM_2_STATUS, dtype: int64"
            ]
          },
          "execution_count": 184,
          "metadata": {}
        }
      ],
      "source": [
        "pd.value_counts(retention['SEM_2_STATUS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9d3c01b4-c22c-4a6a-8506-3e0077299e0b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rsklVgAUgylA"
      },
      "source": [
        "**Rare Classes in Features**\n",
        "\n",
        "Let's take a look at the distribution of values in our qualitative variables. If it turns out that there are some values that are rare, they could cause issues with our downstream data processing. One way to avoid this is to consolidate rare classes into one. Note that consolidating or dropping variables is not a reflection of their importance or relevance to the analysis; instead they highlight one of the limitations of machine learning and the importance of human oversight to create a legitimate representation of the truth.\n",
        "\n",
        "Let's investigate the class distribution for **RACE_ETHNICITY** and consolidate rare occurences into an 'Other' class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "29e3712f-043f-49aa-8e6c-56d5e0341353",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "g3DwzMmGgylA",
        "outputId": "2718ff9a-ef19-4fe1-eb2e-c60b64ec5347"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hispanic/Latino                              8407\n",
              "Asian                                        3510\n",
              "White                                        2030\n",
              "Two or More Races                             735\n",
              "Black or African American                     608\n",
              "Visa Non-U.S.                                 480\n",
              "Unknown                                       206\n",
              "Native Hawaiian or Other Pacific Islander      34\n",
              "American Indian or Alaska Native               15\n",
              "Name: RACE_ETHNICITY, dtype: int64"
            ]
          },
          "execution_count": 185,
          "metadata": {}
        }
      ],
      "source": [
        "#Distribution of classes in RACE_ETHNICITY\n",
        "pd.value_counts(retention['RACE_ETHNICITY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "987a4025-8853-47e1-bd79-26570e4184ad",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "vAtjH-gsgylB"
      },
      "source": [
        "Let's consolidate the Unknown, Native Hawaiian or Other Pacific Islander, and American Indian or Alaska Native classes into one new 'Other' class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fb974652-85f1-465d-bb75-68d11f513184",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "XYDq1CVLgylB",
        "outputId": "01a20b74-063b-43ba-d93a-aea30a34924b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hispanic/Latino              8407\n",
              "Asian                        3510\n",
              "White                        2030\n",
              "Two or More Races             735\n",
              "Black or African American     608\n",
              "Visa Non-U.S.                 480\n",
              "Other                         255\n",
              "Name: RACE_ETHNICITY, dtype: int64"
            ]
          },
          "execution_count": 186,
          "metadata": {}
        }
      ],
      "source": [
        "#Consolidating the three smallest classes into one 'Other' class\n",
        "condition = (retention['RACE_ETHNICITY'] == 'American Indian or Alaska Native') | \\\n",
        "            (retention['RACE_ETHNICITY'] == 'Native Hawaiian or Other Pacific Islander') | \\\n",
        "            (retention['RACE_ETHNICITY'] == 'Unknown')\n",
        "\n",
        "# Get the original indices where the condition is true using loc\n",
        "indices_true = retention.loc[condition].index\n",
        "\n",
        "# Update 'RACE_ETHNICITY' to 'Other' for rows with true condition\n",
        "retention.loc[indices_true, 'RACE_ETHNICITY'] = 'Other'\n",
        "\n",
        "pd.value_counts(retention['RACE_ETHNICITY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "091f025c-3a8d-451c-b922-bbf1173a4c02",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "spFDWx1zgylB"
      },
      "source": [
        "Investigating **FIRST_GEN_STATUS**, it is clear that there are no rare classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e39e30e2-a464-4a1b-bf8f-22bc5e259c7c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cUXgXBkigylB",
        "outputId": "eac94041-ad34-450d-d0e2-d43acf0c6505"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Continuing Generation    9926\n",
              "First Generation         4653\n",
              "Unknown                  1446\n",
              "Name: FIRST_GEN_STATUS, dtype: int64"
            ]
          },
          "execution_count": 187,
          "metadata": {}
        }
      ],
      "source": [
        "#Distribution of classes in FIRST_GEN_STATUS\n",
        "pd.value_counts(retention['FIRST_GEN_STATUS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "978f7b76-1a30-4ca6-a248-2f45f825c2d4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "aWwB96G8gylB"
      },
      "source": [
        "Finally, for **GENDER**, we drop the rare Non-binary class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d449b8a3-d9fb-4bbd-bb15-4c1899365d50",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6oq6R2cegylC",
        "outputId": "203d0944-33d5-4945-a1c5-a77101d9c882"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Female       9778\n",
              "Male         6192\n",
              "Nonbinary      55\n",
              "Name: GENDER, dtype: int64"
            ]
          },
          "execution_count": 188,
          "metadata": {}
        }
      ],
      "source": [
        "#Distribution of classes in GENDER\n",
        "pd.value_counts(retention['GENDER'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8fea8a9-ed93-425d-98f9-d8ecaf0aafd1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "FDJe2XhlgylC"
      },
      "outputs": [],
      "source": [
        "retention = retention[(retention['GENDER']!='Nonbinary')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "699ade66-f531-4527-a38a-c9d8b2319b5f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "gqZXCqA1gylC",
        "outputId": "ba6953df-c789-4328-95a2-3fb9605d4f0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Female    9778\n",
              "Male      6192\n",
              "Name: GENDER, dtype: int64"
            ]
          },
          "execution_count": 190,
          "metadata": {}
        }
      ],
      "source": [
        "pd.value_counts(retention['GENDER'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b1b25675-604f-4778-a387-204e1aa6e3e6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "khwN7J_PgylC"
      },
      "source": [
        "**Noninformative features**\n",
        "\n",
        "Next, let's further refine the retention DataFrame by removing the target, as well as identifier, colinear and unobservable variables. We typically utilise .drop methods, but due to the large amount of variables we'll drop, let's use indices to select columns. First let's identify the ordinal label of each variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4bb05ab4-d930-48ee-a7df-7a1d98b132e8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "LdzubCUzgylC",
        "outputId": "c19365f7-7aad-4c70-b12b-b80ebf9be703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 SID\n1 COHORT\n2 RACE_ETHNICITY\n3 GENDER\n4 FIRST_GEN_STATUS\n5 HS_GPA\n6 HS_MATH_GPA\n7 HS_ENGL_GPA\n8 COLLEGE\n9 UNITS_ATTEMPTED_1\n10 UNITS_ATTEMPTED_2\n11 UNITS_ATTEMPTED_3\n12 UNITS_ATTEMPTED_4\n13 UNITS_COMPLETED_1\n14 UNITS_COMPLETED_2\n15 UNITS_COMPLETED_3\n16 UNITS_COMPLETED_4\n17 DFW_UNITS_1\n18 DFW_UNITS_2\n19 DFW_UNITS_3\n20 DFW_UNITS_4\n21 GPA_1\n22 GPA_2\n23 GPA_3\n24 GPA_4\n25 CUM_GPA_1\n26 CUM_GPA_2\n27 CUM_GPA_3\n28 CUM_GPA_4\n29 SEM_1_STATUS\n30 SEM_2_STATUS\n31 SEM_3_STATUS\n32 SEM_4_STATUS\n33 SEM_5_STATUS\n34 SEM_6_STATUS\n35 SEM_7_STATUS\n36 SEM_8_STATUS\n"
          ]
        }
      ],
      "source": [
        "for i,j in enumerate(retention.columns):\n",
        "    print(i,j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a3fcf447-bc6b-4a01-b30c-eadab948dc8b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "l76AxRXagylD"
      },
      "source": [
        "Now we'll refer to this list to drop the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "82e706eb-a34e-4029-a6a4-dab1c6590246",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hKjp3LqzgylD",
        "outputId": "ed1dd1da-2279-4529-b851-6edac940a140"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JHPSY555D</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.97</td>\n",
              "      <td>4.09</td>\n",
              "      <td>Business</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9KC4NM2YV</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.77</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33M8O2J01</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.78</td>\n",
              "      <td>3.19</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AMX4WP4A0</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.02</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Science</td>\n",
              "      <td>13.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.615385</td>\n",
              "      <td>2.266667</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R32ET2VTA</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.58</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15965</th>\n",
              "      <td>Q08DAI21M</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.31</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15966</th>\n",
              "      <td>IWN7KUUVJ</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.90</td>\n",
              "      <td>4.16</td>\n",
              "      <td>Arts</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15967</th>\n",
              "      <td>FODW9TFSK</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.34</td>\n",
              "      <td>3.73</td>\n",
              "      <td>Science</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15968</th>\n",
              "      <td>N7PLM1GIE</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.71</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15969</th>\n",
              "      <td>G507U5A3M</td>\n",
              "      <td>Fall 2021</td>\n",
              "      <td>Other</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.29</td>\n",
              "      <td>3.97</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15970 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             SID     COHORT   RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  \\\n",
              "0      JHPSY555D  Fall 2023  Hispanic/Latino  Female                Unknown   \n",
              "1      9KC4NM2YV  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "2      33M8O2J01  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "3      AMX4WP4A0  Fall 2023  Hispanic/Latino  Female  Continuing Generation   \n",
              "4      R32ET2VTA  Fall 2023  Hispanic/Latino    Male  Continuing Generation   \n",
              "...          ...        ...              ...     ...                    ...   \n",
              "15965  Q08DAI21M  Fall 2021            Asian  Female  Continuing Generation   \n",
              "15966  IWN7KUUVJ  Fall 2021            White  Female  Continuing Generation   \n",
              "15967  FODW9TFSK  Fall 2021  Hispanic/Latino  Female                Unknown   \n",
              "15968  N7PLM1GIE  Fall 2021  Hispanic/Latino  Female  Continuing Generation   \n",
              "15969  G507U5A3M  Fall 2021            Other  Female  Continuing Generation   \n",
              "\n",
              "       HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  UNITS_ATTEMPTED_1  \\\n",
              "0             3.97         4.09             Business               12.0   \n",
              "1             3.67         3.77                 Arts               15.0   \n",
              "2             2.78         3.19  University Programs               13.0   \n",
              "3             4.02         4.15              Science               13.0   \n",
              "4             3.08         3.58  University Programs                6.0   \n",
              "...            ...          ...                  ...                ...   \n",
              "15965         4.28         4.31                 Arts               15.0   \n",
              "15966         3.90         4.16                 Arts               12.0   \n",
              "15967         3.34         3.73              Science               10.0   \n",
              "15968         3.67         3.71  University Programs               15.0   \n",
              "15969         4.29         3.97  University Programs               12.0   \n",
              "\n",
              "       UNITS_ATTEMPTED_2  UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1  \\\n",
              "0                    NaN                6.0                NaN  1.666667   \n",
              "1                   12.0                6.0                0.0  2.000000   \n",
              "2                    NaN               10.0                NaN  2.000000   \n",
              "3                   15.0               13.0               11.0  2.615385   \n",
              "4                    6.0                3.0                3.0  1.000000   \n",
              "...                  ...                ...                ...       ...   \n",
              "15965               16.0               15.0               16.0  4.000000   \n",
              "15966                NaN               12.0                NaN  3.250000   \n",
              "15967               13.0                0.0                7.0  0.600000   \n",
              "15968               16.0                9.0                0.0  1.400000   \n",
              "15969                NaN                0.0                NaN  0.000000   \n",
              "\n",
              "          GPA_2 SEM_1_STATUS SEM_2_STATUS  \n",
              "0           NaN           NR           NR  \n",
              "1      0.000000            C           NR  \n",
              "2           NaN           NR           NR  \n",
              "3      2.266667            C            C  \n",
              "4      1.500000            C           NR  \n",
              "...         ...          ...          ...  \n",
              "15965  4.000000            C           NR  \n",
              "15966       NaN           NR           NR  \n",
              "15967  2.200000            C           NR  \n",
              "15968  0.000000            C           NR  \n",
              "15969       NaN           NR           NR  \n",
              "\n",
              "[15970 rows x 16 columns]"
            ]
          },
          "execution_count": 192,
          "metadata": {}
        }
      ],
      "source": [
        "ret_columns_to_drop = [5,11,12] + [15,16] + list(range(17,21)) + list(range(23,29)) + list(range(31,37))\n",
        "\n",
        "retention_copy = retention.copy()\n",
        "\n",
        "retention2 = retention_copy.drop(retention.columns[ret_columns_to_drop], axis=1)\n",
        "\n",
        "retention2.reset_index(inplace=True,drop=True)\n",
        "\n",
        "retention2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "09639421-c61d-4c19-a3d6-fea857c3931f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "OnKtmZNOgylE"
      },
      "source": [
        "Removing minority classes and noninformative variables is an aspect of defining the entire data set that is integral to the machine learning process. In contrast, our next two challenges to be overcome should only be done *after* we seperate out our training data from the predict data, and then differentially with nuance therafter. They will be interspersed within our Data Splitting process.\n",
        "\n",
        "This code separates the retention data into a training set and a prediction set. The training set includes data from the Fall 2021 and Fall 2022 cohorts using the condition `retention[\"COHORT\"].isin([\"Fall 2021\", \"Fall 2022\"])`. The prediction set includes data from the Fall 2023 cohort using the condition `retention[\"COHORT\"].isin([\"Fall 2023\"])`. The prediction set then drops columns related to semester statuses (SEM_3_STATUS to SEM_8_STATUS) since these statuses are not observed for the prediction set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6d6a5d59-7133-47d3-9ea6-04692eafb4eb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Y8I4_G63gylE"
      },
      "outputs": [],
      "source": [
        "training = retention2[retention2[\"COHORT\"].isin([\"Fall 2021\", \"Fall 2022\"])]\n",
        "predict = retention2[retention2[\"COHORT\"].isin([\"Fall 2023\"])]\n",
        "\n",
        "#Note prediction set is strictly NR for SEM_3 and beyond, so we may drop thpse indicators\n",
        "predict = predict.drop(columns=['SEM_2_STATUS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9dd29bb5-de72-4519-ac77-8db0a27d0712",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8cfveCfDgylE"
      },
      "source": [
        "The `training` set will be our primary training dataframe for analysis. It consists of 10,245 observations, uniquely identified by SID (as well as their row index from the original **retention** data frame), and 16 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eff4f6a5-1c97-4a89-ade2-b5c5845a46af",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jmFjJyadgylE",
        "outputId": "ce6832ee-20a5-4d94-cff2-c2030d1b15de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set no. of rows 10245\n\ntraining set no. of columns 16\n"
          ]
        }
      ],
      "source": [
        "print(f'training set no. of rows {training.shape[0]}\\n')\n",
        "print(f'training set no. of columns {training.shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5278e76c-ac25-4995-8076-b6e2fb756717",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-G1WHTMzgylE"
      },
      "source": [
        "We will save the processed `training` and `predict` datasets for use in other exercises and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c7af9937-a1b1-4fb9-b63d-be8b76b44bc2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uPfBqIVDgylE"
      },
      "outputs": [],
      "source": [
        "#training.to_csv('/Workspace/ira-ml-cert/data/training.csv', index=False)\n",
        "#predict.to_csv('/Workspace/ira-ml-cert/data/predict.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0a31c94f-c956-4f2f-8d79-e66960ccf7ef",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Zn9vtRZDgylF"
      },
      "source": [
        "As mentioned in *Module 3: Explaining the Machine Learning Cycle Without Hyperparameter Tuning* we observed that a learning algorithm is only useful to the extent that we can confidently apply it to unseen data to make accurate predictions. The ability to generalize is measured by an investigation of model performance on a random sample of the full data called the test set. Before we explore or analyze our data it is imperative that we split it into a training and test set. This step will reintroduce us to Python's machine learning powerhouse, **[scikit learn](https://scikit-learn.org/stable/index.html)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0f92aad4-821e-49fa-9b71-da2b821ca454",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1jWR69lYgylF"
      },
      "source": [
        "###Data Splitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8628f416-ec65-436c-80e7-6aac3ae57b5d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "pauzeSQHgylF"
      },
      "source": [
        "######Full Data to Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ed3a3de1-6e07-4812-a01a-1028886ca309",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3_uH5557gylF"
      },
      "source": [
        "Data splitting is one of the most important steps of the machine learning cycle. We've all had instructors that, let's just say, provided alot of friendly *guidance* for what material would appear on an exam (they were pretty popular professors). Often this was in the form of a \"practice exam\". This led to a scenario where the exam was for all intents and purposes observed before exam day, and those who could memorize well were likely to achieve the most success. As much as stressed out college students might enjoy it, this arrangement does not facilitate genuine learning, which is demonstrated by the ability to accurately generalize concepts and constructs to new scenarios.  This is why we split data. So that instead of memorizing content and being tested on how well we can repeat it, we are attempting to learn the \"how\" and \"why\" behind the data generating process so that when new data comes from the process, we can legitimately demonstrate a deep level of understanding. Splitting the data into a train set an a test set, and not using the test set at all to learn patterns in the data will enable our model to demonstrate this deeper understanding. Let's load the **train_test_split** module from the scikit learn library and get our study on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e86463a0-4384-4d0e-8b82-5a266aa40372",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5RC9xPTCgylF"
      },
      "outputs": [],
      "source": [
        "#Class for data splitting\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d38e7c5b-e322-461d-b202-53973b5138f0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hBBNEODRgylF"
      },
      "source": [
        "Figure 1 displays the first step of the data splitting process: identify and isolate the feature matrix (\\\\(X)\\\\) and label vector (\\\\(y)\\\\) in the context of an easy to visualize dataframe. The figure is followed by the code that gets this process started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0969026e-3baa-4740-92a3-9b3d8085838c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "k_YeH4bpgylG"
      },
      "source": [
        "**Figure 1:** Seperating our curated DataFrame into a feature matrix \\\\((X)\\\\) and label vector \\\\((y\\\\)). An example with a DataFrame with 15 observations.\n",
        "\n",
        "\n",
        "![ih](files/ml-file-store/bronze/Xy_pic.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9e1d8d9d-d193-4099-b88f-44638fb332df",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "EylySCRegylG"
      },
      "source": [
        "Next, let's create the feature matrix by removing the target and identifier variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f6967b7a-4b70-4550-bcfc-6cebccd8ed2b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "T3vBouf5gylG"
      },
      "outputs": [],
      "source": [
        "#\n",
        "X = training.drop(['SID','COHORT','SEM_2_STATUS'],axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06472959-5d0a-428e-ae98-726f158443c0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "AqhppD-CgylG"
      },
      "source": [
        "For the target variable, we need a column in which 1 represents students who leave in semester 3, and 0 represents students who were retained. Thus we need to **one hot encode** the \"NR\" class in our target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bb0201e3-b31e-47ed-a921-31ef277c5b22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "upl4O75zgylG"
      },
      "outputs": [],
      "source": [
        "#The one hot encoding for the NR class\n",
        "y = training['SEM_2_STATUS'].apply(lambda x: 1 if x == 'NR' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1ac18ce1-9896-416b-a299-b88b78bbca96",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GEmE9s-UgylG"
      },
      "source": [
        "The initial split was a vertical one, seperating features from label. We proceed with a horizontal split, randomly holding out a specified percentage of observations for testing.\n",
        "\n",
        "Let's create an 80-20 split of the data for training, and testing on an unlearned hold out set. One of the most useful functions in scikit learn, **[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** gets the job done in one line of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dd054fa9-2b56-4b22-9127-2de5f2c89be5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WgX8Sa-sgylH"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=rms)\n",
        "\n",
        "#The random_state variable makes the code reproducible - everytime we run this code, the same observations will be allocated to the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f07e3b2-cf52-4cdb-a8b2-0a3a3b4d722e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iklPd7XIgylH"
      },
      "source": [
        "**Figure 2:** An example of an 80%-20% train-test split on a dataframe with 20 observations. Randomly sample 20% * 20 = 4 values to hold out for model testing: Observations 2,6,13 and 19.\n",
        "\n",
        "![ih](https://github.com/ksuaray/IRML---Regression-and-Classification/blob/MLCert-Sketches/MLCert%20Sketches%202/80-20-Xy.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a2d26b55-07fd-4d7f-b974-f8a830ea016b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fJIEun9ggylH"
      },
      "source": [
        "From our original 10,280 observations, 20% \\\\(\\times\\\\) 10,280 \\\\(\\approxeq\\\\) 2056 will be reserved for model testing. To prevent *data leakage*, they will not be part of our data exploration or model fitting whatsoever; we don't want to peek at the test before exam day, right?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eec51dbd-d56b-4d97-95be-c54b1e8a8b93",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RaVOppOKgylH",
        "outputId": "b1f92bbf-77a7-495d-80e1-fc8da491c891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8196, 13) (2049, 13)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8501f761-7c95-4a82-8bbe-65c325acee17",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "lBRMl7d6gylH"
      },
      "source": [
        "######Training to Build and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d386de0-b80a-439e-ab45-2baf885a77d2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fciF15lwgylH"
      },
      "source": [
        "Next we need to split our training data into a portion used to fit the data (build set), and an initially untouched part we can use to calibrate our algorithm inputs (validation set). We'll make the validation set 1/8 of the training data, resulting in a 70-10-20 build-validate-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da394684-da5e-4608-9d47-3952f16a6077",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ajfQhj8AgylI"
      },
      "outputs": [],
      "source": [
        "X_build, X_val, y_build, y_val = train_test_split(X_train,y_train,test_size=0.125,random_state=rms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7c695c78-ad3f-4d27-bfdd-2b0b357dbf53",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Gv5XVzRwgylI"
      },
      "source": [
        "We can visualize our data splitting strategy as follows:\n",
        "\n",
        "![data](https://github.com/ksuaray/IRML---Regression-and-Classification/blob/MLCert-Sketches/MLCert%20Sketches%202/4DataSets_MLReady0.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "11f0c40a-e376-4757-a007-45f342bc390e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "LgPomulIgylI"
      },
      "source": [
        "With this completed, we shift our attention to data quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "13d2ae68-7f10-406a-a07e-a8890fb1040e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RLC5z7NagylI"
      },
      "source": [
        "###Addressing Missingness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e3dc42c6-9b0d-409d-a199-a0b3e92a807f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "23Mki2FygylI"
      },
      "source": [
        "As mentioned above, an essential data preprocessing step for modeling in scikit learn is accounting for missingness in our observations. Scikit learn models will not run with missing data, so we need to decide how to deal with it.\n",
        "Let's investigate missingness in our training set, and use that to determine the most effective way to proceed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3ae9cbd7-aa9d-4cad-b323-baa6bbbf524a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cltHpMdKgylI",
        "outputId": "731889c3-b73c-4019-baec-d197119d5274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7171 entries, 14595 to 13755\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   RACE_ETHNICITY     7171 non-null   object \n 1   GENDER             7171 non-null   object \n 2   FIRST_GEN_STATUS   7171 non-null   object \n 3   HS_MATH_GPA        7147 non-null   float64\n 4   HS_ENGL_GPA        7147 non-null   float64\n 5   COLLEGE            7171 non-null   object \n 6   UNITS_ATTEMPTED_1  7169 non-null   float64\n 7   UNITS_ATTEMPTED_2  6890 non-null   float64\n 8   UNITS_COMPLETED_1  7169 non-null   float64\n 9   UNITS_COMPLETED_2  6890 non-null   float64\n 10  GPA_1              7119 non-null   float64\n 11  GPA_2              6849 non-null   float64\n 12  SEM_1_STATUS       7171 non-null   object \ndtypes: float64(8), object(5)\nmemory usage: 784.3+ KB\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1025 entries, 9350 to 9877\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   RACE_ETHNICITY     1025 non-null   object \n 1   GENDER             1025 non-null   object \n 2   FIRST_GEN_STATUS   1025 non-null   object \n 3   HS_MATH_GPA        1019 non-null   float64\n 4   HS_ENGL_GPA        1019 non-null   float64\n 5   COLLEGE            1025 non-null   object \n 6   UNITS_ATTEMPTED_1  1025 non-null   float64\n 7   UNITS_ATTEMPTED_2  987 non-null    float64\n 8   UNITS_COMPLETED_1  1025 non-null   float64\n 9   UNITS_COMPLETED_2  987 non-null    float64\n 10  GPA_1              1019 non-null   float64\n 11  GPA_2              981 non-null    float64\n 12  SEM_1_STATUS       1025 non-null   object \ndtypes: float64(8), object(5)\nmemory usage: 112.1+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 202,
          "metadata": {}
        }
      ],
      "source": [
        "X_build.info(), X_val.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "237ee699-36f7-4664-be38-e0893fb22740",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6McVzp2XgylI"
      },
      "source": [
        "It is very clear from the head-and-tail look at the data above, as well as the **.info()** output that there are an abundance of values missing from our data, coded as *NaN*. This comes as no surprise, as we recall that there are many students for whom high school data is not available. Missingness is a common challenge faced in the data preparation stage, and often need not be directly addressed for exploratory data analysis. However, in the **Analyze** phase, the methods used in the Python libraries associated with predictive modeling - statsmodels and scikit-learn - require complete data. Thus an executive decision needs to be made before we can proceed. Ultimately we have three choices:\n",
        "1. Remove all observations with missing values\n",
        "2. Replace missing values with plausible values\n",
        "3. Exclude variables with missing values from our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e0d31938-5e3d-4b0a-8965-8d16eead73f2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oxqYb1DmgylJ"
      },
      "source": [
        "For the current analysis, we choose option 1: drop incomplete observations. Our goal is to create a model that utilized data that is available for the typical domestic applicant. We can investigate missingness below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f289908b-a881-458a-8a49-9841ade38290",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8pvUTf3CgylJ",
        "outputId": "dfba650d-dd8e-4481-f83c-49b604fddb1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RACE_ETHNICITY         0\n",
              "GENDER                 0\n",
              "FIRST_GEN_STATUS       0\n",
              "HS_MATH_GPA           24\n",
              "HS_ENGL_GPA           24\n",
              "COLLEGE                0\n",
              "UNITS_ATTEMPTED_1      2\n",
              "UNITS_ATTEMPTED_2    281\n",
              "UNITS_COMPLETED_1      2\n",
              "UNITS_COMPLETED_2    281\n",
              "GPA_1                 52\n",
              "GPA_2                322\n",
              "SEM_1_STATUS           0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 203,
          "metadata": {}
        }
      ],
      "source": [
        "#The .isna() method goes column by column and indicates whether a given value is missing or not, then .sum() aggregates these indicators.\n",
        "NaN_counts = X_build.isna().sum()\n",
        "NaN_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "91ecee81-0710-4bac-92ae-b1336f6b6b51",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "tJEIu4khgylJ"
      },
      "source": [
        "Several hundred missing observations entails alot of data to drop, but it is a necessary step if we want to proceed by incorporating high school data in our model, thus avoiding option 3. above. To whatever extent possible, we should use domain knowledge or critical investigation to ascertain *why* data are missing, as this has massive implications for model bias and generalizability. The primary framework for understanding missingness has three possibilities:\n",
        "\n",
        "  1. MCAR - Missing Completely at Random - reasons for missingness are unrelated to any observed variables\n",
        "\n",
        "  2. MAR - Missing at Random - reasons for missingness in a specific variable are unrelated to that variable, and due to some other observed variable\n",
        "\n",
        "  3. MNAR - Missing not at Random - reasons for missingness in a specific variable are related directly to that variable\n",
        "\n",
        "In this scenario, it is most likely that high school is data for observations corresponding to international students, students who were homeschooled or went to a private school. As such, if we restrict our population of interest (and thus scope of our model implementation) to exclude these demographics, bias is mitigated if we drop observations with missing data.\n",
        "In addition, in anticipation of our inclusion of DFW rate, let's remove any observations with 0 units attempted in terms 1 and 2.\n",
        "To enable use in our model, we'll need to do the same with the test data (without explicitly viewing it, of course).  Let's take a look at the complete training data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c63ebbc4-3cdc-47fa-adc1-02ac214c7403",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Kg1IVlr3gylJ"
      },
      "source": [
        "**At this point I wonder if I should remove students who have missng data because they didn't make it to semester 2...**\n",
        "\n",
        "**Perhaps we should do EDA on those observations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "05bfd1c2-db5c-4fee-9287-e9a5a626bf07",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cLxHWsYmgylJ"
      },
      "outputs": [],
      "source": [
        "#Identify the rows in our data with no missing values\n",
        "complete_index_build = (X_build['HS_MATH_GPA'].notna()) & (X_build['UNITS_ATTEMPTED_1']!=0) & (X_build['UNITS_ATTEMPTED_2']!=0) & (X_build['UNITS_ATTEMPTED_1'].notna()) & (X_build['UNITS_ATTEMPTED_2'].notna()) & (X_build['GPA_1'].notna()) & (X_build['GPA_2'].notna())\n",
        "X_build_c = X_build[complete_index_build]\n",
        "y_build_c = y_build[complete_index_build]\n",
        "y_build_c.name = 'SEM_2_STATUS'\n",
        "\n",
        "complete_index_val = (X_val['HS_MATH_GPA'].notna()) & (X_val['UNITS_ATTEMPTED_1']!=0) & (X_val['UNITS_ATTEMPTED_2']!=0) & (X_val['UNITS_ATTEMPTED_1'].notna()) & (X_val['UNITS_ATTEMPTED_2'].notna()) & (X_val['GPA_1'].notna()) & (X_val['GPA_2'].notna())\n",
        "X_val_c = X_val[complete_index_val]\n",
        "y_val_c = y_val[complete_index_val]\n",
        "y_val_c.name = 'SEM_2_STATUS'\n",
        "\n",
        "complete_index_test = (X_test['HS_MATH_GPA'].notna()) & (X_test['UNITS_ATTEMPTED_1']!=0) & (X_test['UNITS_ATTEMPTED_2']!=0) & (X_test['UNITS_ATTEMPTED_1'].notna()) & (X_test['UNITS_ATTEMPTED_2'].notna()) & (X_test['GPA_1'].notna()) & (X_test['GPA_2'].notna())\n",
        "X_test_c = X_test[complete_index_test]\n",
        "y_test_c = y_test[complete_index_test]\n",
        "y_test_c.name = 'SEM_2_STATUS'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fbd0ae1f-d1d6-42a3-b49b-003a225f6e99",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "q5Ki6dXKgylJ",
        "outputId": "ac665bff-b6a8-4091-bfbb-e6615c9c1141"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RACE_ETHNICITY       0\n",
              "GENDER               0\n",
              "FIRST_GEN_STATUS     0\n",
              "HS_MATH_GPA          0\n",
              "HS_ENGL_GPA          0\n",
              "COLLEGE              0\n",
              "UNITS_ATTEMPTED_1    0\n",
              "UNITS_ATTEMPTED_2    0\n",
              "UNITS_COMPLETED_1    0\n",
              "UNITS_COMPLETED_2    0\n",
              "GPA_1                0\n",
              "GPA_2                0\n",
              "SEM_1_STATUS         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 205,
          "metadata": {}
        }
      ],
      "source": [
        "NaN_counts_c = X_build_c.isna().sum()\n",
        "NaN_counts_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "85460465-aff3-434a-afc7-489d558c09f2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "waurjdDmgylK",
        "outputId": "0d58a016-e10c-4aa1-c864-a82bee28a27c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9579</th>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.88</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5611</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>Business</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9379</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.39</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>3.454545</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.93</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15819</th>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.84</td>\n",
              "      <td>Health</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8829</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.03</td>\n",
              "      <td>4.22</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>13.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3.769231</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13809</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.84</td>\n",
              "      <td>4.08</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>2.181818</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10912</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.98</td>\n",
              "      <td>4.38</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6244</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.39</td>\n",
              "      <td>3.88</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.312500</td>\n",
              "      <td>3.375000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13755</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.05</td>\n",
              "      <td>Health</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6826 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  HS_MATH_GPA  \\\n",
              "9579             White  Female  Continuing Generation         3.72   \n",
              "5611   Hispanic/Latino    Male  Continuing Generation         3.57   \n",
              "9379             Asian    Male  Continuing Generation         2.89   \n",
              "5237   Hispanic/Latino  Female  Continuing Generation         3.63   \n",
              "15819    Visa Non-U.S.  Female                Unknown         4.26   \n",
              "...                ...     ...                    ...          ...   \n",
              "8829             Asian    Male  Continuing Generation         4.03   \n",
              "13809            Asian    Male  Continuing Generation         3.84   \n",
              "10912            Asian  Female  Continuing Generation         3.98   \n",
              "6244   Hispanic/Latino  Female  Continuing Generation         3.39   \n",
              "13755  Hispanic/Latino  Female                Unknown         3.50   \n",
              "\n",
              "       HS_ENGL_GPA       COLLEGE  UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  \\\n",
              "9579          3.88          Arts               15.0               15.0   \n",
              "5611          3.90      Business               15.0               12.0   \n",
              "9379          3.39  Liberal Arts               17.0               14.0   \n",
              "5237          3.93  Liberal Arts               16.0               16.0   \n",
              "15819         3.84        Health               13.0               14.0   \n",
              "...            ...           ...                ...                ...   \n",
              "8829          4.22   Engineering               13.0               18.0   \n",
              "13809         4.08   Engineering               15.0               14.0   \n",
              "10912         4.38  Liberal Arts               16.0               15.0   \n",
              "6244          3.88  Liberal Arts               16.0               16.0   \n",
              "13755         4.05        Health               15.0               17.0   \n",
              "\n",
              "       UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  \n",
              "9579                15.0               15.0  3.800000  3.200000            C  \n",
              "5611                15.0               12.0  3.400000  4.000000            C  \n",
              "9379                14.0               11.0  2.625000  3.454545            C  \n",
              "5237                16.0               16.0  4.000000  2.937500            C  \n",
              "15819               13.0               14.0  4.000000  2.500000            C  \n",
              "...                  ...                ...       ...       ...          ...  \n",
              "8829                13.0               18.0  3.769231  4.000000            C  \n",
              "13809               15.0               11.0  3.200000  2.181818            C  \n",
              "10912               16.0               15.0  4.000000  4.000000            C  \n",
              "6244                16.0               16.0  3.312500  3.375000            C  \n",
              "13755               15.0               17.0  3.800000  4.000000            C  \n",
              "\n",
              "[6826 rows x 13 columns]"
            ]
          },
          "execution_count": 206,
          "metadata": {}
        }
      ],
      "source": [
        "#The complete training data (features)\n",
        "X_build_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5efeb332-dcca-4824-87e3-04a3d986e412",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "famq-r-SgylK"
      },
      "source": [
        "Armed with a complete data set on our selected cohort, we're a step closer to predictive modeling. That being said, there is still a gap between having a complete dataset, and having data prepared for analysis. Let's take some steps to get us ready for that goal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da517f9b-dc85-4288-8382-99fbd20c9676",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RziJcdn3gylK"
      },
      "outputs": [],
      "source": [
        "#X_train_c.to_csv('/Workspace/ira-ml-cert/data/X_train_c.csv', index=False)\n",
        "#y_train_c.to_csv('/Workspace/ira-ml-cert/data/y_train_c.csv', index=False)\n",
        "\n",
        "#X_build_c.to_csv('/Workspace/ira-ml-cert/data/X_build_c.csv', index=False)\n",
        "#y_build_bal.to_csv('/Workspace/ira-ml-cert/data/y_build_bal.csv', index=False)\n",
        "\n",
        "#X_val_c.to_csv('/Workspace/ira-ml-cert/data/X_val_c.csv', index=False)\n",
        "#y_val_c.to_csv('/Workspace/ira-ml-cert/data/y_val_c.csv', index=False)\n",
        "\n",
        "#X_test_p1.to_csv('/Workspace/ira-ml-cert/data/X_test_p1.csv', index=False)\n",
        "#y_test_c.to_csv('/Workspace/ira-ml-cert/data/y_test_c.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6aec098b-33e1-4c07-9136-4aa5a3c1fd96",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "orTNgkL-gylK"
      },
      "source": [
        "#2.4. Get Your Data Machine Learning Ready for Classification: EDA and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1d03eca5-a54f-4e83-a0da-2c12507d44c7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fbQCY9WugylK"
      },
      "source": [
        "###Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "537f14b6-9d02-4bb9-aa27-2a52cb37634b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Ms_8Tt5egylK"
      },
      "source": [
        "The strength of statistical and machine learning models is directly connected to the nature of the underlying relationship between the features and our target variable. Indeed, our assumption is that such a relationship genuinely exists, and our main task is to approximate it as accurately as possible. Before we get there though, there are still a couple of hurdles to overcome. The methods for predictive analytics in the scikit learn library will not accept our training data *(X_build_c,y_build_c)* as currently constitued. We'll need to transform some variables to prepare them for modeling. We may also need to create new variables from old ones based on our domain knowledge to enhance our analysis. This endeavor is known as *Feature Engineering*, and is an integral part of the Preparation stage for statistical analysis and machine learning. To inform this step, let's get some visuals on our data that could lead to hidden insights or confirm common trade knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fa7f412a-2b04-4571-9ef6-84ce6ceac932",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "pKN2d5z_gylL"
      },
      "source": [
        "Recall (based on the .info() comand) that in our data we see two types of variables:\n",
        " 1. *object* (4): Qualitative - Values are strings\n",
        " 2. *float64* (23): Quantitative - Values count or measure student academic data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b065e5dd-9206-43f4-92ce-f3d3da4d19a7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9eywoxDDgylL",
        "outputId": "1be2e27b-ea9b-4e4b-c1cd-b1bccd95fed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6826 entries, 9579 to 13755\nData columns (total 13 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   RACE_ETHNICITY     6826 non-null   object \n 1   GENDER             6826 non-null   object \n 2   FIRST_GEN_STATUS   6826 non-null   object \n 3   HS_MATH_GPA        6826 non-null   float64\n 4   HS_ENGL_GPA        6826 non-null   float64\n 5   COLLEGE            6826 non-null   object \n 6   UNITS_ATTEMPTED_1  6826 non-null   float64\n 7   UNITS_ATTEMPTED_2  6826 non-null   float64\n 8   UNITS_COMPLETED_1  6826 non-null   float64\n 9   UNITS_COMPLETED_2  6826 non-null   float64\n 10  GPA_1              6826 non-null   float64\n 11  GPA_2              6826 non-null   float64\n 12  SEM_1_STATUS       6826 non-null   object \ndtypes: float64(8), object(5)\nmemory usage: 746.6+ KB\n"
          ]
        }
      ],
      "source": [
        "X_build_c.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1416ec73-5404-4c0a-89ea-dd435cab6448",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dNG91cvygylL"
      },
      "source": [
        "Variable type is an important characteristic when exploring relationships between features. Note that the methods described here call for joint exploration of features and label. The *train_test_split* function requires us to seperate them. So let's concatenate the X dataframe and y series into one pandas dataframe. We'll return to the seperated X-y version of the data later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "24f71815-885a-4a6f-85d7-ecadab9fcdfb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "FXykPTdPgylL",
        "outputId": "0cdf0b90-cf8f-481d-b45b-b13ac49b7b95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9579</th>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.88</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5611</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>Business</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9379</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.39</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>3.454545</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.93</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15819</th>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.84</td>\n",
              "      <td>Health</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8829</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.03</td>\n",
              "      <td>4.22</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>13.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3.769231</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13809</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.84</td>\n",
              "      <td>4.08</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>2.181818</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10912</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.98</td>\n",
              "      <td>4.38</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6244</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.39</td>\n",
              "      <td>3.88</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.312500</td>\n",
              "      <td>3.375000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13755</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.50</td>\n",
              "      <td>4.05</td>\n",
              "      <td>Health</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6826 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  HS_MATH_GPA  \\\n",
              "9579             White  Female  Continuing Generation         3.72   \n",
              "5611   Hispanic/Latino    Male  Continuing Generation         3.57   \n",
              "9379             Asian    Male  Continuing Generation         2.89   \n",
              "5237   Hispanic/Latino  Female  Continuing Generation         3.63   \n",
              "15819    Visa Non-U.S.  Female                Unknown         4.26   \n",
              "...                ...     ...                    ...          ...   \n",
              "8829             Asian    Male  Continuing Generation         4.03   \n",
              "13809            Asian    Male  Continuing Generation         3.84   \n",
              "10912            Asian  Female  Continuing Generation         3.98   \n",
              "6244   Hispanic/Latino  Female  Continuing Generation         3.39   \n",
              "13755  Hispanic/Latino  Female                Unknown         3.50   \n",
              "\n",
              "       HS_ENGL_GPA       COLLEGE  UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  \\\n",
              "9579          3.88          Arts               15.0               15.0   \n",
              "5611          3.90      Business               15.0               12.0   \n",
              "9379          3.39  Liberal Arts               17.0               14.0   \n",
              "5237          3.93  Liberal Arts               16.0               16.0   \n",
              "15819         3.84        Health               13.0               14.0   \n",
              "...            ...           ...                ...                ...   \n",
              "8829          4.22   Engineering               13.0               18.0   \n",
              "13809         4.08   Engineering               15.0               14.0   \n",
              "10912         4.38  Liberal Arts               16.0               15.0   \n",
              "6244          3.88  Liberal Arts               16.0               16.0   \n",
              "13755         4.05        Health               15.0               17.0   \n",
              "\n",
              "       UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  \\\n",
              "9579                15.0               15.0  3.800000  3.200000            C   \n",
              "5611                15.0               12.0  3.400000  4.000000            C   \n",
              "9379                14.0               11.0  2.625000  3.454545            C   \n",
              "5237                16.0               16.0  4.000000  2.937500            C   \n",
              "15819               13.0               14.0  4.000000  2.500000            C   \n",
              "...                  ...                ...       ...       ...          ...   \n",
              "8829                13.0               18.0  3.769231  4.000000            C   \n",
              "13809               15.0               11.0  3.200000  2.181818            C   \n",
              "10912               16.0               15.0  4.000000  4.000000            C   \n",
              "6244                16.0               16.0  3.312500  3.375000            C   \n",
              "13755               15.0               17.0  3.800000  4.000000            C   \n",
              "\n",
              "       SEM_2_STATUS  \n",
              "9579              0  \n",
              "5611              0  \n",
              "9379              0  \n",
              "5237              0  \n",
              "15819             1  \n",
              "...             ...  \n",
              "8829              0  \n",
              "13809             0  \n",
              "10912             0  \n",
              "6244              0  \n",
              "13755             0  \n",
              "\n",
              "[6826 rows x 14 columns]"
            ]
          },
          "execution_count": 209,
          "metadata": {}
        }
      ],
      "source": [
        "Xy_build_c = pd.concat([X_build_c,y_build_c],axis = 1)\n",
        "Xy_build_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c265107b-7d6f-4fcf-bf42-0825133c7276",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "40R4GMBsgylL"
      },
      "source": [
        "#####Quantitative Explanatory and Qualitative Response\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d69cc370-7e12-4945-bc36-e789a9bdddca",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "X-alzmqxgylM"
      },
      "source": [
        "Using the Xy_build_c dataframe, we can explore whether there appears to be evidence of a relationship between our quantitative features and third term retention. The *groupby* method allows us to compare and contrast the values of descriptive statistics by retention status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d709a388-d294-41d3-b635-274ed3e11de8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "4yedisJOgylM",
        "outputId": "486ad28d-7af7-4197-e341-66cd38de056e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEM_2_STATUS\n0    6052\n1     774\ndtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/spark-5e7b7ea8-cc39-44a2-9105-50/.ipykernel/2597/command-5144643411401048-2125149292:2: FutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.63173</td>\n",
              "      <td>3.879106</td>\n",
              "      <td>14.347323</td>\n",
              "      <td>13.992730</td>\n",
              "      <td>13.123926</td>\n",
              "      <td>12.506444</td>\n",
              "      <td>3.207811</td>\n",
              "      <td>3.100757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.37332</td>\n",
              "      <td>3.624251</td>\n",
              "      <td>14.174419</td>\n",
              "      <td>13.036176</td>\n",
              "      <td>7.846253</td>\n",
              "      <td>5.953488</td>\n",
              "      <td>1.862229</td>\n",
              "      <td>1.571431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              HS_MATH_GPA  HS_ENGL_GPA  UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  \\\n",
              "SEM_2_STATUS                                                                   \n",
              "0                 3.63173     3.879106          14.347323          13.992730   \n",
              "1                 3.37332     3.624251          14.174419          13.036176   \n",
              "\n",
              "              UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1     GPA_2  \n",
              "SEM_2_STATUS                                                            \n",
              "0                     13.123926          12.506444  3.207811  3.100757  \n",
              "1                      7.846253           5.953488  1.862229  1.571431  "
            ]
          },
          "execution_count": 210,
          "metadata": {}
        }
      ],
      "source": [
        "print(Xy_build_c.groupby(by='SEM_2_STATUS',).size())\n",
        "Xy_build_c.groupby(by='SEM_2_STATUS').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "51d45e02-ab4e-4891-ace6-a55335cdd173",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oshkrrg7gylM"
      },
      "source": [
        "It is clear that there seem to be differences in mean academic indicators across the academic timeline. It starts out subtly in the high school data but seems to get more pronounced from there through term 1 to term 2. Perhaps the most stark difference is in mean UNITS_COMPLETED_1 and\tUNITS_COMPLETED_2, even though UNITS_ATTEMPTED are just about the same in both terms. Surprisingly, the difference becomes even more pronounced when we consider the *medians* instead of the mean. These differences indicate that the quantitative variables could be predictive of SEM_2_STATUS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ed9680e9-66dc-4c6a-82ee-a6fb661a02d6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JTD2b8PtgylM"
      },
      "source": [
        "In addition to numerical measures, any graphical object we can use to understand one dimensional data (such as histograms, dotplots) can be plotted based on SEM_2_STATUS to visualize differences. We will be using **[plotly express](https://plotly.com/python/plotly-express/)**, a powerful module for creating interactive data visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7a095256-acad-4a77-859e-cb82b56eeded",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "AXWP19SXgylM"
      },
      "outputs": [],
      "source": [
        "#Import the plotly.express module from the plotly library\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7fb924af-eabf-4964-b04a-de9b38be4ced",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "pfcgSL9sgylN"
      },
      "source": [
        "Side-by-side boxplots are a great way to visualize the differences between the distributions of quantitative variables based on some qualitative factor. First compare semester 2 GPA for students that left in semester 3 versus those that did not. Hover over the plots to identify key descriptive statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "574b702f-1d2b-49be-a68c-d3159214aacb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PEfQbH4PgylN",
        "outputId": "26baf3d6-2f55-4946-824d-daa5f2af801d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"dcd0ca3f-c47c-4d89-b738-517f74c20a8f\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dcd0ca3f-c47c-4d89-b738-517f74c20a8f\")) {                    Plotly.newPlot(                        \"dcd0ca3f-c47c-4d89-b738-517f74c20a8f\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"SEM_2_STATUS=%{y}<br>GPA_2=%{x}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\"},\"name\":\"0\",\"notched\":false,\"offsetgroup\":\"0\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[3.2,4.0,3.4545454545454546,2.9375,3.4,0.0,4.0,2.5,1.8461538461538465,2.8125,2.6666666666666665,3.5384615384615383,2.6923076923076925,3.625,0.0,3.4,2.4285714285714284,4.0,4.0,3.5,3.375,3.588235294117647,4.0,2.25,2.0,3.25,2.6,1.0,3.7857142857142856,4.0,2.4,2.4,3.5625,0.4,3.4,3.076923076923077,3.5,2.8,1.7692307692307692,3.8125,4.0,3.0,2.5,3.25,4.0,3.4375,4.0,0.6153846153846154,1.375,4.0,3.411764705882353,2.0,2.9166666666666665,3.083333333333333,3.6,3.6,3.5384615384615383,2.769230769230769,2.6666666666666665,3.076923076923077,3.333333333333333,3.4375,2.5,3.8125,3.363636363636364,3.411764705882353,1.2857142857142858,2.75,3.230769230769231,3.4166666666666665,1.8461538461538465,4.0,3.5625,2.0,2.7333333333333334,3.5,3.5,4.0,4.0,3.75,4.0,3.6,2.7142857142857144,3.75,4.0,3.142857142857143,3.333333333333333,2.533333333333333,3.2,3.823529411764706,2.642857142857143,3.5,3.4,2.8823529411764706,3.642857142857143,3.4375,2.25,3.0,3.8,4.0,2.75,3.0,3.25,3.4,3.0625,2.769230769230769,3.533333333333333,4.0,3.8,3.75,3.6,3.6,4.0,2.4615384615384617,4.0,3.8,2.0,3.7857142857142856,2.1666666666666665,2.75,2.25,3.7142857142857135,3.076923076923077,3.625,3.0,3.55,3.8125,2.6875,0.0,3.230769230769231,2.6,2.6,3.2,3.4285714285714284,2.875,3.625,2.357142857142857,4.0,3.692307692307693,0.0,1.125,3.272727272727273,1.25,4.0,2.75,3.2,2.0,4.0,2.6,3.636363636363636,4.0,3.6,3.25,3.142857142857143,4.0,3.375,2.75,3.076923076923077,3.3125,2.875,3.8,4.0,4.0,3.0,3.75,4.0,4.0,3.5,2.0,2.642857142857143,4.0,4.0,0.5,3.5384615384615383,3.4,3.6666666666666665,4.0,1.5,3.8,0.5294117647058824,1.6875,3.7857142857142856,3.5384615384615383,2.75,3.5454545454545454,3.8125,2.923076923076923,3.4,2.25,0.5454545454545454,0.0,1.625,3.2,3.5,1.6666666666666667,2.25,3.8,3.0,2.272727272727273,4.0,4.0,3.0,3.0,2.6923076923076925,2.25,3.6666666666666665,3.5,4.0,4.0,2.076923076923077,3.4,4.0,3.25,2.923076923076923,2.857142857142857,3.769230769230769,2.6923076923076925,3.5,2.375,0.4615384615384615,3.8125,3.5,2.25,3.75,3.75,1.25,4.0,2.75,3.25,3.5,1.9285714285714288,4.0,2.6875,3.0,3.0,2.875,4.0,4.0,1.0,4.0,3.25,2.857142857142857,3.5,3.25,3.5625,3.8,0.0,3.333333333333333,3.4,3.5,3.2142857142857144,3.176470588235294,2.8461538461538463,2.25,3.7857142857142856,3.4375,3.8,3.5,2.8,4.0,4.0,2.6153846153846154,2.6,0.4615384615384615,3.8,1.5714285714285714,4.0,3.8125,3.0625,3.769230769230769,3.25,3.1538461538461537,3.25,2.5294117647058822,3.6666666666666665,3.5,3.733333333333333,0.5625,4.0,3.75,1.75,0.4615384615384615,2.6666666666666665,3.1333333333333333,3.25,1.9285714285714288,2.5,3.25,2.5,3.6,4.0,1.2,3.3846153846153846,4.0,3.5625,3.6,3.2222222222222223,3.6875,2.923076923076923,3.0,3.75,4.0,3.2857142857142856,3.076923076923077,4.0,0.0,3.2857142857142856,3.230769230769231,3.235294117647059,3.3529411764705883,3.230769230769231,2.5,4.0,4.0,1.4375,3.6666666666666665,3.571428571428572,2.647058823529412,2.0,4.0,3.6,3.583333333333333,4.0,1.5,3.8,0.9166666666666666,3.333333333333333,3.5625,4.0,2.5,0.5,3.4,4.0,3.0,3.25,4.0,4.0,4.0,4.0,3.2,2.2,3.625,3.692307692307693,1.75,3.0,4.0,2.333333333333333,3.0625,4.0,4.0,2.8461538461538463,4.0,3.75,4.0,3.5,4.0,2.2,3.6666666666666665,3.0,3.411764705882353,4.0,3.5384615384615383,2.9166666666666665,2.875,1.5,2.5,3.8125,3.8,3.857142857142857,3.727272727272727,3.25,3.5625,2.1818181818181817,1.6666666666666667,4.0,3.3076923076923075,3.0,3.625,2.75,2.625,3.5,3.411764705882353,3.0,3.8125,3.5294117647058822,4.0,4.0,1.0,3.8125,3.076923076923077,2.636363636363636,3.75,3.1538461538461537,4.0,3.625,2.769230769230769,2.0,3.1538461538461537,2.769230769230769,3.4444444444444446,3.333333333333333,3.0,4.0,4.0,3.4375,1.5384615384615383,3.25,1.1538461538461535,0.6923076923076923,2.333333333333333,2.75,4.0,3.25,2.933333333333333,2.375,4.0,3.8125,1.9230769230769231,2.6,3.0,4.0,2.769230769230769,2.833333333333333,4.0,3.0588235294117645,4.0,2.25,3.0,0.0,3.8461538461538463,0.0,3.25,3.6,2.6,3.6,4.0,3.6,3.0,4.0,3.076923076923077,4.0,4.0,1.2941176470588236,1.6153846153846154,3.4,2.333333333333333,2.8,4.0,3.8,3.7142857142857135,3.333333333333333,3.466666666666667,4.0,3.4,2.875,2.25,3.8,3.4615384615384617,3.0,2.333333333333333,2.5,2.4375,4.0,3.7777777777777777,3.6666666666666665,3.25,3.75,3.4615384615384617,3.2857142857142856,3.230769230769231,3.6666666666666665,2.076923076923077,1.2727272727272727,3.1875,3.0,4.0,3.25,2.2142857142857144,3.272727272727273,0.7857142857142857,3.0,1.1538461538461535,2.7857142857142856,3.5384615384615383,2.6,3.25,3.333333333333333,4.0,4.0,3.272727272727273,3.3846153846153846,4.0,3.375,0.0,2.75,3.0,3.769230769230769,3.8333333333333335,3.25,2.764705882352941,3.8,2.5454545454545454,4.0,3.8,4.0,3.2,2.0,4.0,3.0,3.7857142857142856,3.230769230769231,2.571428571428572,1.4,4.0,3.1333333333333333,2.25,2.857142857142857,3.3076923076923075,3.8,3.2,3.7142857142857135,3.588235294117647,3.4,3.8333333333333335,4.0,4.0,4.0,3.6,2.4375,2.5,2.727272727272727,3.7142857142857135,3.8,4.0,3.8125,4.0,3.6,0.75,2.1,3.4,2.8125,0.6923076923076923,3.6,3.0,2.642857142857143,3.0,2.5,3.0588235294117645,3.5,2.571428571428572,2.769230769230769,3.7142857142857135,3.076923076923077,3.0,2.857142857142857,2.333333333333333,3.2,1.8125,3.0,3.4615384615384617,3.2,3.823529411764706,3.933333333333333,3.25,3.3076923076923075,2.5294117647058822,4.0,2.333333333333333,2.727272727272727,3.2142857142857144,4.0,2.5,4.0,3.764705882352941,2.9285714285714284,3.5625,2.466666666666667,2.5,3.5384615384615383,4.0,4.0,3.4,3.6,3.0,2.625,2.8,4.0,3.2,1.0,3.6,3.25,3.769230769230769,3.692307692307693,3.2,3.2857142857142856,4.0,3.0,2.909090909090909,4.0,1.75,3.1538461538461537,3.4375,3.25,1.0,3.5,3.230769230769231,4.0,1.4,3.466666666666667,3.75,2.2666666666666666,2.5,2.1,3.6,3.769230769230769,2.9285714285714284,4.0,4.0,3.5,2.6153846153846154,4.0,3.3846153846153846,2.923076923076923,3.4615384615384617,1.3846153846153846,2.875,1.5,4.0,4.0,4.0,4.0,1.5,3.4,2.071428571428572,3.769230769230769,3.769230769230769,3.375,2.25,3.4285714285714284,0.4,2.75,4.0,1.6,2.076923076923077,1.9230769230769231,3.0,3.692307692307693,3.8,3.0,3.0,2.1875,3.2142857142857144,2.5,2.0,3.0,3.5,2.25,2.6,3.5294117647058822,4.0,2.9166666666666665,4.0,3.75,3.0,4.0,4.0,4.0,2.7333333333333334,3.5,1.125,2.375,1.25,4.0,2.4615384615384617,3.0,2.1538461538461537,2.8,3.8,4.0,3.4,3.25,3.6,3.6,3.727272727272727,3.25,3.75,0.375,3.5,3.0625,3.75,3.5,2.5,2.909090909090909,4.0,3.2,3.3846153846153846,3.5294117647058822,3.3125,4.0,3.6,2.5384615384615383,3.692307692307693,4.0,3.555555555555556,3.75,3.25,3.0,3.625,3.5625,0.1538461538461538,3.2,3.25,2.333333333333333,4.0,2.0,4.0,3.533333333333333,3.75,3.8125,3.8,4.0,2.0,4.0,3.0625,3.625,3.5,2.5384615384615383,3.4,4.0,3.230769230769231,3.692307692307693,3.8,3.75,4.0,2.923076923076923,1.5,2.3529411764705883,3.4285714285714284,3.7857142857142856,4.0,3.75,3.4,3.769230769230769,4.0,1.5,3.6,2.111111111111111,4.0,1.4,4.0,2.076923076923077,4.0,0.0,3.1666666666666665,3.727272727272727,3.083333333333333,3.6,3.4615384615384617,3.5,2.642857142857143,3.333333333333333,2.7142857142857144,2.4,4.0,2.333333333333333,1.25,3.6,4.0,3.4444444444444446,4.0,3.294117647058824,1.8333333333333333,4.0,2.5,4.0,3.769230769230769,4.0,3.75,3.5,1.9285714285714288,3.111111111111111,2.555555555555556,3.5384615384615383,4.0,2.0,2.6153846153846154,3.5,3.6,2.7142857142857144,3.0,3.375,3.5,3.8125,1.75,4.0,3.0,1.6666666666666667,3.076923076923077,1.1428571428571428,0.0,2.923076923076923,3.764705882352941,3.5,3.0,4.0,3.7857142857142856,3.6,3.7058823529411766,3.6,4.0,3.0,3.25,2.9411764705882355,3.4,2.5,3.0,3.357142857142857,2.4166666666666665,3.625,1.7142857142857142,3.2,2.5,2.6,3.0,2.8125,4.0,3.333333333333333,0.0,3.692307692307693,3.4375,3.769230769230769,3.7857142857142856,4.0,3.4285714285714284,2.5,2.0,1.7857142857142858,2.1538461538461537,3.1538461538461537,3.8,3.8,3.4,3.8125,4.0,2.909090909090909,4.0,2.5625,4.0,3.8461538461538463,3.625,3.4166666666666665,3.0,3.4285714285714284,2.8,0.0,4.0,2.25,3.375,3.375,3.75,3.692307692307693,3.2,3.4375,4.0,4.0,3.0,3.8,4.0,3.6,3.4166666666666665,3.764705882352941,4.0,4.0,3.272727272727273,4.0,3.357142857142857,3.769230769230769,3.8125,2.5,3.25,2.6666666666666665,2.2,4.0,3.25,3.25,4.0,3.375,4.0,2.8461538461538463,4.0,3.25,3.4,2.4615384615384617,2.25,3.142857142857143,4.0,2.25,4.0,3.2,3.75,3.25,3.7142857142857135,2.4285714285714284,3.25,3.692307692307693,2.642857142857143,2.333333333333333,3.769230769230769,3.769230769230769,2.333333333333333,2.625,3.6,3.769230769230769,4.0,2.75,3.7,3.5454545454545454,3.5384615384615383,1.0,1.3333333333333333,1.9230769230769231,2.75,0.4615384615384615,4.0,4.0,2.3076923076923075,3.769230769230769,2.076923076923077,3.0,3.727272727272727,3.076923076923077,3.5625,3.230769230769231,3.6,3.4375,2.769230769230769,2.3125,3.4285714285714284,2.0,3.692307692307693,2.933333333333333,2.230769230769231,4.0,2.357142857142857,3.7142857142857135,3.7857142857142856,1.75,2.5,2.9285714285714284,4.0,2.6666666666666665,2.923076923076923,3.6666666666666665,2.5454545454545454,3.0,2.0,1.3333333333333333,4.0,3.6,4.0,2.8461538461538463,3.3529411764705883,2.642857142857143,3.0,4.0,1.0,3.4285714285714284,3.8,1.6666666666666667,3.363636363636364,2.375,3.2,2.6,2.333333333333333,0.0,3.4615384615384617,4.0,3.2,3.5,3.75,3.3076923076923075,1.8461538461538465,2.8,3.764705882352941,3.5,3.0,2.1538461538461537,1.6923076923076923,4.0,2.4615384615384617,4.0,3.3076923076923075,2.0,3.4,0.625,4.0,3.6,3.4285714285714284,3.230769230769231,3.3125,2.3076923076923075,3.0,4.0,3.333333333333333,0.2857142857142857,3.2,2.6,3.0,3.5,2.125,2.375,3.1538461538461537,4.0,3.4375,3.3846153846153846,3.0625,3.4,2.7142857142857144,0.7272727272727273,4.0,3.733333333333333,3.25,3.0,4.0,2.769230769230769,0.0,3.8,3.2,3.8,4.0,0.9,3.4,3.333333333333333,3.4,2.3076923076923075,1.9230769230769231,2.909090909090909,1.4,2.6153846153846154,3.5,2.875,3.615384615384616,3.0,4.0,3.823529411764706,4.0,3.4,3.25,2.8,3.142857142857143,3.5,2.0,3.5,3.6,2.6153846153846154,2.0,3.4375,3.7857142857142856,2.25,3.4375,3.7,2.6923076923076925,3.8125,2.833333333333333,4.0,3.4375,3.125,3.4,3.25,2.9285714285714284,3.6,3.7142857142857135,4.0,3.4,2.8,3.8125,3.0,3.4166666666666665,3.2,3.5,3.8333333333333335,2.25,3.625,4.0,3.6,3.5,4.0,3.5,2.923076923076923,3.25,4.0,4.0,3.0,2.25,2.272727272727273,3.769230769230769,3.2,3.692307692307693,3.8,2.25,3.8,3.5384615384615383,3.769230769230769,2.4285714285714284,3.5,4.0,3.692307692307693,1.6428571428571428,3.4,4.0,3.875,2.0,4.0,2.230769230769231,3.727272727272727,4.0,3.2,1.125,4.0,2.6,2.6875,3.4,2.4,0.0,3.0,4.0,4.0,3.2,2.333333333333333,2.5,3.6,1.0,3.3076923076923075,4.0,4.0,3.411764705882353,2.375,3.769230769230769,2.9166666666666665,3.333333333333333,3.25,2.909090909090909,3.625,3.2,2.0,3.2142857142857144,3.8,3.6,3.0,2.357142857142857,3.5,2.0,3.25,4.0,1.8571428571428568,3.75,0.3333333333333333,3.076923076923077,3.7142857142857135,3.2,4.0,4.0,1.25,2.0,4.0,2.083333333333333,4.0,3.2,2.0,4.0,3.764705882352941,3.076923076923077,2.8461538461538463,3.769230769230769,1.5454545454545454,2.2,2.6153846153846154,1.0,3.2142857142857144,3.8333333333333335,4.0,3.3846153846153846,4.0,3.4,3.2,3.0588235294117645,3.636363636363636,4.0,4.0,2.909090909090909,3.0,0.0,3.0,4.0,4.0,2.6,3.3125,3.7857142857142856,3.0,0.0,3.8,3.5384615384615383,3.3,1.7058823529411764,4.0,4.0,3.6,0.0,3.0,3.75,4.0,3.333333333333333,3.5,4.0,2.230769230769231,1.0,2.3846153846153846,3.8125,3.1333333333333333,3.6,2.3846153846153846,4.0,2.6,4.0,2.230769230769231,2.769230769230769,3.0,4.0,3.5384615384615383,4.0,3.8125,3.8125,4.0,3.7857142857142856,3.6,3.5625,2.0,3.076923076923077,3.2,3.75,3.8,3.357142857142857,3.692307692307693,2.4285714285714284,0.25,3.0,3.2142857142857144,4.0,3.75,3.25,3.5625,1.75,2.833333333333333,3.727272727272727,4.0,3.1875,3.333333333333333,3.4,2.769230769230769,2.0,2.647058823529412,4.0,3.769230769230769,2.6666666666666665,3.0,4.0,3.764705882352941,3.230769230769231,2.4615384615384617,3.0,3.0,2.0,3.2,3.533333333333333,2.411764705882353,3.5,3.692307692307693,3.0,2.5,4.0,0.6428571428571429,3.333333333333333,2.7857142857142856,4.0,2.25,3.363636363636364,2.2,4.0,3.9375,1.0,3.2666666666666666,3.25,3.5384615384615383,2.5384615384615383,2.833333333333333,4.0,3.0,3.5,3.823529411764706,2.333333333333333,3.0,3.769230769230769,2.4,4.0,3.1538461538461537,4.0,3.8125,3.4,4.0,0.25,4.0,4.0,2.6923076923076925,1.2727272727272727,3.7142857142857135,4.0,2.5,3.0625,1.25,3.6666666666666665,4.0,3.5,3.4615384615384617,3.4375,4.0,4.0,3.75,2.625,3.5,2.75,4.0,2.357142857142857,4.0,4.0,3.769230769230769,2.769230769230769,1.8,3.4,3.6666666666666665,4.0,4.0,1.75,1.6666666666666667,4.0,2.923076923076923,2.083333333333333,3.2,3.076923076923077,2.25,2.3076923076923075,4.0,4.0,3.5,3.636363636363636,1.9285714285714288,3.0,2.8666666666666667,0.9230769230769232,4.0,4.0,4.0,3.5384615384615383,4.0,3.4285714285714284,3.769230769230769,0.0,1.9,3.8,3.4375,3.25,2.375,3.75,3.8,3.8,4.0,3.6,3.692307692307693,3.076923076923077,4.0,3.0,2.0,2.6666666666666665,3.0,3.25,3.4,4.0,3.5384615384615383,1.25,1.4166666666666667,1.2,4.0,0.0,3.076923076923077,3.333333333333333,3.25,2.9166666666666665,2.4166666666666665,2.0,4.0,2.5,1.5384615384615383,1.8461538461538465,3.0,3.692307692307693,3.2857142857142856,4.0,3.0,3.571428571428572,2.25,3.3125,2.5625,1.8666666666666667,2.9375,2.5384615384615383,3.230769230769231,3.769230769230769,3.8,3.7142857142857135,3.3076923076923075,3.3846153846153846,3.25,3.0625,2.5,3.25,3.25,4.0,4.0,3.615384615384616,4.0,3.0,3.733333333333333,3.8,3.8333333333333335,3.5625,2.3846153846153846,0.6,3.6,3.7857142857142856,3.2,3.466666666666667,4.0,2.076923076923077,4.0,3.2142857142857144,1.0,4.0,3.5,3.4285714285714284,3.8,2.25,1.2727272727272727,3.8,3.8125,3.25,3.5,0.5,4.0,3.3529411764705883,4.0,3.3076923076923075,2.6666666666666665,2.6923076923076925,2.4,2.6923076923076925,2.0,3.0,3.0,3.75,4.0,4.0,3.75,1.8125,0.0,4.0,3.8,2.6666666666666665,3.076923076923077,4.0,3.4166666666666665,3.8125,3.75,3.3076923076923075,3.25,3.230769230769231,2.769230769230769,3.333333333333333,3.8,2.636363636363636,2.75,3.8125,2.0,0.5,2.4,4.0,3.2,3.7857142857142856,2.9,3.0625,2.8125,4.0,2.8181818181818183,2.9166666666666665,1.5,3.533333333333333,3.25,2.230769230769231,1.5,3.357142857142857,3.75,2.0,4.0,4.0,0.0,3.5,3.1875,0.0,3.0,3.357142857142857,3.4,2.923076923076923,2.8461538461538463,3.8125,0.3333333333333333,3.6,2.4,2.875,3.5,3.0,1.6666666666666667,3.5,3.230769230769231,1.5714285714285714,2.071428571428572,2.833333333333333,3.4,3.5,3.8125,2.25,4.0,3.2,1.9230769230769231,3.230769230769231,2.076923076923077,3.7142857142857135,3.4615384615384617,2.6,3.75,3.0,2.5,2.9375,4.0,2.8,3.5384615384615383,3.0,3.0,3.7857142857142856,3.0,2.7142857142857144,2.8,3.6666666666666665,0.3333333333333333,4.0,3.625,3.4,1.5,3.0625,3.4285714285714284,3.5,3.4375,4.0,3.0,0.0,0.6666666666666666,3.769230769230769,2.7857142857142856,3.25,1.6666666666666667,2.142857142857143,4.0,3.25,2.4285714285714284,3.6666666666666665,2.333333333333333,3.571428571428572,3.933333333333333,3.636363636363636,3.75,0.6923076923076923,2.4615384615384617,4.0,2.5625,4.0,4.0,3.6666666666666665,3.1538461538461537,3.0625,2.4,2.25,3.6,3.5,4.0,4.0,4.0,2.1538461538461537,3.8333333333333335,4.0,4.0,0.0,3.333333333333333,3.8,2.0,3.0,3.875,2.75,2.333333333333333,4.0,3.076923076923077,4.0,1.5,2.4285714285714284,3.2857142857142856,4.0,3.333333333333333,4.0,3.0625,0.4285714285714285,2.75,3.4615384615384617,3.8,3.7142857142857135,2.4285714285714284,3.8125,3.357142857142857,4.0,3.0,3.8333333333333335,2.25,3.5,3.0,3.5,3.5384615384615383,2.25,4.0,3.2,4.0,3.4615384615384617,3.0,3.8,2.4,3.8,3.0,3.4285714285714284,4.0,3.5384615384615383,2.7857142857142856,2.75,3.3846153846153846,2.1875,3.5,3.0,4.0,4.0,4.0,3.4615384615384617,3.8,3.6,3.8333333333333335,4.0,3.6,4.0,3.6666666666666665,3.230769230769231,3.0,3.083333333333333,3.25,1.6875,4.0,3.3846153846153846,2.5,2.4615384615384617,3.8125,3.25,2.25,3.0,2.3846153846153846,3.5,4.0,3.2,3.5625,4.0,3.25,3.6,3.375,2.6666666666666665,4.0,3.75,3.75,4.0,3.0,4.0,2.6666666666666665,3.8,3.647058823529412,3.4,2.2,3.8,3.6,3.6,4.0,4.0,0.0,3.3076923076923075,3.4285714285714284,3.4,3.0,3.769230769230769,4.0,3.5625,3.4166666666666665,3.0,2.5,1.2857142857142858,4.0,2.6923076923076925,2.909090909090909,3.7,0.6666666666666666,2.4285714285714284,3.333333333333333,3.647058823529412,2.0,2.333333333333333,2.5,3.5,2.5,0.0,3.1333333333333333,3.625,3.388888888888889,3.4375,4.0,3.4,4.0,3.176470588235294,4.0,3.571428571428572,4.0,3.692307692307693,3.8125,3.823529411764706,2.3076923076923075,3.4,3.5,4.0,2.5384615384615383,4.0,3.5,2.75,3.692307692307693,3.8,2.6153846153846154,3.4,0.5,4.0,3.8125,3.0,3.5384615384615383,3.8,2.3846153846153846,2.9166666666666665,3.333333333333333,3.071428571428572,2.4615384615384617,2.375,3.055555555555556,2.466666666666667,3.066666666666667,3.526315789473684,0.9166666666666666,3.6,2.875,4.0,4.0,1.25,3.733333333333333,3.636363636363636,2.727272727272727,3.6666666666666665,4.0,3.25,3.692307692307693,4.0,4.0,3.1538461538461537,1.4,3.0,3.75,2.625,3.230769230769231,1.6,2.75,3.6666666666666665,3.4,2.3,3.6,3.4,4.0,2.4285714285714284,2.533333333333333,4.0,3.625,1.3333333333333333,3.375,2.857142857142857,3.533333333333333,2.933333333333333,2.066666666666667,4.0,2.75,3.6666666666666665,2.769230769230769,4.0,3.25,4.0,3.3125,2.230769230769231,2.6666666666666665,0.2307692307692307,3.25,2.4,2.8461538461538463,4.0,3.3076923076923075,3.733333333333333,0.0,2.333333333333333,3.6,2.5,1.625,4.0,3.076923076923077,3.588235294117647,4.0,4.0,2.8,3.692307692307693,3.0,2.0625,2.8461538461538463,2.923076923076923,2.6666666666666665,0.0,3.25,3.0,2.888888888888889,2.25,4.0,3.692307692307693,3.1538461538461537,2.857142857142857,3.25,3.5,3.615384615384616,4.0,4.0,3.6666666666666665,2.636363636363636,2.8,3.769230769230769,3.8125,2.8,4.0,3.769230769230769,3.2,4.0,1.8461538461538465,3.4,2.230769230769231,4.0,2.0,3.4,3.2666666666666666,3.2,3.6,3.333333333333333,3.5,3.571428571428572,2.533333333333333,3.75,0.0,3.5,3.2222222222222223,2.25,0.8125,3.363636363636364,4.0,3.0,3.823529411764706,2.636363636363636,1.4615384615384617,2.4615384615384617,3.684210526315789,3.0,3.647058823529412,3.4285714285714284,3.0,3.4375,1.25,2.230769230769231,2.727272727272727,2.6,1.5,2.8666666666666667,2.230769230769231,3.769230769230769,3.4,3.375,1.5,1.25,3.1875,3.4,4.0,2.875,3.1875,3.5,3.5384615384615383,3.7777777777777777,3.6666666666666665,2.230769230769231,4.0,3.8,3.8125,1.5,2.769230769230769,3.2,2.636363636363636,2.1538461538461537,3.8125,3.642857142857143,1.3846153846153846,4.0,4.0,3.2777777777777777,2.0,0.8,3.769230769230769,0.6363636363636364,3.2,4.0,4.0,2.230769230769231,3.0,3.823529411764706,3.2857142857142856,2.0,2.769230769230769,4.0,3.0,0.4,3.2,1.75,3.0,2.0,3.8,3.230769230769231,2.8461538461538463,4.0,3.5,3.647058823529412,1.6428571428571428,3.4,2.25,3.4375,2.4166666666666665,0.0,4.0,3.8333333333333335,1.4,2.75,3.0,2.75,3.6,4.0,2.0,3.0,3.8125,4.0,3.0,3.615384615384616,3.5,0.0,1.3076923076923077,3.411764705882353,2.25,3.5,2.4285714285714284,4.0,2.2,3.5,4.0,3.75,4.0,2.75,2.8,4.0,2.2,3.3846153846153846,3.4285714285714284,3.75,3.6,2.5625,3.533333333333333,2.6153846153846154,3.769230769230769,3.5,4.0,4.0,4.0,2.090909090909091,3.1875,2.6923076923076925,2.6923076923076925,3.5,2.923076923076923,3.4,3.8,4.0,3.733333333333333,3.615384615384616,3.6,3.8,2.4285714285714284,4.0,3.6,2.230769230769231,4.0,3.0,3.8,3.75,4.0,3.769230769230769,1.4,4.0,3.0,3.692307692307693,3.636363636363636,1.0,3.0,3.0625,3.2857142857142856,3.4375,3.4705882352941178,4.0,2.625,3.6,3.5625,3.8666666666666663,3.3846153846153846,3.823529411764706,3.5,3.0,2.5,4.0,4.0,3.2142857142857144,3.25,3.2142857142857144,3.0,3.0,2.571428571428572,4.0,4.0,3.5454545454545454,3.8,3.4285714285714284,3.2,0.8571428571428571,3.5,3.8333333333333335,3.0,3.0625,2.769230769230769,4.0,3.25,4.0,3.0,2.5625,3.4,2.5384615384615383,3.8,4.0,3.75,2.875,2.3,4.0,3.2142857142857144,3.0,4.0,3.2666666666666666,3.25,2.375,3.8125,3.5625,3.4285714285714284,2.0,2.230769230769231,1.5,3.636363636363636,2.909090909090909,3.5,4.0,2.75,4.0,4.0,0.25,3.25,2.0,0.0,2.5,0.0,2.75,3.6,4.0,3.6,3.5384615384615383,4.0,2.5,4.0,4.0,2.375,3.142857142857143,3.25,2.5,4.0,4.0,3.411764705882353,3.8125,3.375,2.4615384615384617,4.0,3.8,3.375,3.25,4.0,3.7142857142857135,4.0,3.5384615384615383,2.9285714285714284,3.25,3.125,3.692307692307693,3.6,2.6153846153846154,3.3846153846153846,3.636363636363636,3.2857142857142856,3.1666666666666665,4.0,3.25,4.0,2.4,3.8333333333333335,4.0,3.272727272727273,3.0,3.333333333333333,3.25,3.6,3.25,0.4,3.8,3.0,3.5,3.4285714285714284,3.176470588235294,3.0,3.571428571428572,2.4,3.75,3.5,2.923076923076923,3.0,2.7857142857142856,3.0,2.076923076923077,3.6,2.4,0.0,1.125,3.8125,3.0625,4.0,3.1875,2.6666666666666665,4.0,4.0,3.625,2.0,4.0,4.0,4.0,0.5,3.6666666666666665,4.0,3.4375,3.75,0.4285714285714285,0.7692307692307693,0.75,4.0,3.6,3.25,3.9375,3.5,2.6923076923076925,3.1875,2.75,3.4,2.1538461538461537,2.7,0.1,3.0,3.7142857142857135,3.6,3.1538461538461537,2.769230769230769,3.333333333333333,4.0,4.0,3.692307692307693,3.0,2.6666666666666665,2.625,3.571428571428572,3.692307692307693,3.6666666666666665,3.8125,3.25,3.230769230769231,3.692307692307693,3.1875,1.8,4.0,4.0,3.25,3.0,2.0,3.75,2.8,4.0,3.375,3.625,3.3529411764705883,3.6,4.0,3.823529411764706,0.4285714285714285,4.0,2.6923076923076925,4.0,3.8666666666666663,3.7142857142857135,2.2,2.75,3.5,2.4615384615384617,2.7777777777777777,4.0,3.4,3.230769230769231,3.8,4.0,2.75,3.7857142857142856,3.0,4.0,3.769230769230769,3.2857142857142856,0.625,3.733333333333333,4.0,0.0,3.7142857142857135,3.4,3.083333333333333,4.0,3.7857142857142856,3.2,4.0,4.0,3.0,4.0,3.0,4.0,4.0,4.0,3.1875,2.6666666666666665,4.0,3.076923076923077,1.9090909090909087,3.375,4.0,3.2666666666666666,3.5,2.75,3.375,3.4,4.0,3.6,2.875,2.2,2.4615384615384617,3.0,4.0,3.571428571428572,4.0,3.083333333333333,3.8,2.833333333333333,3.5384615384615383,2.357142857142857,2.2222222222222223,3.0,3.2857142857142856,3.2,3.0,4.0,3.6,2.0,4.0,3.8125,3.125,1.9230769230769231,2.6666666666666665,3.5,4.0,2.5625,3.5,2.923076923076923,4.0,2.8,3.0588235294117645,2.7857142857142856,3.625,3.25,3.2857142857142856,3.272727272727273,4.0,4.0,4.0,2.5,3.5,1.5294117647058822,3.2142857142857144,2.5384615384615383,3.4375,3.333333333333333,4.0,3.8125,3.3076923076923075,3.8125,3.625,2.25,3.625,3.769230769230769,3.0,3.4375,3.0,3.25,3.3125,2.076923076923077,2.888888888888889,3.75,4.0,2.4,2.625,3.0,3.769230769230769,0.7647058823529411,3.8,3.5384615384615383,4.0,3.5625,3.076923076923077,0.5384615384615384,3.0,3.4,1.6875,2.2,3.375,2.2,2.5,3.3125,0.8333333333333334,3.8125,2.6,3.3076923076923075,4.0,2.5,3.571428571428572,3.0,2.230769230769231,2.6,3.8,3.0,3.3076923076923075,3.25,3.692307692307693,2.083333333333333,3.0,2.857142857142857,2.75,2.727272727272727,4.0,0.0,3.2,3.0625,3.8,3.25,2.3,1.0,3.625,2.9285714285714284,2.333333333333333,4.0,1.5,2.6923076923076925,4.0,3.0,3.076923076923077,3.8125,3.0,3.0,2.0,3.8,4.0,3.4285714285714284,1.5,3.333333333333333,3.8125,4.0,3.25,2.0,3.2,3.5384615384615383,4.0,3.7857142857142856,4.0,2.642857142857143,1.5,3.5294117647058822,3.071428571428572,3.0,4.0,3.333333333333333,2.5,4.0,4.0,4.0,3.0,3.4,3.823529411764706,4.0,3.3076923076923075,3.75,3.8125,4.0,4.0,3.0,3.0,4.0,3.5,3.6666666666666665,3.75,3.5,3.0,2.4,2.8125,3.5,3.5,3.764705882352941,0.9333333333333332,3.6666666666666665,3.4,3.8,4.0,3.75,3.0,3.25,3.6666666666666665,2.0,2.5625,4.0,2.636363636363636,3.75,3.6,3.0,3.411764705882353,3.4615384615384617,3.0,3.6,2.75,2.8,3.6666666666666665,3.8,3.8125,3.25,3.3076923076923075,2.764705882352941,3.5,4.0,2.6666666666666665,4.0,3.8,3.6,3.636363636363636,3.5,2.625,2.083333333333333,2.0,3.75,4.0,3.2857142857142856,3.411764705882353,3.3076923076923075,4.0,3.25,4.0,2.923076923076923,3.230769230769231,0.3333333333333333,3.4,3.0,3.75,3.8,2.9166666666666665,3.4285714285714284,3.375,4.0,3.6666666666666665,2.4,3.25,3.3,0.875,3.2857142857142856,3.1875,4.0,0.3333333333333333,3.357142857142857,3.25,3.625,3.6,2.75,3.7,3.0,3.636363636363636,3.5,3.4,0.75,4.0,3.0,4.0,3.5,2.4,2.75,1.3333333333333333,1.0,3.8125,2.4,3.0,2.3125,4.0,2.333333333333333,3.7142857142857135,3.0,4.0,3.071428571428572,3.4,2.8125,2.7142857142857144,2.7333333333333334,3.8,3.5,4.0,1.6666666666666667,3.75,3.4,3.2,3.4375,3.8,3.230769230769231,3.7142857142857135,3.6,2.2,3.4285714285714284,2.6666666666666665,3.5,3.8,3.25,4.0,4.0,1.5294117647058822,3.0,2.75,4.0,4.0,2.333333333333333,3.333333333333333,3.8,2.0,3.5,2.875,3.1666666666666665,2.8,4.0,2.6666666666666665,2.923076923076923,3.5384615384615383,3.769230769230769,2.4,4.0,3.0,1.4,3.0,1.5,3.625,4.0,4.0,2.0,4.0,2.6666666666666665,1.0625,4.0,2.833333333333333,2.6875,3.6,1.75,3.8125,2.6666666666666665,4.0,1.4285714285714286,4.0,0.2307692307692307,2.8,3.8,3.071428571428572,4.0,3.4285714285714284,2.5384615384615383,4.0,1.1428571428571428,0.7692307692307693,1.0,2.4285714285714284,3.0,2.857142857142857,3.3846153846153846,4.0,3.727272727272727,4.0,4.0,3.5,2.3125,2.0,4.0,3.375,3.1538461538461537,4.0,4.0,3.5,4.0,1.5,2.7142857142857144,2.333333333333333,3.25,2.0,3.636363636363636,2.6153846153846154,0.75,2.833333333333333,3.4444444444444446,2.6153846153846154,3.75,4.0,0.0,2.0,3.692307692307693,2.5384615384615383,1.6666666666666667,1.3333333333333333,2.3846153846153846,2.857142857142857,3.6,3.375,2.6923076923076925,3.4,3.2,3.625,3.0,3.4615384615384617,3.466666666666667,3.8125,3.625,4.0,3.25,4.0,4.0,2.4,1.75,3.0,3.5625,3.466666666666667,2.857142857142857,1.5714285714285714,3.5,4.0,2.5,1.0909090909090908,3.0,0.75,2.5,3.1538461538461537,2.6153846153846154,3.25,3.25,4.0,4.0,3.466666666666667,3.0,3.2,3.5,2.6,4.0,4.0,2.25,4.0,3.8,3.071428571428572,2.8,1.125,2.909090909090909,2.7,3.8,2.071428571428572,2.6,2.0,3.294117647058824,3.0,3.25,3.533333333333333,3.8125,3.230769230769231,2.6,4.0,3.6,2.4,3.235294117647059,2.7333333333333334,3.2142857142857144,0.0,3.333333333333333,3.2,3.692307692307693,3.363636363636364,2.6875,3.5384615384615383,3.2666666666666666,3.8,3.8,3.4166666666666665,3.636363636363636,2.5,3.6,3.75,2.764705882352941,2.625,3.5625,4.0,3.4615384615384617,1.8333333333333333,3.4615384615384617,3.75,2.769230769230769,2.571428571428572,4.0,3.8333333333333335,3.0,3.75,1.375,2.769230769230769,3.0,2.5,3.076923076923077,1.3333333333333333,3.75,1.3,3.7857142857142856,3.0,4.0,3.4375,2.8181818181818183,2.9285714285714284,1.9285714285714288,4.0,3.8,2.375,1.2307692307692308,3.25,3.6,2.588235294117647,3.533333333333333,4.0,0.5,1.6923076923076923,2.933333333333333,1.1875,2.769230769230769,4.0,3.75,2.9285714285714284,4.0,3.5,1.9230769230769231,2.642857142857143,2.75,2.4285714285714284,3.5384615384615383,3.4615384615384617,4.0,2.8666666666666667,3.8,1.0,3.5,3.272727272727273,2.6875,3.769230769230769,3.3076923076923075,2.857142857142857,3.5384615384615383,3.333333333333333,0.3,2.4615384615384617,2.2,3.8461538461538463,3.8125,3.25,4.0,4.0,3.6,3.4375,3.4,3.6666666666666665,3.333333333333333,3.75,4.0,3.4615384615384617,3.7142857142857135,1.6470588235294117,3.1875,2.1538461538461537,4.0,3.625,4.0,1.5333333333333334,0.4375,3.769230769230769,3.6666666666666665,3.5,4.0,3.0,3.333333333333333,3.6875,3.769230769230769,3.0,3.076923076923077,3.0,3.4,1.7142857142857142,3.25,4.0,4.0,1.75,2.3846153846153846,4.0,1.6666666666666667,3.6,2.647058823529412,4.0,4.0,3.3846153846153846,3.2,1.6,3.571428571428572,3.076923076923077,2.0,3.0,3.3076923076923075,1.4375,2.0,3.75,4.0,2.25,3.3846153846153846,2.2,3.3076923076923075,3.0625,3.1666666666666665,3.2666666666666666,3.75,3.75,3.0588235294117645,3.230769230769231,3.533333333333333,4.0,2.4,3.4375,4.0,2.2142857142857144,3.823529411764706,3.2,2.2,2.8461538461538463,3.0,3.076923076923077,3.230769230769231,3.75,3.0,4.0,3.6,3.0,3.75,4.0,3.8333333333333335,3.4375,0.5,3.882352941176471,3.2857142857142856,1.5,1.3333333333333333,3.4,2.6,2.6,3.8125,2.6923076923076925,2.4,3.5,3.6666666666666665,3.6,2.6666666666666665,2.9375,2.230769230769231,2.25,2.8461538461538463,4.0,4.0,3.0,3.5,1.75,4.0,2.8125,3.8125,3.6,2.5,3.4375,2.6923076923076925,3.4,4.0,3.769230769230769,0.5,0.2857142857142857,3.2,4.0,3.6666666666666665,0.6,3.2,2.7,2.4615384615384617,4.0,3.3125,3.6666666666666665,3.25,0.0,4.0,4.0,2.75,4.0,3.8,2.0,4.0,4.0,4.0,3.6875,3.5384615384615383,2.357142857142857,3.25,1.8461538461538465,3.6,3.8,4.0,4.0,1.3076923076923077,1.5,3.1333333333333333,3.8125,4.0,3.8,3.625,2.769230769230769,3.769230769230769,3.75,4.0,4.0,3.0,3.6666666666666665,3.5,2.571428571428572,3.333333333333333,0.5,3.75,3.2142857142857144,3.25,4.0,2.8125,2.923076923076923,3.4,3.076923076923077,3.75,2.6,4.0,2.8461538461538463,0.0,3.0,3.0,3.272727272727273,3.066666666666667,0.1875,2.923076923076923,4.0,3.2142857142857144,3.4375,3.0625,3.625,3.076923076923077,1.875,1.25,3.75,3.5,3.142857142857143,4.0,3.0,4.0,2.555555555555556,3.5,2.875,3.142857142857143,3.375,4.0,4.0,3.3076923076923075,3.75,4.0,1.0,3.6,1.8,4.0,4.0,2.5,3.0,4.0,3.823529411764706,2.6,4.0,3.3076923076923075,2.5384615384615383,3.571428571428572,4.0,3.8181818181818175,2.1666666666666665,2.0,3.727272727272727,4.0,2.25,3.1875,3.75,3.2,1.6666666666666667,3.5,3.230769230769231,3.5,3.75,4.0,3.0,2.8,3.8,2.8,3.4,3.6666666666666665,3.375,3.2857142857142856,1.7142857142857142,4.0,3.6,3.0,2.75,2.5384615384615383,2.764705882352941,1.2666666666666666,4.0,3.375,3.928571428571429,4.0,2.4285714285714284,3.1538461538461537,3.636363636363636,3.2857142857142856,4.0,4.0,3.4615384615384617,3.769230769230769,3.5384615384615383,3.75,3.1538461538461537,3.4615384615384617,0.9333333333333332,4.0,2.8666666666666667,3.0625,1.9230769230769231,3.4,3.75,3.5,4.0,3.4375,2.0,3.071428571428572,2.923076923076923,3.4375,3.8333333333333335,1.2307692307692308,3.0,3.75,4.0,2.6666666666666665,3.8,3.4,4.0,1.3333333333333333,3.5294117647058822,4.0,3.25,3.2,3.8,3.0625,4.0,3.0,3.3846153846153846,0.0,3.4,4.0,1.5,4.0,1.5,2.0,2.0,4.0,2.375,3.5,3.625,2.8,3.1538461538461537,3.5454545454545454,3.4,2.6923076923076925,3.5,3.230769230769231,1.588235294117647,3.769230769230769,3.5294117647058822,0.0,3.6666666666666665,3.2857142857142856,3.4,4.0,2.375,4.0,2.3846153846153846,3.117647058823529,3.75,3.2,3.5384615384615383,3.230769230769231,3.230769230769231,3.0,3.25,3.6,3.5384615384615383,2.4,2.923076923076923,3.0,3.0588235294117645,3.533333333333333,0.2307692307692307,3.4375,2.9285714285714284,3.583333333333333,2.333333333333333,2.8461538461538463,3.6,3.2857142857142856,4.0,3.5384615384615383,2.6,2.2,3.5,3.0,1.0,3.6,2.3076923076923075,2.75,3.0,4.0,4.0,3.0,1.8461538461538465,3.4285714285714284,2.857142857142857,2.357142857142857,3.0,4.0,3.75,3.6,3.75,3.125,2.625,4.0,4.0,3.4,3.0,2.533333333333333,3.5,3.4444444444444446,4.0,1.3333333333333333,0.0,3.4,3.769230769230769,3.0,0.4444444444444444,3.3,3.5,4.0,0.0,4.0,3.7857142857142856,2.0,3.625,4.0,3.6666666666666665,2.1538461538461537,3.0,3.8,3.8,3.3076923076923075,2.769230769230769,3.071428571428572,4.0,3.6,3.75,3.272727272727273,3.0,4.0,2.9285714285714284,3.4,3.2,4.0,3.3,0.75,2.3076923076923075,2.6666666666666665,3.684210526315789,0.0,3.0,1.5384615384615383,3.5,4.0,3.25,2.7857142857142856,3.6,2.7,3.6,3.5,3.5,3.769230769230769,3.230769230769231,3.636363636363636,3.25,4.0,3.75,3.8,3.764705882352941,3.5625,1.5,3.0,4.0,4.0,3.4,3.5384615384615383,2.4,3.0,3.8,3.25,3.0,3.25,2.6,1.1875,3.6,4.0,2.8,4.0,2.7857142857142856,2.75,2.4285714285714284,4.0,4.0,3.583333333333333,2.5,4.0,2.3076923076923075,4.0,2.5,4.0,3.2142857142857144,3.4,3.25,3.0,4.0,4.0,2.6666666666666665,3.2,3.142857142857143,2.2,3.0,2.875,3.7857142857142856,3.692307692307693,4.0,3.5,2.2,2.25,3.733333333333333,2.75,1.2857142857142858,4.0,3.4,3.25,3.764705882352941,2.230769230769231,2.75,3.636363636363636,3.769230769230769,0.5,2.75,2.533333333333333,3.3076923076923075,4.0,3.3076923076923075,2.857142857142857,2.75,3.2,2.5,4.0,2.1,1.4285714285714286,2.6923076923076925,3.8125,3.2857142857142856,1.5333333333333334,3.9375,2.4285714285714284,4.0,4.0,3.25,3.333333333333333,1.9,3.4615384615384617,3.2,3.9166666666666665,3.076923076923077,3.769230769230769,2.5625,2.1666666666666665,1.2727272727272727,3.0,4.0,2.75,3.4375,2.333333333333333,2.5,2.7857142857142856,4.0,4.0,2.0,3.4615384615384617,4.0,0.0,3.5625,3.25,3.75,3.25,4.0,3.769230769230769,3.4,4.0,4.0,3.2,2.142857142857143,4.0,3.25,3.4,4.0,3.9,4.0,3.6666666666666665,3.2,2.875,3.5294117647058822,2.636363636363636,3.272727272727273,4.0,3.692307692307693,0.0,4.0,2.8461538461538463,3.3125,3.4375,3.0,3.75,3.5,3.4,3.25,3.2,4.0,3.0,3.0,3.4375,3.1538461538461537,2.0,3.6,2.9166666666666665,3.0625,4.0,2.375,2.4615384615384617,4.0,3.0,3.25,4.0,1.75,3.0,4.0,4.0,2.875,3.769230769230769,2.5384615384615383,3.25,3.8,3.636363636363636,3.5,3.0,3.5384615384615383,2.4615384615384617,4.0,2.0,3.5,4.0,2.4,2.636363636363636,2.6,2.9285714285714284,3.75,2.333333333333333,3.75,3.25,2.25,3.1875,3.0588235294117645,3.333333333333333,1.1428571428571428,3.625,3.692307692307693,3.0625,4.0,2.6,1.6666666666666667,3.4375,3.588235294117647,3.571428571428572,1.6153846153846154,2.8461538461538463,3.357142857142857,4.0,3.4615384615384617,4.0,1.25,3.0,3.769230769230769,1.0,3.6,2.9285714285714284,3.230769230769231,3.4,3.4285714285714284,3.7142857142857135,4.0,3.333333333333333,3.0,3.0,3.076923076923077,3.5625,3.888888888888889,1.4615384615384617,4.0,3.75,3.8125,4.0,3.2,4.0,3.75,3.3529411764705883,4.0,2.875,2.9375,0.3333333333333333,3.647058823529412,3.3,2.923076923076923,2.9375,2.2142857142857144,0.8181818181818182,3.076923076923077,4.0,3.0,2.6,4.0,2.75,4.0,3.8,3.0,3.6,3.533333333333333,3.7,3.1875,2.6,1.4615384615384617,2.090909090909091,3.625,4.0,3.3846153846153846,3.1333333333333333,3.5384615384615383,4.0,3.0,3.8,3.6666666666666665,3.75,2.875,3.75,2.2,0.0,3.142857142857143,1.4,3.1875,3.4615384615384617,4.0,3.8,3.1538461538461537,4.0,3.8,2.1666666666666665,1.5625,2.25,2.8461538461538463,4.0,3.5,3.25,2.823529411764706,2.6923076923076925,3.0,3.333333333333333,3.4,1.6153846153846154,2.7142857142857144,3.692307692307693,3.0,3.0,4.0,2.75,3.727272727272727,3.066666666666667,3.5,0.6666666666666666,4.0,3.0,1.5454545454545454,3.25,3.6666666666666665,3.142857142857143,4.0,3.6666666666666665,3.6666666666666665,3.5384615384615383,4.0,4.0,3.5,3.0,1.6428571428571428,2.5384615384615383,1.6923076923076923,2.6666666666666665,3.75,4.0,3.0,4.0,1.5,1.7692307692307692,2.333333333333333,2.9285714285714284,3.8125,2.6923076923076925,0.0,3.0,3.5,3.625,3.6,3.1538461538461537,3.25,3.75,2.0,3.2,2.9375,3.0,3.0,3.6,3.0,3.842105263157895,4.0,3.6666666666666665,3.1875,4.0,2.6,3.0,4.0,2.0625,4.0,4.0,4.0,3.75,3.1,4.0,3.142857142857143,4.0,3.5,3.0,3.5,3.571428571428572,3.6666666666666665,3.625,2.5625,2.7142857142857144,2.75,2.230769230769231,3.076923076923077,3.7142857142857135,4.0,1.0909090909090908,3.4705882352941178,3.5384615384615383,3.4285714285714284,4.0,3.4285714285714284,3.8,3.0,0.8888888888888888,3.764705882352941,3.75,3.333333333333333,3.2142857142857144,0.6666666666666666,3.1666666666666665,3.6,2.6666666666666665,1.4705882352941178,2.5,3.692307692307693,3.2857142857142856,3.692307692307693,3.7222222222222223,2.0,3.75,4.0,2.8,3.25,0.0,4.0,0.0,3.3,3.272727272727273,3.6,4.0,3.0625,4.0,3.636363636363636,2.4285714285714284,3.5,3.692307692307693,2.923076923076923,2.857142857142857,2.0,1.0769230769230769,1.3333333333333333,1.0,2.4615384615384617,3.7142857142857135,3.142857142857143,4.0,4.0,3.6,3.2,4.0,2.076923076923077,3.6,4.0,3.764705882352941,1.0909090909090908,3.230769230769231,1.8888888888888888,4.0,3.2857142857142856,4.0,4.0,3.0,3.6,3.2,2.75,3.25,3.0,3.8,4.0,3.230769230769231,3.5,3.0,3.2,4.0,3.0,3.875,2.5,3.0,0.0,3.769230769230769,3.75,2.0,3.083333333333333,3.375,2.636363636363636,3.5,3.375,0.6923076923076923,4.0,3.3076923076923075,4.0,2.6,3.333333333333333,3.5625,4.0,3.6,4.0,3.388888888888889,3.4,3.25,2.8461538461538463,2.6923076923076925,3.6666666666666665,1.25,2.083333333333333,2.7142857142857144,2.6153846153846154,3.533333333333333,3.0,4.0,2.6666666666666665,4.0,2.6153846153846154,4.0,4.0,3.647058823529412,0.8181818181818182,3.6,4.0,3.25,4.0,2.8,3.0,2.333333333333333,3.5,3.769230769230769,4.0,4.0,3.333333333333333,3.0625,3.7,3.230769230769231,2.142857142857143,4.0,3.25,2.6923076923076925,2.857142857142857,2.2666666666666666,4.0,2.5384615384615383,1.7692307692307692,4.0,2.642857142857143,2.0,3.5625,2.142857142857143,3.0,2.6,3.6,2.5,3.6875,4.0,3.4375,3.4,4.0,3.8,1.2307692307692308,3.571428571428572,2.0,2.5,3.25,4.0,2.0,3.8125,3.0,3.5,4.0,4.0,4.0,2.9375,4.0,4.0,4.0,3.4166666666666665,3.8,3.692307692307693,2.7142857142857144,3.769230769230769,2.8,3.2,3.0,3.333333333333333,4.0,3.0,4.0,3.2142857142857144,3.375,0.0,3.0,3.6666666666666665,3.6,3.4,3.4,3.4,3.6,1.6153846153846154,3.823529411764706,3.071428571428572,3.076923076923077,1.3333333333333333,4.0,3.5,2.071428571428572,3.2,2.3076923076923075,3.533333333333333,1.6666666666666667,4.0,3.8125,2.0,4.0,3.0,2.75,3.071428571428572,2.0,2.0,4.0,3.6,3.5,4.0,3.642857142857143,4.0,2.8,2.9375,3.4,3.076923076923077,3.5,3.533333333333333,1.0769230769230769,1.3,3.5,1.3333333333333333,2.5,2.4,3.0588235294117645,3.5,2.2,2.25,2.3,2.9,3.7857142857142856,3.636363636363636,3.8,2.0,3.1818181818181817,3.5294117647058822,2.25,3.0,3.571428571428572,2.875,3.0,3.2,3.5625,2.333333333333333,3.8125,3.8125,1.3,4.0,2.5,2.75,3.5,3.8,3.235294117647059,2.3076923076923075,3.0,3.0,1.6875,3.1875,3.2666666666666666,4.0,2.333333333333333,1.9285714285714288,2.4615384615384617,3.5,3.75,3.75,1.0769230769230769,3.230769230769231,4.0,4.0,3.3846153846153846,2.333333333333333,2.636363636363636,3.769230769230769,2.75,4.0,3.235294117647059,4.0,3.25,3.769230769230769,3.8125,2.25,3.0,2.8125,4.0,3.5454545454545454,3.363636363636364,1.9285714285714288,4.0,4.0,3.076923076923077,2.4615384615384617,3.5,3.0,3.6,4.0,1.1,2.8,2.5,2.7142857142857144,3.8125,0.0,3.5625,3.2,4.0,4.0,4.0,4.0,3.071428571428572,2.5454545454545454,3.625,2.333333333333333,2.8,3.1333333333333333,3.9,2.0,2.111111111111111,3.363636363636364,2.4285714285714284,1.6,2.2142857142857144,3.3529411764705883,3.142857142857143,2.571428571428572,3.4,2.9285714285714284,3.0,2.9375,2.6,2.6,3.4375,3.6,0.6666666666666666,1.0769230769230769,4.0,3.769230769230769,1.0,2.25,1.8,1.75,2.4,3.769230769230769,4.0,3.25,3.8,2.5625,3.0,4.0,3.2,4.0,2.1875,3.0,1.2,2.533333333333333,3.8333333333333335,4.0,2.8125,4.0,0.25,0.0,3.4166666666666665,4.0,4.0,2.0,2.8,3.615384615384616,3.25,3.8,3.75,1.7333333333333334,3.1333333333333333,2.6666666666666665,3.75,2.6,3.25,2.75,3.588235294117647,2.642857142857143,2.875,2.066666666666667,4.0,3.4,4.0,2.6923076923076925,2.25,3.8,3.769230769230769,3.1333333333333333,3.5384615384615383,2.9285714285714284,2.0,3.5384615384615383,3.2,3.25,1.6153846153846154,4.0,3.3,3.0,2.0,2.0,3.571428571428572,4.0,3.6,4.0,2.4,3.4,2.75,2.588235294117647,2.411764705882353,4.0,4.0,2.7333333333333334,3.0,2.5,4.0,1.5,3.769230769230769,2.5,2.7857142857142856,3.5384615384615383,2.333333333333333,3.8125,3.1875,3.357142857142857,2.857142857142857,3.0,3.0,3.5,1.875,3.25,4.0,2.5,3.6,4.0,1.0,4.0,3.727272727272727,4.0,4.0,2.75,4.0,3.0,2.909090909090909,3.8125,2.076923076923077,2.6,1.8,3.0,3.0,4.0,1.0,3.75,3.8125,3.4166666666666665,4.0,4.0,3.8125,3.0,4.0,4.0,3.2,4.0,4.0,4.0,3.083333333333333,4.0,2.857142857142857,3.333333333333333,3.8125,3.4,2.75,2.333333333333333,3.8,0.875,1.75,4.0,3.1538461538461537,2.2,2.583333333333333,2.375,4.0,1.8,3.625,3.75,2.6,3.8,2.857142857142857,3.3076923076923075,2.8,2.25,3.125,2.7857142857142856,4.0,3.533333333333333,2.1538461538461537,4.0,4.0,1.5,2.2,4.0,3.8125,2.9166666666666665,3.0,4.0,1.1428571428571428,3.363636363636364,2.0,2.333333333333333,2.75,2.5454545454545454,3.0,1.4444444444444444,3.076923076923077,2.6153846153846154,4.0,3.1875,3.75,3.636363636363636,3.4,3.4615384615384617,3.25,2.6153846153846154,3.0,4.0,4.0,3.230769230769231,3.5384615384615383,3.4,3.4375,4.0,2.3076923076923075,2.0,2.1,3.6666666666666665,4.0,2.769230769230769,2.833333333333333,3.5384615384615383,4.0,4.0,2.25,3.6666666666666665,4.0,2.642857142857143,3.5,2.9,0.0,3.3125,3.142857142857143,3.625,3.0,4.0,3.6,2.1875,2.066666666666667,4.0,3.8125,2.8,3.75,3.7142857142857135,2.1538461538461537,2.2,3.411764705882353,4.0,3.8,2.0588235294117645,4.0,4.0,3.8,2.6,3.533333333333333,3.75,3.8,4.0,1.7142857142857142,3.75,3.2142857142857144,3.25,3.5,2.25,3.6,3.75,3.5,1.5454545454545454,3.923076923076923,3.769230769230769,2.6,4.0,3.3125,1.6,2.333333333333333,3.333333333333333,2.75,0.5,2.571428571428572,3.4,3.3125,1.2307692307692308,4.0,4.0,2.6,4.0,4.0,3.6,2.235294117647059,1.5,3.0,2.4615384615384617,3.4615384615384617,3.7,2.8,3.230769230769231,4.0,2.75,3.6,2.0,4.0,3.692307692307693,3.0,4.0,3.4444444444444446,3.2142857142857144,4.0,3.6,3.4,1.0769230769230769,2.6666666666666665,3.25,3.3846153846153846,3.0625,4.0,3.294117647058824,3.8125,2.0,2.5,3.5384615384615383,3.3529411764705883,2.3076923076923075,4.0,3.2,3.6,4.0,3.4,3.6,3.0,3.4375,4.0,0.0,4.0,3.4,2.4615384615384617,3.4,3.8125,2.6666666666666665,3.5384615384615383,4.0,3.2,3.1875,4.0,3.25,3.571428571428572,2.2,3.6666666666666665,3.2857142857142856,3.6,4.0,3.125,4.0,2.6923076923076925,0.0,3.333333333333333,4.0,1.3333333333333333,4.0,2.769230769230769,4.0,2.9375,3.6,2.642857142857143,2.9285714285714284,2.75,3.090909090909091,3.4,3.727272727272727,3.25,2.75,2.076923076923077,4.0,4.0,3.6,0.5,1.75,4.0,3.75,4.0,3.8,4.0,3.6666666666666665,4.0,2.0,3.0,2.8,3.769230769230769,2.230769230769231,3.769230769230769,3.1538461538461537,2.8,2.1666666666666665,4.0,2.5,4.0,2.0,2.5,4.0,2.571428571428572,3.0,3.583333333333333,4.0,3.4705882352941178,3.363636363636364,2.9,1.8,3.769230769230769,2.4,1.375,2.466666666666667,3.3076923076923075,3.769230769230769,3.8125,3.066666666666667,3.6666666666666665,3.6,3.230769230769231,2.4444444444444446,3.684210526315789,4.0,2.1818181818181817,4.0,2.6875,0.375,3.142857142857143,3.0,3.0,3.6666666666666665,2.4166666666666665,4.0,3.0,3.2222222222222223,3.7857142857142856,3.4615384615384617,3.5,3.25,4.0,3.0625,3.8,2.4166666666666665,1.2857142857142858,3.3125,3.8125,2.7333333333333334,3.571428571428572,3.3076923076923075,4.0,3.4,3.466666666666667,4.0,0.0,3.533333333333333,3.3846153846153846,2.4285714285714284,3.5625,3.6666666666666665,2.769230769230769,3.4705882352941178,4.0,3.625,3.769230769230769,1.0,3.642857142857143,3.75,4.0,2.0,2.75,3.5,4.0,4.0,4.0,1.2142857142857142,2.8125,0.0,3.333333333333333,3.2,4.0,3.4615384615384617,1.4285714285714286,3.769230769230769,3.5,3.090909090909091,3.3,3.5294117647058822,3.769230769230769,1.7142857142857142,3.2,2.4,3.7142857142857135,4.0,2.933333333333333,3.75,3.75,3.5384615384615383,3.142857142857143,3.7142857142857135,4.0,3.066666666666667,3.2142857142857144,3.5,3.692307692307693,4.0,3.733333333333333,4.0,4.0,2.3,3.25,3.5,3.3125,4.0,3.2666666666666666,3.6,4.0,3.0,3.142857142857143,3.230769230769231,3.4,3.466666666666667,3.333333333333333,0.0,3.0,3.8,2.5,3.5,2.5625,3.5384615384615383,4.0,3.125,3.6,2.75,1.1111111111111112,1.0,3.125,3.0,4.0,3.5294117647058822,2.5,2.5625,2.5,3.142857142857143,4.0,2.076923076923077,2.6666666666666665,3.466666666666667,2.75,0.5,4.0,3.4166666666666665,3.1666666666666665,3.4615384615384617,3.1538461538461537,3.7142857142857135,2.6923076923076925,3.0,3.3076923076923075,3.0,3.0,4.0,0.8,3.0,3.75,3.5,3.0,3.75,2.764705882352941,2.5384615384615383,0.4615384615384615,2.4166666666666665,3.7857142857142856,3.230769230769231,3.8125,3.0,2.6875,3.0,2.8,2.1875,4.0,3.4,3.8,1.0,2.769230769230769,3.5384615384615383,3.4,3.636363636363636,3.5,2.9375,2.8461538461538463,4.0,3.6,0.0,3.5384615384615383,3.076923076923077,2.4,3.5384615384615383,4.0,3.1333333333333333,2.25,1.5,3.25,2.8,3.4705882352941178,4.0,3.0,4.0,0.0,2.769230769230769,4.0,3.2142857142857144,2.6,4.0,2.6666666666666665,3.8125,3.75,4.0,3.75,3.4375,3.25,3.692307692307693,4.0,3.823529411764706,3.8,3.25,3.583333333333333,3.769230769230769,3.3076923076923075,3.4,4.0,3.75,3.933333333333333,3.076923076923077,3.5,4.0,3.411764705882353,2.125,4.0,4.0,4.0,2.769230769230769,3.5384615384615383,2.857142857142857,3.75,3.25,1.3333333333333333,2.75,1.7692307692307692,3.75,3.642857142857143,4.0,2.888888888888889,4.0,0.0,3.6,3.636363636363636,4.0,3.1538461538461537,4.0,2.4,4.0,4.0,3.8,1.4,3.75,1.0909090909090908,1.2,4.0,3.230769230769231,3.6,3.0,2.9166666666666665,1.9,3.388888888888889,2.857142857142857,3.8,4.0,4.0,3.5384615384615383,3.076923076923077,0.5,1.0,3.2,3.4615384615384617,1.75,4.0,3.764705882352941,4.0,3.0,3.0,2.764705882352941,3.25,1.2727272727272727,3.272727272727273,3.647058823529412,2.8,3.6,3.0,3.8,2.7142857142857144,2.8,3.076923076923077,2.8461538461538463,3.0,3.7142857142857135,1.6,0.0,1.875,3.5,3.692307692307693,4.0,4.0,4.0,4.0,3.466666666666667,3.769230769230769,2.8,2.5,2.6666666666666665,3.333333333333333,2.4285714285714284,3.230769230769231,3.3125,3.3846153846153846,3.571428571428572,4.0,4.0,3.0,2.0,2.6923076923076925,4.0,4.0,3.4375,0.3333333333333333,4.0,3.2,3.125,3.8,2.333333333333333,3.4,3.733333333333333,3.5,3.0,2.7,3.692307692307693,3.0,4.0,3.7857142857142856,3.2,3.4,3.0,3.375,2.75,2.4615384615384617,4.0,2.823529411764706,4.0,3.769230769230769,3.0,2.875,3.2,3.25,3.7857142857142856,4.0,2.7777777777777777,4.0,3.692307692307693,3.6,4.0,3.6,2.0,2.4,2.25,4.0,2.25,4.0,2.1538461538461537,3.2142857142857144,4.0,2.4615384615384617,4.0,4.0,2.25,4.0,2.6666666666666665,4.0,3.5,4.0,3.8125,3.692307692307693,3.0,3.5384615384615383,4.0,2.769230769230769,4.0,4.0,3.0,0.0,3.25,4.0,3.25,2.7142857142857144,3.25,2.333333333333333,3.5384615384615383,2.6666666666666665,2.076923076923077,3.4,0.0,3.75,2.4,3.8,2.4285714285714284,3.3125,3.4615384615384617,4.0,3.5,4.0,3.333333333333333,3.625,4.0,2.5,3.0,2.8125,2.5,4.0,2.4615384615384617,3.8125,2.75,3.5,2.7142857142857144,2.923076923076923,2.75,3.75,3.5,2.25,3.4,4.0,3.5454545454545454,3.2857142857142856,3.4,3.25,3.0,3.0,3.733333333333333,3.733333333333333,2.555555555555556,2.923076923076923,3.75,2.6923076923076925,3.5,1.3846153846153846,3.5,0.0,3.6875,2.466666666666667,3.4,4.0,2.7857142857142856,4.0,3.4,4.0,3.066666666666667,3.4615384615384617,4.0,3.75,3.5384615384615383,2.625,3.0,3.75,1.3333333333333333,3.6,3.692307692307693,3.25,2.25,3.2,2.8,3.647058823529412,2.6153846153846154,3.0,2.25,2.7142857142857144,4.0,2.5,2.5,3.25,2.0,2.571428571428572,3.8461538461538463,2.2142857142857144,3.0,2.823529411764706,3.2666666666666666,3.571428571428572,0.875,3.25,4.0,3.4615384615384617,3.0,2.6923076923076925,2.75,3.2,4.0,2.6153846153846154,2.066666666666667,3.2142857142857144,0.0,2.9375,2.5625,4.0,4.0,3.2,3.0,3.0,3.8125,3.5454545454545454,2.769230769230769,2.25,2.0,3.692307692307693,4.0,2.8461538461538463,4.0,3.1875,2.333333333333333,3.5,3.6,4.0,2.5,4.0,3.4,4.0,0.4,3.3076923076923075,3.7142857142857135,3.625,3.692307692307693,3.625,3.8,3.5,4.0,3.7,4.0,3.083333333333333,3.4615384615384617,3.0625,2.8461538461538463,4.0,2.2857142857142856,4.0,3.8125,3.5,3.0,2.571428571428572,4.0,2.875,1.5,3.6666666666666665,4.0,3.4615384615384617,3.2857142857142856,3.769230769230769,4.0,2.8,3.0,2.8,4.0,3.25,1.8333333333333333,3.0,3.25,3.3076923076923075,0.0,3.6666666666666665,4.0,3.7,2.75,3.071428571428572,3.533333333333333,2.6666666666666665,3.75,3.230769230769231,4.0,3.3076923076923075,4.0,3.0,3.4285714285714284,2.4,1.4375,4.0,2.2,3.5,3.75,3.615384615384616,4.0,3.2142857142857144,3.769230769230769,4.0,4.0,1.2857142857142858,3.2142857142857144,2.75,3.8,0.8,3.2,3.571428571428572,4.0,1.0,3.0,1.6428571428571428,2.2,1.0,1.5,2.7142857142857144,3.625,3.0,3.0,3.764705882352941,1.5,3.25,3.0,2.2,1.8461538461538465,3.75,4.0,3.357142857142857,2.8666666666666667,3.769230769230769,3.2857142857142856,4.0,3.0,2.769230769230769,3.0,3.375,3.7142857142857135,4.0,3.0,3.6,4.0,3.75,2.75,1.25,3.727272727272727,3.411764705882353,2.333333333333333,0.0,3.8,3.75,2.8125,4.0,3.615384615384616,0.5833333333333334,2.9285714285714284,3.75,3.8125,3.2,3.7,4.0,2.230769230769231,3.8,3.6666666666666665,4.0,4.0,3.4,2.75,3.076923076923077,2.3846153846153846,2.7777777777777777,4.0,3.733333333333333,3.117647058823529,4.0,2.142857142857143,3.5,0.0,2.071428571428572,3.823529411764706,3.4615384615384617,2.4615384615384617,4.0,3.8125,3.625,0.4615384615384615,2.0,4.0,4.0,3.142857142857143,4.0,4.0,1.7142857142857142,3.8125,1.8333333333333333,2.6153846153846154,2.4615384615384617,4.0,3.75,3.25,1.6153846153846154,1.4666666666666666,3.733333333333333,4.0,4.0,3.75,2.4375,4.0,3.1333333333333333,3.4166666666666665,3.0526315789473686,2.8,3.0,0.0,3.333333333333333,2.571428571428572,3.4615384615384617,3.4375,3.3846153846153846,4.0,2.8,3.0,2.1538461538461537,3.6,3.1538461538461537,3.2,3.071428571428572,3.8125,3.2,2.6,3.5,3.3125,2.909090909090909,2.75,2.8125,3.2142857142857144,2.2,0.0,3.076923076923077,3.4285714285714284,1.0,4.0,0.2307692307692307,3.0,2.8,4.0,4.0,3.7142857142857135,3.571428571428572,2.0,2.3125,4.0,2.230769230769231,2.6666666666666665,3.571428571428572,2.833333333333333,3.5,3.411764705882353,2.375,4.0,3.642857142857143,3.0625,3.6,3.272727272727273,3.6666666666666665,4.0,2.0,2.769230769230769,3.75,4.0,4.0,2.0,3.625,2.111111111111111,0.1875,3.5384615384615383,3.272727272727273,2.933333333333333,3.0,3.5294117647058822,4.0,3.0,3.625,4.0,3.2,2.6,3.6666666666666665,3.0,2.9166666666666665,3.0,0.9230769230769232,2.0,3.8125,3.5,2.4,3.857142857142857,4.0,3.0,1.3333333333333333,3.5,4.0,3.5,2.4615384615384617,3.076923076923077,3.2142857142857144,3.7857142857142856,4.0,2.142857142857143,1.8125,1.5,1.8333333333333333,3.727272727272727,2.9166666666666665,3.3076923076923075,0.3076923076923077,3.0588235294117645,3.4,2.75,3.5,2.0,3.6,3.5,2.9375,2.571428571428572,4.0,1.6153846153846154,3.6,4.0,3.8,3.625,1.8666666666666667,4.0,2.0625,3.6,2.9166666666666665,3.5625,3.4615384615384617,1.8461538461538465,4.0,4.0,1.6,2.8,1.25,3.4285714285714284,3.625,1.9375,4.0,3.733333333333333,3.4615384615384617,4.0,2.1538461538461537,3.230769230769231,2.2142857142857144,4.0,3.3846153846153846,4.0,3.083333333333333,0.6,3.6666666666666665,3.2142857142857144,3.5,4.0,0.5,4.0,0.25,3.25,3.2,3.375,2.857142857142857,2.333333333333333,3.5625,3.2857142857142856,2.090909090909091,3.272727272727273,1.6923076923076923,4.0,3.3076923076923075,3.4,2.642857142857143,3.0,4.0,2.6153846153846154,3.8,1.8571428571428568,3.6,4.0,3.2,3.733333333333333,3.1875,4.0,3.692307692307693,2.6153846153846154,2.933333333333333,2.875,0.0,3.0625,3.7142857142857135,4.0,3.2,1.1538461538461535,2.5454545454545454,4.0,2.7857142857142856,2.75,1.2307692307692308,3.0,2.642857142857143,4.0,3.5,3.8125,3.4,3.6,3.4,4.0,2.2857142857142856,3.071428571428572,4.0,2.25,3.2,3.333333333333333,1.6,1.8461538461538465,3.3529411764705883,2.294117647058824,3.0,4.0,3.75,3.692307692307693,3.6,3.375,3.25,3.8125,2.3,4.0,3.0,3.333333333333333,3.5,4.0,2.75,3.2142857142857144,3.3529411764705883,3.6666666666666665,3.8125,0.0,3.75,3.375,3.823529411764706,1.6666666666666667,3.769230769230769,3.8125,2.5384615384615383,3.588235294117647,2.5,0.9230769230769232,3.8125,0.0,3.230769230769231,3.625,3.625,3.5,0.0,3.4,3.0,2.8461538461538463,3.5,1.6,0.0,3.692307692307693,3.5384615384615383,3.769230769230769,3.692307692307693,3.5,4.0,3.4375,2.909090909090909,3.0,3.7,3.2,3.692307692307693,3.0,4.0,4.0,4.0,2.8125,0.4285714285714285,4.0,3.5294117647058822,3.692307692307693,3.0,3.6,3.4,3.5454545454545454,3.6,2.2,1.7692307692307692,2.9411764705882355,3.0,3.4285714285714284,3.375,3.466666666666667,1.6363636363636365,1.7,3.4615384615384617,1.8461538461538465,3.7857142857142856,4.0,3.636363636363636,3.0,1.9230769230769231,3.5625,2.5,1.8571428571428568,2.363636363636364,3.8125,4.0,2.4,2.5,2.7142857142857144,3.0,3.230769230769231,3.3846153846153846,3.142857142857143,0.0,3.0,4.0,3.6666666666666665,2.5,1.9090909090909087,3.0,3.8125,3.142857142857143,0.2727272727272727,3.2,3.4285714285714284,2.769230769230769,4.0,1.0,2.25,3.8,4.0,4.0,4.0,4.0,4.0,2.8461538461538463,3.2,3.090909090909091,3.625,3.4285714285714284,2.5384615384615383,3.533333333333333,3.8125,2.923076923076923,3.2,3.375,3.5,0.7692307692307693,4.0,3.333333333333333,4.0,3.1666666666666665,1.7692307692307692,3.8666666666666663,3.8125,3.25,2.4545454545454546,3.8,3.2666666666666666,3.769230769230769,3.8,3.0,3.6666666666666665,4.0,0.0,3.333333333333333,0.0,3.6666666666666665,2.6153846153846154,3.0,2.6666666666666665,3.75,2.0,2.8125,3.2857142857142856,3.6,3.4,4.0,2.833333333333333,3.692307692307693,3.5,3.625,2.75,4.0,1.3846153846153846,3.5,3.6,3.1538461538461537,3.0,3.8333333333333335,3.8333333333333335,3.5,4.0,3.375,3.7142857142857135,3.357142857142857,3.0,4.0,2.9,3.1666666666666665,3.8,3.0,2.8461538461538463,3.571428571428572,1.8571428571428568,3.8,3.75,3.357142857142857,2.25,3.111111111111111,2.1,2.8,4.0,3.8125,3.769230769230769,3.1,2.333333333333333,4.0,4.0,3.75,2.076923076923077,3.076923076923077,2.4166666666666665,3.5,1.5,4.0,1.9230769230769231,0.6923076923076923,0.0,3.25,3.25,4.0,2.25,3.2,2.769230769230769,3.5,3.6,3.0,3.75,2.5625,3.3,4.0,3.5,3.25,3.625,4.0,0.9090909090909092,4.0,3.5384615384615383,3.625,1.25,2.75,2.4705882352941178,2.8,3.5294117647058822,2.857142857142857,3.2857142857142856,3.0,1.6666666666666667,3.357142857142857,3.583333333333333,3.8,4.0,2.6923076923076925,2.6923076923076925,1.0,3.5625,3.6666666666666665,4.0,2.5,0.6666666666666666,3.8,3.2666666666666666,3.375,3.7777777777777777,2.642857142857143,3.466666666666667,3.5625,4.0,1.75,3.25,2.0,3.2,3.4545454545454546,3.625,3.076923076923077,2.3,2.0,4.0,3.4,3.625,4.0,3.25,4.0,3.0,0.0,2.0,4.0,3.25,3.75,4.0,2.6,2.5,2.6153846153846154,3.4,4.0,1.5454545454545454,3.8,3.6,4.0,2.833333333333333,3.0,1.7692307692307692,4.0,3.6,3.769230769230769,3.230769230769231,3.076923076923077,3.9375,2.090909090909091,3.4,2.4,3.0,3.75,4.0,4.0,4.0,3.0,3.692307692307693,2.8,2.4,2.071428571428572,3.076923076923077,4.0,3.0,2.076923076923077,3.4705882352941178,3.571428571428572,4.0,4.0,2.0,4.0,2.3076923076923075,3.6,4.0,3.4,3.8,3.625,2.4,3.4,4.0,4.0,2.75,3.25,3.0,1.5,3.3,3.5384615384615383,3.1,3.0,4.0,2.0,4.0,3.5625,3.176470588235294,3.6666666666666665,3.4285714285714284,4.0,3.8125,2.7857142857142856,3.333333333333333,0.0,4.0,4.0,3.3076923076923075,4.0,2.9285714285714284,1.3,2.142857142857143,2.25,3.230769230769231,2.0,3.8,4.0,1.6666666666666667,3.411764705882353,3.4615384615384617,2.75,4.0,2.75,1.5384615384615383,3.625,2.764705882352941,3.533333333333333,3.5,2.1875,1.9375,2.6,2.75,1.25,2.6875,3.769230769230769,3.25,3.4285714285714284,1.818181818181818,4.0,2.6153846153846154,3.1333333333333333,3.0,3.1875,3.4615384615384617,3.25,3.4,3.0,2.6,2.5,3.5384615384615383,3.4,3.4,3.625,2.1818181818181817,2.875,3.8,2.7,1.1538461538461535,3.5,3.8125,4.0,3.8125,3.823529411764706,4.0,3.4375,3.75,4.0,3.3846153846153846,2.3846153846153846,3.625,0.5,2.1,4.0,3.5384615384615383,3.7142857142857135,2.8461538461538463,4.0,3.769230769230769,3.4375,3.2857142857142856,3.176470588235294,3.083333333333333,3.8,4.0,4.0,2.8,1.3333333333333333,2.9285714285714284,4.0,4.0,2.6,3.4615384615384617,3.8,3.8,3.5,4.0,4.0,2.5,3.8125,4.0,2.8,3.4615384615384617,3.4615384615384617,4.0,2.923076923076923,3.4615384615384617,3.4,1.3636363636363635,4.0,2.0,3.4166666666666665,3.333333333333333,4.0,3.5,3.533333333333333,3.75,3.1875,2.8,3.0,4.0,3.1666666666666665,2.7,3.0,3.0,3.2666666666666666,3.75,4.0,4.0,3.0,3.0,3.071428571428572,3.357142857142857,3.533333333333333,3.75,4.0,2.0,2.5,3.0,2.6,3.0,2.8125,4.0,2.6875,3.8,1.9230769230769231,3.083333333333333,0.0,3.6666666666666665,3.5,4.0,4.0,3.4,3.125,3.625,3.25,2.8823529411764706,3.75,4.0,3.692307692307693,4.0,4.0,3.5625,4.0,3.2666666666666666,2.0,1.9166666666666667,2.5,4.0,3.0,3.588235294117647,3.5,3.3076923076923075,3.333333333333333,2.5625,3.25,3.8,3.0,4.0,3.5,1.6666666666666667,2.923076923076923,3.0,3.071428571428572,3.5,3.2142857142857144,2.571428571428572,2.7857142857142856,2.230769230769231,4.0,1.75,3.7,3.333333333333333,4.0,0.75,1.9444444444444444,3.4,1.0,3.1875,3.4,4.0,3.375,2.7,2.6,3.8125,3.363636363636364,3.0,3.2142857142857144,2.6153846153846154,1.3333333333333333,2.0,2.2,3.533333333333333,3.4375,3.2142857142857144,4.0,3.5,1.3333333333333333,4.0,3.2,2.6923076923076925,0.0,1.7333333333333334,4.0,2.8,4.0,1.5,2.6666666666666665,4.0,0.6428571428571429,3.125,4.0,3.0,2.3846153846153846,2.923076923076923,3.4285714285714284,2.0,3.4,2.769230769230769,2.8,2.75,3.7142857142857135,2.8,2.4285714285714284,3.8125,2.0,1.2,4.0,4.0,3.0,1.5,3.5,4.0,2.0,3.8,2.75,3.230769230769231,2.857142857142857,4.0,3.933333333333333,3.230769230769231,3.1818181818181817,4.0,2.6,4.0,2.0,3.692307692307693,2.8,2.5,3.823529411764706,4.0,3.5,3.25,3.272727272727273,0.0,3.4,3.8125,4.0,4.0,3.75,1.5,3.5,3.3076923076923075,3.5,3.375,4.0,3.769230769230769,1.3846153846153846,3.6,2.5384615384615383,3.083333333333333,3.2,3.6,2.0,4.0,2.090909090909091,3.5625,3.363636363636364,3.25,4.0,3.75,0.8461538461538461,4.0,2.3076923076923075,1.9166666666666667,0.8461538461538461,2.25,4.0,4.0,1.0,2.8461538461538463,2.5,3.0,1.8461538461538465,2.111111111111111,3.1875,3.6,4.0,3.769230769230769,2.0,3.75,2.625,2.333333333333333,4.0,3.8,2.0,3.5,3.733333333333333,3.0,4.0,4.0,4.0,4.0,3.0,3.333333333333333,3.75,3.0,3.4,3.0,4.0,2.9375,3.6,3.7142857142857135,4.0,3.5384615384615383,2.0,3.4,3.6666666666666665,3.5,2.25,3.6666666666666665,4.0,4.0,2.923076923076923,3.0,0.0,3.8,3.4285714285714284,3.5384615384615383,4.0,4.0,4.0,1.5,2.75,3.583333333333333,4.0,2.0,3.25,4.0,2.6923076923076925,3.2857142857142856,1.625,4.0,2.1818181818181817,4.0,3.375,4.0],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"SEM_2_STATUS=%{y}<br>GPA_2=%{x}<extra></extra>\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#EF553B\"},\"name\":\"1\",\"notched\":false,\"offsetgroup\":\"1\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[2.5,0.75,0.0,1.25,2.8,3.8,4.0,0.0,1.0,2.588235294117647,0.0,0.0,2.75,0.0,0.8,3.75,3.571428571428572,0.9230769230769232,2.5,1.25,0.0,0.0,4.0,3.4705882352941178,0.0,3.230769230769231,0.0,3.333333333333333,4.0,0.0,0.7142857142857143,2.0625,0.7,0.8333333333333334,0.0,3.625,0.4,0.2,0.0,0.8,0.1875,0.0,1.5,0.7142857142857143,0.0,0.6666666666666666,4.0,2.4375,0.6666666666666666,2.2,2.5,1.5,1.0,1.8,1.5,3.230769230769231,0.0,3.4375,2.5,2.0,1.5,3.0,0.0,1.3333333333333333,0.0,2.0,4.0,0.4,0.0,2.75,1.5833333333333333,1.4,2.571428571428572,1.5384615384615383,0.0,3.769230769230769,0.0,1.5,2.5,0.5454545454545454,0.0,0.0,0.0,0.0,3.375,2.375,0.0,1.25,3.8333333333333335,4.0,2.0,2.0,3.3846153846153846,4.0,3.0,0.0,1.2307692307692308,1.0,0.7,1.3333333333333333,0.75,1.25,4.0,2.7,3.0,0.0,3.071428571428572,4.0,2.083333333333333,0.0,3.071428571428572,2.333333333333333,0.0,3.2,0.2,0.0,2.25,0.5,4.0,0.0,0.25,2.0,2.0,4.0,1.0,1.5,2.333333333333333,1.5,0.0,2.4285714285714284,3.1875,1.0,1.5714285714285714,1.75,4.0,0.0,4.0,0.0,4.0,2.5,0.0,3.142857142857143,0.0,0.0,0.2,0.0,0.3636363636363636,1.5,1.6,1.375,3.8,2.0,1.6,1.0,0.0,3.4,0.25,4.0,0.6666666666666666,1.0,0.0,0.0,3.75,3.25,0.0,1.1111111111111112,3.4,2.1,2.75,3.230769230769231,1.4,1.0,2.0,0.2307692307692307,3.8333333333333335,4.0,2.5454545454545454,1.75,1.3333333333333333,3.5384615384615383,1.0,4.0,0.3333333333333333,3.5625,0.0,1.25,0.4,2.0,1.0,1.6153846153846154,0.6,1.0,0.0,0.0,3.4,0.0,1.0,4.0,1.3333333333333333,0.8,0.5384615384615384,0.0,4.0,1.1538461538461535,1.5,0.8,2.5454545454545454,0.0,0.0,3.111111111111111,0.0,3.636363636363636,0.0,1.3333333333333333,3.8333333333333335,3.6,3.6,0.0,2.2,0.25,0.4,0.6,2.333333333333333,4.0,0.2222222222222222,0.6666666666666666,3.5384615384615383,1.0,3.0,0.0,3.636363636363636,3.235294117647059,1.4285714285714286,2.75,2.25,3.5,3.333333333333333,3.25,0.0,0.0,0.1875,4.0,4.0,2.071428571428572,0.4615384615384615,3.0,2.4615384615384617,3.6,0.4285714285714285,0.0,0.9,3.0625,2.6,3.0,3.0,2.230769230769231,1.4,1.2222222222222223,0.25,0.0,0.0,2.5384615384615383,0.0,3.25,1.5,1.25,2.3,4.0,4.0,1.0,1.5,1.0,0.0,2.0,3.333333333333333,0.4,0.0,3.0,1.25,0.0,3.125,3.0,4.0,2.5,1.25,0.0,4.0,0.25,4.0,0.7692307692307693,3.0,2.0,2.8,0.0,4.0,0.3333333333333333,1.6923076923076923,0.5833333333333334,0.2307692307692307,1.0,4.0,0.0,0.4,1.0,2.2857142857142856,0.9090909090909092,1.0,2.75,0.6666666666666666,1.8571428571428568,0.0,0.0,0.4,0.0,1.8461538461538465,0.5,3.6,0.0,2.75,2.230769230769231,0.0,2.0,3.5,3.692307692307693,3.692307692307693,2.5,0.0,1.0,3.4375,4.0,1.5,4.0,0.0,2.7142857142857144,1.5,2.5,3.5,0.0,3.2857142857142856,1.5,3.0,2.923076923076923,3.769230769230769,3.769230769230769,1.5,1.25,0.2727272727272727,0.4615384615384615,0.5384615384615384,3.111111111111111,0.0,1.5,3.5,0.0,1.2,1.3333333333333333,1.5,0.0,2.25,0.0,1.0,0.6666666666666666,1.5,0.0,0.5,2.333333333333333,2.8,2.0625,0.8571428571428571,0.0,2.333333333333333,1.5,2.125,3.75,3.3076923076923075,3.333333333333333,0.75,1.2857142857142858,0.0,0.0,3.6666666666666665,0.0,3.75,3.1875,2.125,2.0,1.0,1.1538461538461535,1.5,1.7272727272727273,3.4615384615384617,0.0,0.375,2.75,0.8571428571428571,0.5,0.75,2.0,0.0,3.0,0.0,0.0,2.0,0.5,0.5,0.0,0.0,0.1875,0.75,1.2142857142857142,1.8461538461538465,0.0,0.3333333333333333,2.636363636363636,3.2,0.25,1.7142857142857142,0.0,1.25,0.2142857142857142,1.8,1.0,1.25,0.0,0.25,0.0,1.6363636363636365,0.5714285714285714,3.25,4.0,0.6,0.0,3.0,0.2,2.625,2.8,1.6666666666666667,0.0,0.1538461538461538,0.6666666666666666,3.8333333333333335,0.0,1.0,4.0,3.625,1.75,2.5,2.0,0.3333333333333333,0.0,3.0,3.6666666666666665,1.0,4.0,3.5,2.333333333333333,0.0,1.0,1.125,1.2,0.0,1.5,1.1538461538461535,3.4375,0.0,2.5,1.25,2.4285714285714284,1.3333333333333333,3.4,0.0,0.25,0.0,1.375,2.923076923076923,0.5,0.2,2.076923076923077,2.75,0.3333333333333333,3.642857142857143,4.0,0.0,3.4,0.0,2.5,3.5,1.9166666666666667,0.1333333333333333,0.0,1.2,0.7692307692307693,3.6666666666666665,0.0,2.833333333333333,1.9230769230769231,2.5,3.230769230769231,0.6,3.2,3.733333333333333,4.0,1.0,0.0,0.25,1.5,0.0,1.2,0.0,0.0,0.0,0.0,2.0,4.0,3.0,3.235294117647059,1.4285714285714286,3.8,0.0,2.0,1.25,1.8,3.466666666666667,0.0,2.333333333333333,4.0,1.9230769230769231,1.7,1.5454545454545454,2.294117647058824,1.3333333333333333,0.0,3.230769230769231,0.6,3.5,0.0,0.0,0.0,3.769230769230769,1.75,0.8,0.75,1.6428571428571428,3.764705882352941,4.0,0.9166666666666666,0.4,0.0,0.0,0.5,1.0833333333333333,0.0,0.0,2.0,0.0,1.1,0.9230769230769232,0.0,0.0,0.0,3.25,2.857142857142857,1.75,2.5,0.0,0.0,1.0,1.3333333333333333,0.0,0.0,2.0,0.25,2.5,4.0,0.0,0.0,0.0,0.75,2.2,3.5,0.0,1.4166666666666667,0.5,0.0,0.5,3.090909090909091,0.0,0.0,2.6923076923076925,0.6666666666666666,4.0,0.0,0.6428571428571429,1.6153846153846154,0.0,2.4,0.0,2.25,0.6,2.272727272727273,0.6666666666666666,0.5,0.0,2.6666666666666665,2.8461538461538463,3.375,1.375,1.2,0.0,0.5,0.0,4.0,0.0,4.0,0.0,0.2307692307692307,0.0,1.5,4.0,1.4615384615384617,3.071428571428572,0.0,0.0,3.6666666666666665,2.75,1.2857142857142858,0.6666666666666666,0.0,3.142857142857143,0.0,0.25,2.3529411764705883,0.0,3.5,1.9375,0.3846153846153846,3.333333333333333,2.875,3.25,2.3076923076923075,3.5,0.0,2.0,1.0,3.5,3.230769230769231,0.6666666666666666,3.076923076923077,0.3333333333333333,0.0,2.0,0.0,0.0,4.0,0.25,0.0,0.75,0.5,4.0,0.3076923076923077,0.0,3.357142857142857,0.0,2.0,1.75,1.3333333333333333,3.8,3.7857142857142856,2.176470588235294,3.75,0.6923076923076923,1.3636363636363635,0.0,0.2142857142857142,0.0,1.6,1.0,0.0,0.0,4.0,0.6666666666666666,3.2,0.0,2.8125,1.6666666666666667,1.1538461538461535,2.6666666666666665,0.0,2.2857142857142856,0.0,0.75,0.0,2.625,2.6,3.8,3.0,2.4375,0.0,4.0,0.8888888888888888,0.0,0.0,0.0,1.3333333333333333,0.0,0.0,1.3333333333333333,0.75,2.75,1.6666666666666667,1.8333333333333333,0.0,0.0,0.0,0.0,1.4166666666666667,3.4615384615384617,0.0,0.0,3.363636363636364,2.6666666666666665,2.6,0.5,2.25,4.0,0.25,0.0,0.75,3.4,0.0,0.8,0.0,0.0,2.5,2.0,2.2,2.4166666666666665,0.0,1.0,0.0,0.0,4.0,1.0,0.1538461538461538,0.0,1.1538461538461535,1.3333333333333333,0.5,0.6666666666666666,4.0,2.333333333333333,3.75,2.642857142857143,1.25,0.0,0.4,0.0,1.0769230769230769,2.8181818181818183,3.764705882352941,1.3333333333333333,2.375,2.4,1.0,2.5,0.0,1.0,3.2,1.5,0.75,3.75,2.9375,0.0,4.0],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"GPA_2\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"SEM_2_STATUS\"},\"categoryorder\":\"array\",\"categoryarray\":[1,0]},\"legend\":{\"title\":{\"text\":\"SEM_2_STATUS\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.box(Xy_build_c, x='GPA_2', y='SEM_2_STATUS', color='SEM_2_STATUS', boxmode=\"overlay\")\n",
        "fig.update_layout(boxmode='group')  # To group the box plots by category\n",
        "fig.update_traces(orientation='h') # horizontal box plots\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "368a8bba-07fa-4117-961b-d0663f5b8d29",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "SSsfRJEQgylN"
      },
      "source": [
        "Next compare semester 2 units completed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "eb05bb9f-f9e5-42fb-9eb9-b07f93e50e63",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "MQWcnBdjgylN",
        "outputId": "1cbbe1cb-86a9-427e-dfb2-2371dce51483"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"b07794f8-be86-479f-a45a-f1ae06351ab2\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b07794f8-be86-479f-a45a-f1ae06351ab2\")) {                    Plotly.newPlot(                        \"b07794f8-be86-479f-a45a-f1ae06351ab2\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"SEM_2_STATUS=%{y}<br>UNITS_COMPLETED_2=%{x}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\"},\"name\":\"0\",\"notched\":false,\"offsetgroup\":\"0\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[15.0,12.0,11.0,16.0,15.0,0.0,6.0,12.0,9.0,16.0,9.0,13.0,13.0,16.0,0.0,15.0,10.0,16.0,17.0,14.0,16.0,17.0,13.0,12.0,13.0,12.0,12.0,6.0,14.0,16.0,13.0,12.0,16.0,0.0,15.0,13.0,13.0,12.0,7.0,16.0,8.0,17.0,6.0,16.0,11.0,16.0,16.0,2.0,6.0,4.0,17.0,14.0,8.0,12.0,15.0,15.0,16.0,13.0,9.0,10.0,9.0,16.0,13.0,16.0,13.0,17.0,3.0,15.0,13.0,13.0,10.0,13.0,16.0,6.0,16.0,6.0,12.0,13.0,16.0,12.0,14.0,15.0,14.0,12.0,14.0,14.0,10.0,11.0,15.0,17.0,14.0,16.0,15.0,14.0,14.0,16.0,9.0,14.0,15.0,14.0,12.0,15.0,12.0,15.0,12.0,13.0,15.0,15.0,15.0,15.0,15.0,15.0,12.0,13.0,16.0,15.0,11.0,14.0,12.0,12.0,6.0,14.0,13.0,8.0,12.0,20.0,16.0,16.0,0.0,13.0,12.0,15.0,15.0,14.0,13.0,16.0,14.0,12.0,13.0,0.0,6.0,11.0,6.0,17.0,12.0,15.0,3.0,15.0,15.0,12.0,16.0,15.0,16.0,14.0,12.0,16.0,12.0,13.0,16.0,13.0,15.0,15.0,16.0,11.0,14.0,16.0,12.0,14.0,7.0,11.0,12.0,12.0,0.0,16.0,15.0,15.0,12.0,7.0,15.0,4.0,8.0,14.0,13.0,9.0,11.0,16.0,13.0,15.0,9.0,1.0,0.0,10.0,15.0,12.0,12.0,12.0,15.0,12.0,7.0,18.0,16.0,13.0,15.0,13.0,9.0,12.0,13.0,12.0,12.0,7.0,10.0,15.0,15.0,13.0,14.0,13.0,13.0,12.0,16.0,0.0,16.0,12.0,12.0,12.0,12.0,6.0,16.0,12.0,12.0,13.0,11.0,14.0,16.0,13.0,14.0,16.0,16.0,12.0,6.0,13.0,12.0,14.0,16.0,12.0,16.0,15.0,3.0,9.0,15.0,15.0,14.0,17.0,13.0,9.0,14.0,16.0,15.0,13.0,15.0,15.0,13.0,10.0,10.0,3.0,16.0,7.0,13.0,16.0,16.0,13.0,12.0,13.0,12.0,17.0,12.0,16.0,15.0,3.0,12.0,16.0,6.0,3.0,12.0,16.0,16.0,8.0,12.0,16.0,15.0,15.0,12.0,6.0,13.0,16.0,16.0,16.0,18.0,16.0,14.0,12.0,15.0,12.0,14.0,13.0,12.0,0.0,14.0,13.0,17.0,17.0,13.0,12.0,8.0,16.0,7.0,9.0,15.0,17.0,9.0,14.0,15.0,12.0,17.0,1.0,15.0,5.0,9.0,16.0,16.0,6.0,3.0,15.0,18.0,12.0,12.0,13.0,9.0,13.0,12.0,15.0,7.0,16.0,13.0,7.0,9.0,13.0,12.0,16.0,12.0,12.0,13.0,9.0,12.0,14.0,12.0,15.0,13.0,12.0,6.0,17.0,14.0,13.0,13.0,16.0,6.0,9.0,16.0,15.0,14.0,12.0,16.0,16.0,6.0,3.0,15.0,13.0,16.0,16.0,12.0,16.0,14.0,17.0,14.0,16.0,17.0,13.0,10.0,3.0,16.0,13.0,11.0,17.0,13.0,13.0,16.0,10.0,12.0,13.0,10.0,18.0,0.0,15.0,15.0,13.0,16.0,5.0,15.0,4.0,3.0,15.0,12.0,16.0,15.0,15.0,16.0,16.0,16.0,10.0,12.0,15.0,13.0,13.0,12.0,6.0,14.0,15.0,0.0,15.0,0.0,13.0,0.0,16.0,15.0,12.0,15.0,13.0,15.0,16.0,14.0,13.0,13.0,13.0,6.0,9.0,12.0,9.0,15.0,17.0,16.0,14.0,12.0,15.0,6.0,15.0,16.0,9.0,15.0,13.0,5.0,11.0,9.0,16.0,11.0,13.0,9.0,16.0,12.0,13.0,14.0,13.0,6.0,10.0,7.0,16.0,12.0,12.0,12.0,7.0,11.0,5.0,6.0,6.0,14.0,13.0,15.0,12.0,13.0,16.0,15.0,11.0,13.0,12.0,16.0,0.0,13.0,12.0,13.0,18.0,12.0,17.0,10.0,11.0,16.0,15.0,11.0,15.0,3.0,16.0,11.0,14.0,13.0,12.0,7.0,15.0,15.0,10.0,14.0,13.0,15.0,15.0,14.0,17.0,10.0,18.0,6.0,14.0,15.0,15.0,16.0,12.0,12.0,14.0,15.0,13.0,16.0,13.0,15.0,3.0,9.0,10.0,16.0,3.0,15.0,11.0,14.0,9.0,12.0,17.0,12.0,11.0,13.0,14.0,13.0,16.0,14.0,12.0,15.0,7.0,9.0,13.0,15.0,17.0,15.0,12.0,13.0,12.0,13.0,6.0,11.0,14.0,15.0,9.0,16.0,17.0,14.0,16.0,12.0,11.0,13.0,13.0,14.0,10.0,15.0,12.0,12.0,15.0,17.0,15.0,4.0,15.0,12.0,13.0,13.0,15.0,15.0,11.0,15.0,11.0,4.0,6.0,13.0,16.0,12.0,3.0,16.0,13.0,16.0,6.0,13.0,12.0,15.0,12.0,7.0,15.0,13.0,15.0,12.0,13.0,12.0,9.0,13.0,13.0,14.0,14.0,3.0,16.0,12.0,17.0,13.0,13.0,16.0,3.0,15.0,11.0,14.0,13.0,17.0,0.0,8.0,3.0,12.0,12.0,9.0,10.0,9.0,13.0,13.0,15.0,15.0,10.0,13.0,14.0,9.0,9.0,14.0,12.0,13.0,15.0,17.0,11.0,12.0,15.0,12.0,16.0,16.0,14.0,15.0,12.0,12.0,9.0,16.0,7.0,13.0,13.0,12.0,11.0,15.0,15.0,17.0,15.0,13.0,15.0,15.0,11.0,12.0,15.0,0.0,12.0,16.0,16.0,18.0,12.0,13.0,13.0,12.0,13.0,17.0,16.0,16.0,15.0,13.0,13.0,13.0,13.0,15.0,12.0,12.0,16.0,17.0,0.0,15.0,16.0,13.0,12.0,9.0,13.0,15.0,12.0,16.0,15.0,14.0,11.0,17.0,16.0,16.0,12.0,10.0,15.0,16.0,13.0,13.0,15.0,12.0,17.0,13.0,6.0,10.0,14.0,14.0,15.0,12.0,15.0,13.0,12.0,6.0,15.0,7.0,10.0,6.0,16.0,9.0,16.0,0.0,12.0,12.0,12.0,15.0,13.0,14.0,10.0,12.0,15.0,12.0,16.0,11.0,10.0,15.0,12.0,9.0,15.0,17.0,7.0,16.0,10.0,14.0,13.0,17.0,16.0,12.0,7.0,9.0,15.0,13.0,13.0,6.0,16.0,12.0,15.0,14.0,12.0,16.0,12.0,16.0,9.0,12.0,16.0,4.0,13.0,9.0,0.0,13.0,17.0,9.0,14.0,12.0,14.0,10.0,17.0,15.0,12.0,12.0,12.0,17.0,16.0,9.0,15.0,14.0,8.0,16.0,7.0,15.0,10.0,15.0,15.0,16.0,13.0,12.0,0.0,13.0,16.0,13.0,14.0,14.0,14.0,9.0,3.0,6.0,8.0,13.0,15.0,15.0,15.0,16.0,13.0,11.0,16.0,16.0,13.0,13.0,16.0,12.0,16.0,14.0,15.0,0.0,16.0,12.0,8.0,16.0,17.0,13.0,10.0,15.0,15.0,3.0,12.0,15.0,10.0,15.0,12.0,17.0,16.0,8.0,11.0,15.0,14.0,13.0,16.0,13.0,12.0,8.0,12.0,12.0,16.0,12.0,12.0,16.0,12.0,14.0,12.0,12.0,10.0,13.0,9.0,13.0,16.0,8.0,12.0,15.0,16.0,12.0,14.0,14.0,12.0,13.0,10.0,12.0,13.0,13.0,8.0,16.0,15.0,14.0,6.0,16.0,13.0,12.0,13.0,3.0,6.0,10.0,12.0,4.0,13.0,14.0,9.0,13.0,9.0,16.0,12.0,13.0,16.0,13.0,15.0,17.0,10.0,13.0,7.0,12.0,13.0,15.0,10.0,7.0,9.0,14.0,14.0,6.0,9.0,14.0,12.0,15.0,13.0,9.0,12.0,10.0,3.0,3.0,14.0,15.0,15.0,16.0,17.0,10.0,9.0,16.0,0.0,15.0,15.0,0.0,11.0,16.0,15.0,11.0,9.0,0.0,13.0,15.0,15.0,12.0,16.0,13.0,7.0,10.0,17.0,12.0,15.0,7.0,3.0,12.0,10.0,18.0,13.0,6.0,15.0,2.0,14.0,15.0,14.0,13.0,16.0,13.0,15.0,15.0,12.0,0.0,15.0,15.0,15.0,12.0,14.0,13.0,13.0,13.0,16.0,13.0,16.0,15.0,14.0,0.0,12.0,15.0,12.0,15.0,13.0,13.0,0.0,15.0,15.0,15.0,16.0,3.0,15.0,9.0,15.0,9.0,7.0,11.0,6.0,16.0,15.0,12.0,13.0,18.0,13.0,17.0,13.0,15.0,12.0,15.0,7.0,12.0,12.0,12.0,15.0,14.0,6.0,16.0,14.0,12.0,16.0,13.0,13.0,16.0,9.0,17.0,16.0,16.0,15.0,13.0,14.0,15.0,14.0,18.0,15.0,13.0,16.0,15.0,12.0,15.0,12.0,18.0,12.0,16.0,13.0,15.0,15.0,12.0,12.0,13.0,16.0,12.0,15.0,12.0,9.0,11.0,13.0,15.0,13.0,15.0,9.0,15.0,13.0,13.0,14.0,14.0,15.0,13.0,6.0,15.0,14.0,16.0,12.0,11.0,13.0,11.0,13.0,15.0,4.0,12.0,12.0,16.0,15.0,12.0,0.0,12.0,16.0,18.0,15.0,6.0,12.0,15.0,6.0,13.0,12.0,16.0,17.0,16.0,14.0,12.0,9.0,13.0,11.0,16.0,15.0,12.0,14.0,15.0,15.0,12.0,11.0,16.0,15.0,12.0,13.0,6.0,12.0,0.0,16.0,15.0,15.0,13.0,16.0,6.0,12.0,12.0,12.0,12.0,15.0,3.0,17.0,17.0,13.0,13.0,13.0,3.0,6.0,9.0,3.0,14.0,18.0,12.0,13.0,12.0,15.0,15.0,17.0,12.0,13.0,13.0,12.0,13.0,0.0,16.0,12.0,16.0,8.0,16.0,14.0,12.0,0.0,15.0,13.0,10.0,14.0,15.0,15.0,15.0,0.0,15.0,13.0,13.0,9.0,12.0,7.0,13.0,6.0,9.0,16.0,15.0,15.0,13.0,12.0,15.0,12.0,10.0,13.0,13.0,14.0,13.0,12.0,16.0,16.0,12.0,14.0,15.0,17.0,3.0,13.0,16.0,12.0,15.0,14.0,13.0,15.0,0.0,12.0,14.0,12.0,12.0,12.0,16.0,10.0,12.0,11.0,17.0,16.0,9.0,15.0,10.0,12.0,14.0,12.0,13.0,9.0,3.0,12.0,17.0,16.0,13.0,3.0,12.0,10.0,15.0,15.0,17.0,12.0,13.0,16.0,14.0,15.0,3.0,12.0,14.0,13.0,9.0,11.0,15.0,12.0,16.0,3.0,15.0,12.0,13.0,12.0,15.0,15.0,15.0,15.0,17.0,6.0,6.0,13.0,5.0,10.0,13.0,14.0,16.0,15.0,15.0,0.0,16.0,15.0,13.0,8.0,15.0,12.0,13.0,16.0,6.0,10.0,15.0,15.0,13.0,16.0,18.0,16.0,12.0,8.0,12.0,12.0,15.0,10.0,13.0,15.0,13.0,13.0,9.0,15.0,12.0,14.0,14.0,9.0,6.0,12.0,13.0,8.0,15.0,16.0,12.0,13.0,14.0,13.0,14.0,12.0,11.0,12.0,15.0,1.0,15.0,15.0,12.0,16.0,15.0,14.0,13.0,0.0,6.0,15.0,16.0,4.0,16.0,12.0,15.0,15.0,17.0,15.0,13.0,13.0,13.0,15.0,13.0,9.0,12.0,12.0,15.0,13.0,13.0,6.0,7.0,4.0,12.0,0.0,13.0,9.0,12.0,12.0,8.0,12.0,13.0,15.0,8.0,9.0,13.0,13.0,14.0,16.0,16.0,14.0,9.0,16.0,17.0,11.0,16.0,13.0,13.0,16.0,15.0,15.0,13.0,13.0,13.0,16.0,10.0,13.0,13.0,15.0,16.0,13.0,12.0,15.0,15.0,15.0,18.0,17.0,13.0,3.0,15.0,14.0,15.0,15.0,15.0,10.0,13.0,14.0,3.0,15.0,12.0,14.0,15.0,9.0,3.0,15.0,16.0,12.0,12.0,0.0,15.0,17.0,12.0,13.0,13.0,13.0,13.0,13.0,14.0,13.0,12.0,12.0,12.0,18.0,13.0,9.0,0.0,14.0,15.0,9.0,9.0,7.0,13.0,16.0,12.0,16.0,12.0,11.0,13.0,9.0,15.0,12.0,12.0,16.0,13.0,3.0,10.0,13.0,15.0,14.0,11.0,16.0,16.0,17.0,11.0,12.0,9.0,15.0,13.0,10.0,9.0,14.0,12.0,7.0,15.0,13.0,0.0,6.0,16.0,0.0,9.0,14.0,15.0,13.0,13.0,16.0,0.0,15.0,15.0,16.0,16.0,8.0,9.0,12.0,13.0,7.0,6.0,14.0,10.0,12.0,16.0,9.0,13.0,16.0,9.0,13.0,7.0,14.0,13.0,15.0,12.0,12.0,6.0,16.0,12.0,15.0,13.0,6.0,16.0,14.0,15.0,11.0,15.0,13.0,0.0,13.0,16.0,15.0,3.0,16.0,14.0,12.0,16.0,12.0,12.0,0.0,3.0,17.0,14.0,16.0,9.0,10.0,10.0,12.0,14.0,12.0,12.0,14.0,15.0,12.0,13.0,3.0,13.0,18.0,16.0,8.0,6.0,9.0,13.0,16.0,10.0,13.0,15.0,9.0,14.0,18.0,13.0,10.0,18.0,12.0,16.0,0.0,12.0,15.0,7.0,15.0,16.0,12.0,9.0,13.0,13.0,15.0,3.0,10.0,14.0,15.0,9.0,9.0,16.0,0.0,13.0,13.0,15.0,14.0,15.0,16.0,14.0,15.0,12.0,18.0,15.0,12.0,12.0,16.0,12.0,13.0,17.0,15.0,15.0,13.0,9.0,15.0,10.0,15.0,16.0,14.0,12.0,16.0,14.0,15.0,16.0,14.0,12.0,13.0,12.0,15.0,11.0,13.0,15.0,15.0,18.0,14.0,15.0,14.0,18.0,13.0,11.0,12.0,12.0,9.0,13.0,13.0,13.0,13.0,16.0,16.0,9.0,12.0,11.0,12.0,16.0,10.0,16.0,15.0,16.0,15.0,16.0,15.0,13.0,12.0,12.0,15.0,12.0,16.0,9.0,15.0,17.0,15.0,15.0,15.0,10.0,15.0,16.0,13.0,9.0,13.0,7.0,15.0,16.0,13.0,13.0,16.0,12.0,13.0,8.0,7.0,14.0,13.0,12.0,13.0,3.0,10.0,9.0,17.0,7.0,10.0,12.0,12.0,12.0,0.0,15.0,8.0,18.0,16.0,14.0,15.0,14.0,17.0,15.0,14.0,14.0,14.0,16.0,17.0,13.0,15.0,12.0,15.0,10.0,14.0,13.0,12.0,13.0,15.0,9.0,15.0,0.0,12.0,16.0,12.0,13.0,15.0,9.0,13.0,15.0,14.0,10.0,13.0,18.0,12.0,14.0,19.0,3.0,15.0,13.0,18.0,16.0,7.0,15.0,12.0,14.0,10.0,16.0,12.0,13.0,17.0,16.0,13.0,7.0,12.0,12.0,10.0,13.0,8.0,12.0,13.0,15.0,10.0,15.0,15.0,3.0,7.0,12.0,15.0,16.0,6.0,16.0,15.0,15.0,15.0,15.0,15.0,9.0,18.0,13.0,15.0,12.0,12.0,16.0,13.0,12.0,0.0,12.0,12.0,13.0,15.0,13.0,15.0,0.0,9.0,15.0,12.0,6.0,15.0,13.0,17.0,12.0,15.0,12.0,13.0,12.0,12.0,10.0,13.0,6.0,0.0,12.0,16.0,18.0,13.0,16.0,13.0,13.0,14.0,12.0,14.0,13.0,15.0,14.0,15.0,13.0,12.0,13.0,16.0,15.0,14.0,16.0,15.0,3.0,9.0,15.0,14.0,14.0,16.0,15.0,15.0,13.0,15.0,12.0,12.0,14.0,11.0,12.0,0.0,14.0,9.0,14.0,5.0,12.0,15.0,14.0,17.0,11.0,6.0,13.0,19.0,9.0,17.0,14.0,15.0,16.0,3.0,10.0,12.0,15.0,7.0,15.0,13.0,13.0,15.0,16.0,6.0,3.0,16.0,15.0,15.0,16.0,16.0,14.0,13.0,18.0,12.0,13.0,15.0,15.0,16.0,9.0,13.0,15.0,12.0,13.0,16.0,14.0,6.0,17.0,13.0,18.0,10.0,4.0,13.0,0.0,15.0,14.0,16.0,13.0,6.0,17.0,14.0,14.0,13.0,13.0,16.0,0.0,15.0,12.0,12.0,10.0,15.0,13.0,13.0,16.0,6.0,17.0,8.0,15.0,12.0,16.0,12.0,0.0,15.0,18.0,9.0,12.0,3.0,12.0,15.0,17.0,3.0,15.0,16.0,17.0,15.0,13.0,12.0,0.0,4.0,17.0,9.0,14.0,7.0,16.0,7.0,18.0,15.0,12.0,15.0,12.0,16.0,13.0,6.0,13.0,14.0,14.0,10.0,16.0,15.0,13.0,13.0,12.0,17.0,16.0,14.0,11.0,16.0,14.0,14.0,12.0,13.0,15.0,16.0,16.0,15.0,13.0,15.0,15.0,14.0,15.0,15.0,8.0,15.0,15.0,15.0,16.0,12.0,13.0,9.0,13.0,16.0,13.0,11.0,4.0,16.0,16.0,15.0,16.0,17.0,13.0,16.0,15.0,16.0,15.0,13.0,17.0,0.0,16.0,12.0,14.0,15.0,14.0,12.0,14.0,13.0,12.0,11.0,14.0,13.0,11.0,15.0,14.0,15.0,6.0,12.0,18.0,15.0,16.0,13.0,15.0,12.0,15.0,12.0,16.0,15.0,13.0,15.0,16.0,12.0,16.0,10.0,12.0,15.0,9.0,6.0,15.0,12.0,13.0,16.0,16.0,15.0,6.0,8.0,6.0,12.0,12.0,14.0,16.0,12.0,13.0,14.0,1.0,12.0,9.0,0.0,12.0,0.0,8.0,15.0,16.0,15.0,13.0,12.0,11.0,12.0,13.0,16.0,0.0,12.0,10.0,14.0,14.0,17.0,16.0,17.0,13.0,12.0,15.0,16.0,16.0,15.0,15.0,13.0,13.0,14.0,16.0,8.0,14.0,15.0,10.0,13.0,12.0,14.0,12.0,15.0,16.0,14.0,12.0,18.0,16.0,12.0,13.0,9.0,16.0,15.0,12.0,3.0,15.0,12.0,12.0,14.0,17.0,15.0,7.0,7.0,13.0,18.0,13.0,12.0,14.0,15.0,9.0,15.0,15.0,0.0,6.0,16.0,12.0,16.0,16.0,9.0,14.0,12.0,16.0,9.0,12.0,6.0,14.0,3.0,18.0,9.0,17.0,12.0,2.0,0.0,3.0,13.0,15.0,12.0,18.0,14.0,13.0,16.0,9.0,15.0,9.0,10.0,0.0,12.0,14.0,16.0,13.0,16.0,0.0,15.0,15.0,13.0,13.0,15.0,13.0,14.0,16.0,18.0,16.0,15.0,14.0,13.0,16.0,9.0,13.0,13.0,12.0,15.0,12.0,16.0,15.0,14.0,15.0,16.0,16.0,10.0,12.0,17.0,3.0,12.0,13.0,16.0,15.0,14.0,15.0,12.0,18.0,13.0,9.0,13.0,15.0,14.0,15.0,12.0,15.0,14.0,14.0,15.0,13.0,14.0,0.0,15.0,16.0,0.0,14.0,15.0,12.0,12.0,14.0,15.0,16.0,12.0,9.0,15.0,12.0,12.0,17.0,15.0,16.0,10.0,14.0,16.0,6.0,16.0,14.0,15.0,12.0,12.0,16.0,15.0,17.0,15.0,16.0,9.0,13.0,12.0,18.0,14.0,13.0,12.0,15.0,18.0,13.0,14.0,5.0,12.0,15.0,15.0,14.0,1.0,10.0,12.0,15.0,16.0,16.0,6.0,12.0,12.0,12.0,16.0,12.0,13.0,13.0,15.0,17.0,14.0,16.0,12.0,14.0,12.0,17.0,12.0,16.0,12.0,15.0,11.0,14.0,13.0,16.0,10.0,13.0,16.0,13.0,16.0,16.0,9.0,16.0,13.0,12.0,16.0,9.0,15.0,16.0,10.0,18.0,12.0,15.0,15.0,16.0,12.0,13.0,3.0,15.0,13.0,15.0,16.0,16.0,1.0,9.0,15.0,10.0,12.0,16.0,15.0,12.0,16.0,3.0,16.0,15.0,13.0,12.0,6.0,14.0,16.0,13.0,15.0,15.0,17.0,13.0,12.0,13.0,9.0,18.0,0.0,16.0,11.0,12.0,15.0,15.0,16.0,15.0,12.0,10.0,3.0,16.0,14.0,9.0,14.0,6.0,13.0,16.0,13.0,13.0,16.0,14.0,16.0,7.0,15.0,14.0,14.0,6.0,12.0,16.0,11.0,12.0,9.0,15.0,13.0,13.0,14.0,15.0,10.0,7.0,17.0,14.0,15.0,12.0,15.0,10.0,17.0,15.0,12.0,17.0,15.0,17.0,14.0,13.0,13.0,17.0,13.0,11.0,12.0,15.0,21.0,18.0,18.0,16.0,12.0,14.0,13.0,13.0,13.0,14.0,17.0,2.0,13.0,15.0,15.0,12.0,13.0,9.0,12.0,12.0,9.0,16.0,13.0,11.0,12.0,15.0,16.0,17.0,14.0,17.0,15.0,12.0,6.0,18.0,16.0,16.0,12.0,13.0,14.0,18.0,13.0,12.0,15.0,15.0,15.0,12.0,12.0,13.0,12.0,4.0,15.0,16.0,14.0,17.0,13.0,13.0,12.0,16.0,13.0,14.0,0.0,15.0,15.0,12.0,15.0,14.0,14.0,16.0,12.0,16.0,7.0,12.0,10.0,4.0,14.0,16.0,16.0,0.0,14.0,15.0,16.0,16.0,12.0,10.0,12.0,12.0,13.0,15.0,0.0,13.0,15.0,15.0,12.0,10.0,12.0,3.0,3.0,16.0,12.0,12.0,12.0,15.0,9.0,14.0,15.0,13.0,14.0,15.0,16.0,14.0,15.0,15.0,12.0,14.0,6.0,16.0,10.0,16.0,16.0,15.0,13.0,14.0,15.0,12.0,14.0,13.0,12.0,15.0,12.0,12.0,16.0,10.0,15.0,16.0,12.0,15.0,6.0,12.0,15.0,9.0,14.0,16.0,12.0,12.0,15.0,9.0,13.0,13.0,16.0,12.0,10.0,12.0,9.0,9.0,6.0,16.0,17.0,12.0,7.0,16.0,18.0,7.0,13.0,12.0,16.0,15.0,6.0,16.0,12.0,12.0,7.0,16.0,0.0,12.0,15.0,14.0,14.0,14.0,13.0,7.0,6.0,3.0,3.0,14.0,10.0,10.0,13.0,12.0,11.0,15.0,14.0,12.0,12.0,12.0,15.0,16.0,13.0,16.0,13.0,12.0,12.0,6.0,14.0,9.0,15.0,15.0,13.0,10.0,3.0,15.0,9.0,9.0,12.0,14.0,0.0,6.0,13.0,13.0,6.0,3.0,13.0,10.0,15.0,8.0,13.0,15.0,15.0,16.0,16.0,13.0,15.0,16.0,16.0,12.0,15.0,13.0,13.0,12.0,6.0,12.0,16.0,15.0,7.0,4.0,13.0,10.0,12.0,3.0,14.0,2.0,16.0,13.0,13.0,12.0,12.0,16.0,14.0,15.0,12.0,15.0,6.0,5.0,12.0,13.0,12.0,12.0,15.0,14.0,12.0,7.0,11.0,13.0,15.0,6.0,12.0,3.0,17.0,12.0,12.0,15.0,16.0,14.0,12.0,13.0,10.0,12.0,17.0,11.0,14.0,0.0,9.0,15.0,13.0,12.0,16.0,13.0,15.0,15.0,15.0,12.0,12.0,12.0,15.0,12.0,17.0,16.0,16.0,16.0,13.0,8.0,19.0,15.0,13.0,14.0,12.0,18.0,12.0,12.0,4.0,13.0,14.0,9.0,13.0,3.0,15.0,3.0,14.0,15.0,16.0,16.0,12.0,12.0,10.0,13.0,15.0,8.0,3.0,12.0,16.0,17.0,15.0,13.0,0.0,6.0,15.0,7.0,16.0,17.0,12.0,14.0,16.0,15.0,9.0,14.0,13.0,10.0,13.0,13.0,17.0,16.0,15.0,6.0,12.0,11.0,16.0,13.0,13.0,8.0,14.0,15.0,0.0,13.0,12.0,13.0,16.0,16.0,16.0,18.0,16.0,16.0,15.0,18.0,15.0,16.0,16.0,14.0,14.0,11.0,16.0,9.0,15.0,16.0,15.0,9.0,0.0,13.0,9.0,12.0,16.0,15.0,12.0,16.0,13.0,16.0,0.0,15.0,15.0,7.0,15.0,13.0,12.0,9.0,14.0,16.0,6.0,15.0,17.0,14.0,12.0,14.0,15.0,6.0,14.0,13.0,10.0,13.0,16.0,7.0,5.0,12.0,15.0,12.0,13.0,12.0,13.0,16.0,15.0,15.0,12.0,12.0,17.0,13.0,15.0,6.0,9.0,13.0,13.0,11.0,17.0,15.0,11.0,9.0,13.0,13.0,14.0,16.0,6.0,17.0,15.0,12.0,12.0,12.0,18.0,16.0,0.0,17.0,14.0,6.0,6.0,15.0,10.0,15.0,16.0,13.0,12.0,12.0,12.0,15.0,9.0,16.0,13.0,9.0,10.0,16.0,14.0,12.0,12.0,6.0,14.0,13.0,16.0,15.0,14.0,16.0,13.0,10.0,16.0,13.0,0.0,0.0,15.0,11.0,15.0,0.0,15.0,10.0,13.0,16.0,16.0,9.0,13.0,0.0,13.0,15.0,12.0,12.0,15.0,12.0,14.0,14.0,16.0,16.0,13.0,10.0,12.0,9.0,15.0,15.0,14.0,12.0,7.0,6.0,15.0,16.0,10.0,15.0,16.0,13.0,13.0,12.0,16.0,15.0,12.0,18.0,12.0,14.0,18.0,4.0,12.0,14.0,16.0,16.0,16.0,14.0,15.0,13.0,16.0,12.0,14.0,10.0,0.0,13.0,15.0,13.0,15.0,0.0,13.0,14.0,14.0,16.0,16.0,16.0,13.0,5.0,6.0,12.0,13.0,7.0,16.0,13.0,14.0,9.0,16.0,16.0,14.0,16.0,15.0,15.0,13.0,16.0,16.0,0.0,15.0,9.0,17.0,16.0,9.0,15.0,13.0,17.0,15.0,9.0,11.0,8.0,12.0,15.0,11.0,15.0,9.0,12.0,13.0,12.0,16.0,16.0,15.0,6.0,12.0,13.0,15.0,12.0,12.0,14.0,15.0,15.0,13.0,15.0,15.0,16.0,15.0,8.0,15.0,15.0,13.0,12.0,13.0,17.0,5.0,12.0,16.0,15.0,12.0,15.0,13.0,14.0,15.0,14.0,17.0,13.0,13.0,13.0,12.0,13.0,13.0,4.0,16.0,16.0,16.0,7.0,10.0,12.0,12.0,15.0,16.0,3.0,13.0,13.0,16.0,18.0,3.0,15.0,12.0,9.0,9.0,15.0,15.0,13.0,6.0,17.0,12.0,12.0,15.0,16.0,16.0,18.0,16.0,13.0,0.0,15.0,17.0,3.0,13.0,10.0,3.0,11.0,16.0,13.0,15.0,16.0,15.0,13.0,12.0,10.0,13.0,10.0,13.0,10.0,13.0,18.0,1.0,18.0,14.0,10.0,12.0,13.0,15.0,14.0,17.0,12.0,15.0,13.0,13.0,13.0,9.0,12.0,15.0,13.0,15.0,13.0,12.0,17.0,15.0,0.0,16.0,15.0,12.0,9.0,10.0,15.0,14.0,16.0,13.0,12.0,12.0,12.0,15.0,3.0,15.0,12.0,12.0,15.0,16.0,13.0,15.0,8.0,14.0,14.0,11.0,12.0,13.0,16.0,15.0,12.0,16.0,16.0,13.0,16.0,15.0,6.0,15.0,12.0,12.0,18.0,8.0,0.0,15.0,13.0,15.0,1.0,10.0,12.0,18.0,0.0,12.0,14.0,15.0,16.0,16.0,18.0,10.0,12.0,15.0,15.0,13.0,13.0,14.0,13.0,15.0,16.0,13.0,15.0,12.0,14.0,15.0,15.0,16.0,10.0,3.0,10.0,12.0,19.0,0.0,14.0,10.0,14.0,14.0,13.0,14.0,10.0,10.0,15.0,12.0,14.0,13.0,16.0,11.0,12.0,14.0,12.0,15.0,17.0,16.0,9.0,12.0,12.0,14.0,15.0,13.0,16.0,12.0,15.0,16.0,12.0,12.0,12.0,6.0,15.0,13.0,15.0,16.0,14.0,9.0,7.0,12.0,12.0,13.0,6.0,14.0,9.0,17.0,13.0,16.0,14.0,12.0,12.0,13.0,14.0,13.0,9.0,15.0,14.0,6.0,12.0,16.0,14.0,13.0,12.0,12.0,12.0,12.0,15.0,9.0,0.0,15.0,15.0,12.0,17.0,13.0,9.0,12.0,13.0,2.0,12.0,11.0,13.0,16.0,13.0,15.0,12.0,15.0,6.0,15.0,10.0,7.0,13.0,16.0,14.0,0.0,16.0,8.0,12.0,16.0,12.0,18.0,6.0,13.0,15.0,13.0,13.0,13.0,13.0,7.0,3.0,16.0,16.0,12.0,16.0,9.0,12.0,15.0,15.0,14.0,9.0,14.0,10.0,0.0,16.0,16.0,12.0,12.0,16.0,13.0,15.0,10.0,13.0,15.0,10.0,10.0,12.0,15.0,16.0,10.0,16.0,12.0,15.0,16.0,17.0,11.0,13.0,15.0,13.0,0.0,12.0,13.0,16.0,16.0,9.0,12.0,18.0,15.0,15.0,15.0,15.0,10.0,16.0,16.0,13.0,9.0,15.0,12.0,16.0,15.0,11.0,13.0,13.0,9.0,16.0,13.0,0.0,15.0,12.0,14.0,13.0,16.0,13.0,12.0,15.0,12.0,14.0,15.0,13.0,13.0,13.0,3.0,8.0,12.0,12.0,11.0,11.0,14.0,12.0,8.0,12.0,12.0,5.0,16.0,17.0,9.0,4.0,8.0,13.0,16.0,13.0,15.0,3.0,16.0,17.0,14.0,8.0,13.0,14.0,15.0,13.0,15.0,6.0,12.0,13.0,0.0,15.0,14.0,13.0,15.0,15.0,14.0,13.0,12.0,13.0,11.0,9.0,15.0,18.0,6.0,12.0,12.0,16.0,18.0,15.0,14.0,15.0,17.0,16.0,12.0,16.0,0.0,17.0,10.0,13.0,16.0,11.0,3.0,13.0,13.0,12.0,16.0,14.0,12.0,16.0,15.0,12.0,15.0,16.0,14.0,16.0,15.0,6.0,11.0,17.0,9.0,13.0,16.0,13.0,16.0,15.0,15.0,9.0,12.0,16.0,12.0,15.0,0.0,14.0,7.0,16.0,16.0,13.0,15.0,13.0,12.0,15.0,10.0,9.0,12.0,14.0,12.0,16.0,12.0,17.0,13.0,12.0,9.0,15.0,6.0,7.0,13.0,10.0,15.0,17.0,12.0,11.0,15.0,12.0,3.0,16.0,13.0,5.0,12.0,12.0,14.0,12.0,9.0,12.0,13.0,16.0,16.0,12.0,13.0,3.0,13.0,9.0,10.0,15.0,8.0,18.0,13.0,6.0,7.0,12.0,14.0,16.0,13.0,0.0,15.0,14.0,16.0,10.0,13.0,12.0,15.0,12.0,11.0,16.0,16.0,12.0,10.0,13.0,19.0,16.0,12.0,16.0,13.0,10.0,9.0,15.0,10.0,12.0,15.0,12.0,12.0,10.0,13.0,14.0,16.0,12.0,13.0,14.0,14.0,12.0,16.0,16.0,14.0,12.0,13.0,13.0,15.0,14.0,4.0,17.0,14.0,14.0,13.0,15.0,15.0,12.0,4.0,17.0,12.0,15.0,14.0,1.0,12.0,15.0,16.0,4.0,6.0,13.0,14.0,14.0,18.0,10.0,16.0,12.0,15.0,16.0,0.0,14.0,0.0,10.0,13.0,15.0,4.0,16.0,13.0,12.0,7.0,0.0,14.0,13.0,14.0,9.0,4.0,6.0,3.0,13.0,14.0,14.0,16.0,13.0,15.0,6.0,15.0,7.0,15.0,14.0,17.0,6.0,13.0,5.0,15.0,14.0,13.0,12.0,15.0,10.0,15.0,12.0,12.0,9.0,15.0,13.0,13.0,15.0,12.0,15.0,17.0,12.0,16.0,9.0,10.0,0.0,13.0,12.0,4.0,12.0,16.0,11.0,15.0,16.0,3.0,12.0,13.0,16.0,15.0,9.0,16.0,15.0,15.0,13.0,18.0,16.0,12.0,9.0,10.0,9.0,4.0,9.0,10.0,13.0,15.0,12.0,21.0,7.0,16.0,9.0,15.0,17.0,17.0,3.0,15.0,13.0,12.0,12.0,12.0,16.0,9.0,13.0,13.0,18.0,14.0,12.0,16.0,10.0,13.0,10.0,14.0,12.0,14.0,14.0,10.0,13.0,10.0,8.0,13.0,11.0,9.0,16.0,10.0,14.0,15.0,15.0,12.0,17.0,3.0,16.0,15.0,12.0,15.0,3.0,14.0,9.0,9.0,16.0,13.0,6.0,16.0,3.0,15.0,12.0,15.0,18.0,16.0,6.0,12.0,16.0,12.0,15.0,13.0,14.0,16.0,15.0,15.0,15.0,9.0,15.0,3.0,15.0,14.0,16.0,0.0,12.0,18.0,15.0,15.0,15.0,10.0,15.0,6.0,17.0,14.0,13.0,6.0,14.0,12.0,11.0,15.0,9.0,15.0,6.0,13.0,19.0,12.0,16.0,15.0,13.0,14.0,8.0,7.0,11.0,15.0,14.0,15.0,15.0,13.0,15.0,16.0,15.0,13.0,12.0,15.0,7.0,4.0,17.0,6.0,10.0,12.0,17.0,12.0,15.0,15.0,10.0,10.0,14.0,13.0,15.0,12.0,11.0,17.0,9.0,8.0,14.0,16.0,12.0,15.0,16.0,6.0,16.0,16.0,4.0,14.0,14.0,12.0,16.0,15.0,17.0,13.0,17.0,15.0,9.0,16.0,15.0,16.0,9.0,10.0,8.0,12.0,12.0,16.0,7.0,13.0,15.0,14.0,13.0,12.0,11.0,13.0,12.0,16.0,17.0,14.0,17.0,13.0,16.0,9.0,15.0,16.0,13.0,12.0,12.0,9.0,16.0,15.0,13.0,10.0,12.0,12.0,15.0,13.0,4.0,12.0,12.0,14.0,16.0,0.0,16.0,13.0,9.0,15.0,13.0,15.0,11.0,7.0,16.0,9.0,12.0,15.0,11.0,9.0,5.0,12.0,14.0,9.0,10.0,17.0,7.0,11.0,15.0,9.0,13.0,16.0,15.0,15.0,16.0,15.0,0.0,3.0,12.0,13.0,0.0,9.0,6.0,6.0,12.0,13.0,12.0,12.0,15.0,13.0,9.0,16.0,11.0,9.0,12.0,9.0,3.0,11.0,18.0,16.0,16.0,13.0,0.0,0.0,12.0,15.0,3.0,16.0,15.0,13.0,12.0,15.0,12.0,12.0,15.0,11.0,15.0,15.0,16.0,12.0,17.0,14.0,16.0,10.0,14.0,15.0,13.0,13.0,9.0,15.0,13.0,15.0,13.0,14.0,12.0,14.0,15.0,12.0,7.0,16.0,10.0,13.0,8.0,9.0,14.0,16.0,15.0,12.0,7.0,15.0,16.0,17.0,14.0,16.0,13.0,15.0,15.0,12.0,15.0,10.0,13.0,16.0,14.0,13.0,12.0,16.0,16.0,14.0,10.0,13.0,13.0,12.0,9.0,15.0,16.0,12.0,15.0,12.0,0.0,13.0,14.0,16.0,15.0,12.0,16.0,15.0,12.0,16.0,9.0,10.0,9.0,13.0,8.0,16.0,3.0,12.0,16.0,12.0,16.0,15.0,16.0,12.0,15.0,16.0,15.0,12.0,15.0,16.0,12.0,18.0,7.0,12.0,16.0,15.0,16.0,4.0,15.0,4.0,6.0,13.0,13.0,13.0,9.0,16.0,15.0,12.0,16.0,12.0,15.0,15.0,10.0,13.0,15.0,9.0,16.0,11.0,15.0,15.0,9.0,12.0,13.0,6.0,13.0,15.0,16.0,12.0,15.0,14.0,6.0,12.0,9.0,18.0,9.0,11.0,10.0,2.0,13.0,12.0,9.0,16.0,16.0,12.0,15.0,13.0,12.0,13.0,16.0,13.0,12.0,13.0,13.0,15.0,16.0,15.0,9.0,12.0,10.0,9.0,16.0,9.0,18.0,13.0,15.0,8.0,10.0,14.0,13.0,15.0,16.0,10.0,0.0,16.0,14.0,16.0,13.0,12.0,15.0,10.0,11.0,12.0,16.0,15.0,15.0,14.0,9.0,7.0,17.0,12.0,15.0,14.0,15.0,16.0,17.0,12.0,15.0,16.0,15.0,12.0,3.0,15.0,14.0,12.0,14.0,9.0,15.0,12.0,12.0,4.0,13.0,13.0,12.0,12.0,16.0,6.0,9.0,15.0,16.0,0.0,14.0,10.0,16.0,3.0,15.0,15.0,12.0,12.0,16.0,15.0,17.0,6.0,12.0,13.0,13.0,10.0,15.0,13.0,14.0,12.0,15.0,7.0,12.0,14.0,12.0,15.0,17.0,14.0,13.0,15.0,15.0,4.0,13.0,16.0,13.0,16.0,14.0,17.0,16.0,9.0,13.0,16.0,17.0,9.0,15.0,15.0,15.0,14.0,15.0,15.0,12.0,16.0,15.0,0.0,15.0,16.0,13.0,15.0,16.0,9.0,13.0,18.0,15.0,16.0,8.0,12.0,14.0,12.0,13.0,14.0,15.0,15.0,16.0,15.0,13.0,0.0,9.0,12.0,6.0,12.0,13.0,13.0,16.0,15.0,14.0,14.0,12.0,11.0,15.0,11.0,16.0,12.0,8.0,16.0,13.0,15.0,3.0,4.0,14.0,16.0,16.0,15.0,15.0,15.0,10.0,12.0,6.0,15.0,13.0,13.0,13.0,13.0,15.0,8.0,14.0,12.0,10.0,8.0,9.0,13.0,11.0,13.0,13.0,16.0,17.0,11.0,10.0,6.0,13.0,10.0,4.0,15.0,16.0,13.0,16.0,15.0,18.0,15.0,13.0,12.0,19.0,17.0,8.0,13.0,13.0,0.0,14.0,8.0,6.0,12.0,9.0,12.0,15.0,9.0,14.0,13.0,12.0,15.0,12.0,16.0,15.0,7.0,4.0,16.0,16.0,15.0,14.0,13.0,12.0,15.0,15.0,13.0,0.0,15.0,13.0,10.0,16.0,15.0,13.0,17.0,0.0,16.0,16.0,7.0,14.0,15.0,13.0,12.0,16.0,14.0,15.0,13.0,17.0,6.0,16.0,0.0,12.0,15.0,13.0,13.0,6.0,13.0,12.0,11.0,10.0,17.0,13.0,10.0,15.0,12.0,15.0,18.0,12.0,13.0,12.0,13.0,14.0,15.0,13.0,15.0,14.0,14.0,13.0,15.0,15.0,13.0,13.0,10.0,12.0,16.0,16.0,17.0,15.0,15.0,13.0,15.0,14.0,13.0,15.0,16.0,9.0,0.0,9.0,15.0,12.0,12.0,16.0,13.0,13.0,16.0,15.0,12.0,5.0,6.0,16.0,13.0,13.0,17.0,9.0,14.0,12.0,14.0,16.0,7.0,10.0,15.0,12.0,0.0,15.0,12.0,13.0,13.0,13.0,14.0,13.0,15.0,13.0,7.0,12.0,12.0,4.0,15.0,12.0,12.0,16.0,12.0,17.0,13.0,1.0,9.0,14.0,13.0,16.0,14.0,13.0,15.0,15.0,13.0,12.0,16.0,15.0,4.0,13.0,13.0,15.0,12.0,6.0,16.0,13.0,16.0,11.0,0.0,13.0,13.0,15.0,13.0,14.0,15.0,10.0,3.0,16.0,15.0,18.0,14.0,15.0,14.0,0.0,13.0,15.0,14.0,10.0,15.0,6.0,16.0,12.0,3.0,12.0,16.0,12.0,13.0,15.0,17.0,15.0,16.0,12.0,13.0,13.0,15.0,15.0,14.0,15.0,13.0,16.0,16.0,17.0,10.0,17.0,16.0,10.0,9.0,13.0,8.0,16.0,14.0,6.0,13.0,10.0,15.0,14.0,14.0,18.0,14.0,0.0,15.0,12.0,16.0,14.0,18.0,12.0,17.0,15.0,15.0,4.0,15.0,4.0,6.0,15.0,13.0,15.0,12.0,13.0,4.0,18.0,15.0,15.0,16.0,12.0,13.0,13.0,1.0,0.0,15.0,13.0,9.0,15.0,17.0,14.0,13.0,13.0,17.0,12.0,3.0,13.0,17.0,15.0,15.0,17.0,15.0,14.0,15.0,13.0,13.0,15.0,14.0,6.0,0.0,0.0,8.0,13.0,17.0,12.0,12.0,15.0,15.0,13.0,15.0,12.0,12.0,9.0,11.0,13.0,16.0,15.0,7.0,15.0,16.0,13.0,9.0,13.0,13.0,12.0,16.0,0.0,17.0,15.0,16.0,15.0,12.0,15.0,15.0,12.0,15.0,10.0,13.0,17.0,16.0,14.0,15.0,16.0,15.0,16.0,12.0,9.0,13.0,17.0,16.0,13.0,15.0,16.0,15.0,16.0,14.0,13.0,9.0,12.0,13.0,15.0,13.0,15.0,14.0,15.0,12.0,12.0,9.0,15.0,10.0,14.0,13.0,9.0,6.0,12.0,11.0,14.0,10.0,17.0,13.0,15.0,16.0,14.0,15.0,13.0,13.0,13.0,12.0,13.0,13.0,1.0,12.0,10.0,12.0,11.0,16.0,6.0,13.0,9.0,10.0,15.0,0.0,12.0,12.0,15.0,14.0,16.0,13.0,12.0,12.0,13.0,12.0,16.0,9.0,12.0,15.0,16.0,9.0,14.0,13.0,16.0,8.0,14.0,14.0,13.0,16.0,16.0,16.0,12.0,15.0,16.0,12.0,14.0,15.0,12.0,12.0,15.0,15.0,15.0,9.0,13.0,16.0,13.0,12.0,9.0,12.0,0.0,16.0,15.0,15.0,15.0,12.0,12.0,15.0,13.0,15.0,13.0,15.0,15.0,16.0,16.0,15.0,16.0,3.0,15.0,14.0,12.0,9.0,15.0,15.0,17.0,10.0,16.0,13.0,14.0,12.0,15.0,14.0,15.0,9.0,10.0,13.0,14.0,14.0,15.0,15.0,14.0,7.0,15.0,13.0,13.0,15.0,13.0,12.0,15.0,14.0,13.0,10.0,14.0,0.0,16.0,16.0,12.0,15.0,15.0,12.0,15.0,16.0,12.0,13.0,12.0,13.0,16.0,7.0,13.0,15.0,16.0,6.0,14.0,16.0,13.0,12.0,12.0,15.0,16.0,3.0,13.0,14.0,16.0,14.0,16.0,15.0,16.0,9.0,13.0,12.0,12.0,14.0,16.0,14.0,14.0,4.0,13.0,16.0,12.0,12.0,10.0,12.0,16.0,5.0,12.0,16.0,13.0,14.0,13.0,13.0,15.0,12.0,16.0,14.0,12.0,7.0,13.0,16.0,16.0,0.0,12.0,16.0,10.0,12.0,14.0,15.0,9.0,12.0,13.0,16.0,13.0,17.0,12.0,14.0,12.0,10.0,12.0,12.0,16.0,12.0,13.0,12.0,14.0,14.0,14.0,12.0,6.0,14.0,12.0,15.0,3.0,15.0,14.0,17.0,6.0,12.0,10.0,6.0,3.0,6.0,10.0,16.0,15.0,9.0,17.0,6.0,16.0,15.0,15.0,6.0,12.0,17.0,14.0,15.0,13.0,14.0,15.0,17.0,10.0,12.0,16.0,14.0,14.0,9.0,11.0,11.0,12.0,13.0,3.0,11.0,17.0,8.0,0.0,15.0,16.0,16.0,15.0,15.0,1.0,14.0,16.0,16.0,15.0,10.0,12.0,13.0,15.0,18.0,18.0,16.0,15.0,12.0,13.0,9.0,9.0,11.0,15.0,17.0,14.0,9.0,12.0,0.0,13.0,17.0,13.0,13.0,12.0,16.0,16.0,3.0,8.0,19.0,16.0,14.0,18.0,15.0,6.0,16.0,7.0,9.0,13.0,15.0,16.0,16.0,7.0,8.0,15.0,16.0,12.0,12.0,13.0,14.0,15.0,13.0,19.0,15.0,13.0,0.0,12.0,7.0,13.0,16.0,13.0,14.0,15.0,15.0,6.0,15.0,13.0,15.0,14.0,17.0,15.0,10.0,12.0,16.0,11.0,9.0,16.0,14.0,15.0,0.0,16.0,14.0,6.0,15.0,0.0,10.0,15.0,13.0,16.0,15.0,7.0,8.0,16.0,6.0,10.0,9.0,14.0,12.0,18.0,17.0,16.0,13.0,14.0,16.0,15.0,13.0,9.0,15.0,9.0,10.0,12.0,17.0,15.0,5.0,16.0,5.0,0.0,13.0,11.0,15.0,6.0,17.0,12.0,13.0,16.0,12.0,16.0,15.0,9.0,9.0,12.0,6.0,4.0,4.0,16.0,15.0,12.0,14.0,15.0,15.0,6.0,6.0,13.0,15.0,13.0,13.0,14.0,14.0,13.0,14.0,10.0,4.0,9.0,11.0,8.0,16.0,0.0,14.0,15.0,12.0,12.0,9.0,15.0,14.0,16.0,7.0,13.0,9.0,15.0,12.0,15.0,16.0,10.0,6.0,12.0,15.0,13.0,16.0,13.0,7.0,9.0,13.0,9.0,12.0,4.0,7.0,16.0,12.0,12.0,15.0,13.0,14.0,9.0,13.0,10.0,16.0,13.0,19.0,12.0,3.0,13.0,14.0,12.0,16.0,2.0,18.0,0.0,12.0,15.0,17.0,14.0,9.0,16.0,14.0,5.0,11.0,9.0,12.0,16.0,17.0,10.0,9.0,14.0,13.0,15.0,6.0,15.0,15.0,15.0,15.0,16.0,18.0,13.0,13.0,15.0,16.0,0.0,16.0,14.0,16.0,10.0,6.0,8.0,14.0,14.0,12.0,7.0,9.0,14.0,17.0,12.0,16.0,15.0,15.0,15.0,16.0,14.0,14.0,15.0,12.0,15.0,9.0,6.0,6.0,17.0,14.0,13.0,13.0,12.0,13.0,15.0,16.0,12.0,16.0,11.0,16.0,15.0,18.0,12.0,9.0,12.0,15.0,17.0,10.0,16.0,0.0,12.0,16.0,17.0,9.0,13.0,16.0,13.0,17.0,12.0,3.0,16.0,0.0,13.0,16.0,16.0,14.0,0.0,14.0,12.0,10.0,12.0,6.0,0.0,13.0,13.0,13.0,13.0,15.0,13.0,16.0,8.0,15.0,13.0,12.0,13.0,13.0,14.0,15.0,16.0,16.0,3.0,13.0,17.0,13.0,12.0,15.0,15.0,11.0,15.0,12.0,7.0,17.0,15.0,14.0,16.0,15.0,7.0,7.0,13.0,6.0,14.0,15.0,12.0,14.0,10.0,16.0,11.0,11.0,6.0,16.0,13.0,12.0,9.0,14.0,15.0,13.0,13.0,14.0,0.0,9.0,9.0,12.0,6.0,8.0,15.0,16.0,15.0,0.0,15.0,7.0,13.0,15.0,0.0,13.0,15.0,13.0,14.0,15.0,16.0,15.0,13.0,15.0,12.0,16.0,7.0,10.0,15.0,16.0,13.0,15.0,16.0,12.0,3.0,13.0,9.0,16.0,18.0,10.0,15.0,16.0,16.0,12.0,15.0,16.0,13.0,15.0,16.0,13.0,12.0,0.0,12.0,0.0,9.0,14.0,18.0,12.0,12.0,9.0,16.0,14.0,15.0,15.0,18.0,12.0,14.0,15.0,16.0,16.0,13.0,5.0,13.0,15.0,13.0,9.0,12.0,18.0,12.0,10.0,16.0,14.0,14.0,6.0,12.0,9.0,18.0,15.0,15.0,13.0,14.0,6.0,15.0,15.0,15.0,12.0,11.0,7.0,15.0,12.0,16.0,13.0,14.0,6.0,15.0,16.0,12.0,9.0,13.0,12.0,12.0,6.0,13.0,6.0,3.0,0.0,15.0,12.0,9.0,12.0,10.0,13.0,15.0,15.0,15.0,16.0,16.0,11.0,13.0,16.0,13.0,16.0,15.0,4.0,13.0,13.0,16.0,3.0,16.0,14.0,15.0,17.0,14.0,14.0,15.0,12.0,14.0,12.0,15.0,18.0,13.0,13.0,3.0,16.0,18.0,16.0,9.0,3.0,15.0,15.0,16.0,18.0,14.0,15.0,16.0,18.0,6.0,15.0,9.0,15.0,11.0,16.0,13.0,10.0,8.0,15.0,15.0,16.0,15.0,12.0,15.0,15.0,0.0,14.0,12.0,12.0,12.0,15.0,15.0,12.0,14.0,13.0,14.0,7.0,15.0,15.0,15.0,13.0,10.0,8.0,15.0,15.0,13.0,13.0,13.0,16.0,6.0,15.0,12.0,17.0,12.0,18.0,12.0,12.0,6.0,14.0,15.0,12.0,14.0,13.0,15.0,12.0,10.0,17.0,14.0,12.0,13.0,9.0,13.0,9.0,15.0,8.0,15.0,15.0,16.0,15.0,15.0,15.0,16.0,12.0,12.0,15.0,3.0,10.0,13.0,13.0,12.0,16.0,4.0,15.0,16.0,17.0,15.0,15.0,16.0,16.0,12.0,9.0,0.0,12.0,16.0,13.0,17.0,14.0,3.0,11.0,9.0,13.0,9.0,15.0,6.0,6.0,15.0,15.0,9.0,3.0,12.0,7.0,16.0,17.0,15.0,12.0,10.0,9.0,15.0,16.0,3.0,12.0,13.0,12.0,14.0,7.0,21.0,13.0,13.0,12.0,16.0,13.0,15.0,15.0,12.0,10.0,4.0,13.0,15.0,15.0,16.0,11.0,17.0,15.0,10.0,6.0,12.0,16.0,15.0,16.0,17.0,12.0,16.0,16.0,15.0,13.0,13.0,16.0,3.0,6.0,14.0,14.0,15.0,13.0,12.0,14.0,16.0,14.0,17.0,12.0,15.0,12.0,15.0,15.0,6.0,14.0,15.0,6.0,15.0,13.0,15.0,15.0,12.0,15.0,14.0,14.0,16.0,12.0,10.0,13.0,13.0,13.0,13.0,14.0,15.0,6.0,14.0,12.0,12.0,18.0,12.0,12.0,15.0,12.0,16.0,10.0,16.0,14.0,15.0,10.0,12.0,16.0,14.0,12.0,16.0,16.0,17.0,13.0,11.0,14.0,15.0,16.0,15.0,9.0,12.0,7.0,15.0,12.0,16.0,16.0,17.0,15.0,10.0,12.0,0.0,9.0,6.0,16.0,14.0,15.0,16.0,16.0,16.0,17.0,16.0,0.0,13.0,12.0,16.0,16.0,16.0,15.0,6.0,9.0,10.0,16.0,15.0,17.0,12.0,13.0,12.0,16.0,16.0,15.0,15.0,3.0,18.0,9.0,13.0,12.0,14.0,12.0,14.0,10.0,14.0,10.0,12.0,7.0,11.0,18.0,16.0,3.0,15.0,15.0,3.0,16.0,15.0,16.0,16.0,11.0,15.0,16.0,13.0,4.0,15.0,9.0,3.0,0.0,12.0,15.0,16.0,14.0,18.0,12.0,6.0,13.0,15.0,13.0,0.0,11.0,10.0,15.0,15.0,8.0,13.0,16.0,3.0,16.0,13.0,12.0,7.0,13.0,14.0,7.0,15.0,16.0,15.0,16.0,15.0,15.0,10.0,16.0,12.0,3.0,14.0,15.0,12.0,9.0,14.0,3.0,6.0,15.0,12.0,13.0,8.0,18.0,15.0,13.0,12.0,16.0,15.0,8.0,10.0,13.0,12.0,13.0,17.0,13.0,12.0,13.0,12.0,0.0,15.0,16.0,12.0,15.0,16.0,4.0,12.0,13.0,16.0,16.0,15.0,13.0,7.0,15.0,10.0,12.0,15.0,16.0,3.0,13.0,5.0,16.0,11.0,16.0,16.0,12.0,4.0,11.0,7.0,10.0,4.0,12.0,17.0,15.0,1.0,10.0,12.0,15.0,9.0,5.0,16.0,15.0,16.0,16.0,9.0,12.0,16.0,10.0,12.0,15.0,9.0,12.0,15.0,6.0,12.0,12.0,6.0,9.0,14.0,9.0,16.0,15.0,15.0,15.0,12.0,16.0,11.0,14.0,16.0,13.0,9.0,15.0,13.0,12.0,12.0,18.0,15.0,13.0,13.0,15.0,0.0,15.0,14.0,16.0,16.0,12.0,13.0,6.0,16.0,12.0,12.0,10.0,13.0,15.0,13.0,14.0,10.0,18.0,11.0,15.0,16.0,17.0],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"SEM_2_STATUS=%{y}<br>UNITS_COMPLETED_2=%{x}<extra></extra>\",\"legendgroup\":\"1\",\"marker\":{\"color\":\"#EF553B\"},\"name\":\"1\",\"notched\":false,\"offsetgroup\":\"1\",\"orientation\":\"h\",\"showlegend\":true,\"x\":[14.0,3.0,0.0,6.0,15.0,15.0,5.0,0.0,3.0,17.0,0.0,0.0,12.0,0.0,3.0,12.0,14.0,3.0,7.0,6.0,0.0,0.0,3.0,17.0,0.0,13.0,0.0,9.0,16.0,0.0,2.0,11.0,0.0,3.0,0.0,16.0,4.0,0.0,0.0,4.0,0.0,0.0,6.0,1.0,0.0,0.0,16.0,13.0,4.0,12.0,10.0,3.0,4.0,9.0,6.0,13.0,0.0,16.0,13.0,4.0,9.0,3.0,0.0,6.0,0.0,10.0,17.0,0.0,0.0,15.0,4.0,9.0,9.0,7.0,0.0,13.0,0.0,10.0,6.0,3.0,0.0,0.0,0.0,0.0,16.0,17.0,0.0,3.0,0.0,13.0,9.0,9.0,13.0,18.0,15.0,0.0,6.0,6.0,0.0,9.0,3.0,3.0,6.0,10.0,6.0,0.0,14.0,6.0,9.0,0.0,14.0,6.0,0.0,15.0,0.0,0.0,8.0,3.0,10.0,0.0,0.0,3.0,3.0,15.0,1.0,9.0,9.0,6.0,0.0,7.0,16.0,3.0,5.0,3.0,15.0,0.0,15.0,0.0,12.0,12.0,0.0,7.0,0.0,3.0,0.0,0.0,3.0,6.0,9.0,6.0,15.0,9.0,9.0,3.0,0.0,15.0,0.0,15.0,3.0,1.0,0.0,0.0,12.0,13.0,0.0,4.0,15.0,6.0,12.0,13.0,7.0,6.0,6.0,1.0,0.0,15.0,11.0,6.0,6.0,13.0,6.0,13.0,0.0,16.0,0.0,3.0,0.0,9.0,3.0,6.0,3.0,3.0,0.0,0.0,15.0,0.0,4.0,12.0,7.0,1.0,0.0,0.0,18.0,6.0,7.0,3.0,11.0,0.0,0.0,9.0,0.0,13.0,0.0,4.0,18.0,15.0,10.0,0.0,7.0,0.0,3.0,3.0,7.0,15.0,1.0,3.0,13.0,3.0,15.0,0.0,12.0,17.0,6.0,9.0,5.0,12.0,9.0,12.0,0.0,0.0,0.0,16.0,13.0,9.0,3.0,4.0,14.0,15.0,1.0,0.0,4.0,16.0,16.0,3.0,9.0,13.0,6.0,4.0,0.0,0.0,0.0,13.0,0.0,12.0,6.0,6.0,14.0,16.0,15.0,0.0,9.0,3.0,0.0,3.0,12.0,3.0,0.0,12.0,3.0,0.0,16.0,15.0,12.0,12.0,6.0,0.0,5.0,0.0,12.0,3.0,8.0,4.0,15.0,0.0,17.0,0.0,9.0,1.0,0.0,3.0,6.0,3.0,4.0,3.0,10.0,5.0,3.0,12.0,3.0,4.0,0.0,0.0,0.0,1.0,8.0,3.0,15.0,1.0,13.0,13.0,0.0,9.0,6.0,13.0,13.0,9.0,0.0,3.0,16.0,16.0,6.0,14.0,0.0,10.0,6.0,12.0,12.0,0.0,14.0,6.0,3.0,13.0,13.0,13.0,6.0,3.0,1.0,3.0,1.0,9.0,0.0,7.0,12.0,0.0,6.0,3.0,7.0,0.0,7.0,0.0,3.0,3.0,5.0,0.0,3.0,6.0,12.0,11.0,5.0,0.0,9.0,9.0,10.0,16.0,13.0,9.0,3.0,6.0,0.0,0.0,9.0,0.0,12.0,16.0,12.0,4.0,6.0,7.0,7.0,8.0,13.0,0.0,2.0,12.0,2.0,0.0,1.0,6.0,0.0,8.0,0.0,0.0,9.0,3.0,3.0,0.0,0.0,0.0,3.0,4.0,8.0,0.0,0.0,7.0,15.0,0.0,3.0,0.0,6.0,0.0,12.0,3.0,6.0,0.0,0.0,0.0,7.0,0.0,12.0,16.0,3.0,0.0,6.0,0.0,16.0,15.0,6.0,0.0,1.0,3.0,18.0,0.0,3.0,0.0,16.0,6.0,12.0,9.0,0.0,0.0,12.0,13.0,3.0,15.0,16.0,6.0,0.0,3.0,5.0,6.0,0.0,3.0,3.0,16.0,0.0,6.0,4.0,7.0,6.0,12.0,0.0,0.0,0.0,7.0,14.0,1.0,0.0,9.0,9.0,0.0,15.0,6.0,0.0,15.0,0.0,6.0,12.0,7.0,1.0,0.0,6.0,2.0,9.0,0.0,12.0,6.0,9.0,13.0,3.0,12.0,15.0,13.0,6.0,0.0,0.0,6.0,0.0,3.0,0.0,0.0,0.0,0.0,7.0,15.0,11.0,17.0,9.0,15.0,0.0,10.0,6.0,9.0,15.0,0.0,6.0,13.0,6.0,7.0,3.0,11.0,3.0,0.0,14.0,3.0,16.0,0.0,0.0,0.0,13.0,9.0,3.0,3.0,10.0,17.0,14.0,4.0,0.0,0.0,0.0,3.0,4.0,0.0,0.0,3.0,0.0,4.0,3.0,0.0,0.0,0.0,12.0,14.0,9.0,12.0,0.0,0.0,3.0,3.0,0.0,1.0,10.0,0.0,9.0,16.0,0.0,0.0,0.0,3.0,9.0,15.0,0.0,5.0,3.0,0.0,3.0,11.0,0.0,0.0,13.0,0.0,15.0,0.0,3.0,8.0,0.0,12.0,4.0,9.0,4.0,9.0,3.0,3.0,0.0,6.0,13.0,16.0,3.0,7.0,0.0,1.0,0.0,16.0,0.0,0.0,0.0,0.0,0.0,10.0,14.0,4.0,14.0,0.0,0.0,18.0,12.0,4.0,4.0,0.0,10.0,0.0,0.0,14.0,0.0,12.0,12.0,1.0,9.0,13.0,14.0,13.0,12.0,0.0,12.0,3.0,18.0,13.0,3.0,14.0,0.0,0.0,9.0,0.0,0.0,13.0,0.0,0.0,3.0,0.0,10.0,1.0,3.0,14.0,0.0,4.0,6.0,4.0,15.0,14.0,10.0,12.0,3.0,7.0,0.0,0.0,0.0,3.0,3.0,0.0,0.0,17.0,3.0,15.0,0.0,16.0,3.0,3.0,9.0,0.0,10.0,0.0,4.0,0.0,13.0,15.0,15.0,7.0,13.0,0.0,0.0,1.0,0.0,0.0,1.0,3.0,0.0,0.0,6.0,2.0,16.0,7.0,7.0,0.0,0.0,0.0,0.0,7.0,13.0,0.0,0.0,13.0,13.0,15.0,3.0,12.0,9.0,0.0,0.0,3.0,15.0,0.0,4.0,0.0,0.0,6.0,3.0,10.0,9.0,0.0,3.0,0.0,0.0,12.0,0.0,1.0,0.0,3.0,4.0,3.0,3.0,12.0,9.0,12.0,14.0,6.0,0.0,3.0,0.0,7.0,12.0,17.0,3.0,8.0,12.0,3.0,6.0,0.0,3.0,15.0,6.0,3.0,16.0,16.0,0.0,15.0],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UNITS_COMPLETED_2\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"SEM_2_STATUS\"},\"categoryorder\":\"array\",\"categoryarray\":[1,0]},\"legend\":{\"title\":{\"text\":\"SEM_2_STATUS\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.box(Xy_build_c, x='UNITS_COMPLETED_2', y='SEM_2_STATUS', color='SEM_2_STATUS', boxmode=\"overlay\")\n",
        "fig.update_layout(boxmode='group')  # To group the box plots by category\n",
        "fig.update_traces(orientation='h') # horizontal box plots\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2869fd19-4d2f-4461-a3ad-005ff39b38c7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "qbSJKaOigylN"
      },
      "source": [
        "These plots tell the same story as the numerical statistics: dropout in tem 3 sems to be strongly asociated with poor academic performance leading up to that term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "337ed008-5be7-4214-b818-46670c014709",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jPUjFSAmgylN"
      },
      "source": [
        "In spite of the above, the following scatterplot can give us some insight as to how challenging it might be to distinguish between classes when implementing a classification model. The red points indicate students who leave in semester 3, wheras the purple ones are retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9cbd59d0-ed56-4f3c-9eac-dc93e33bb0f0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3D-4_MmSgylN",
        "outputId": "ea6d7750-37b9-4a4c-e89c-eddb28f3b0c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"f16cc3c3-e3b3-4ed4-a73a-ec5561988210\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f16cc3c3-e3b3-4ed4-a73a-ec5561988210\")) {                    Plotly.newPlot(                        \"f16cc3c3-e3b3-4ed4-a73a-ec5561988210\",                        [{\"hovertemplate\":\"GPA_2=%{x}<br>UNITS_COMPLETED_2=%{y}<br>SEM_2_STATUS=%{marker.color}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0],\"coloraxis\":\"coloraxis\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"showlegend\":false,\"x\":[3.2,4.0,3.4545454545454546,2.9375,2.5,3.4,0.0,4.0,2.5,1.8461538461538465,2.8125,2.6666666666666665,3.5384615384615383,0.75,0.0,2.6923076923076925,3.625,0.0,3.4,2.4285714285714284,4.0,4.0,3.5,3.375,3.588235294117647,4.0,2.25,2.0,3.25,1.25,2.6,1.0,3.7857142857142856,4.0,2.4,2.4,3.5625,0.4,3.4,3.076923076923077,3.5,2.8,2.8,1.7692307692307692,3.8125,4.0,3.0,2.5,3.25,3.8,4.0,4.0,3.4375,4.0,0.6153846153846154,1.375,4.0,3.411764705882353,2.0,2.9166666666666665,0.0,3.083333333333333,3.6,3.6,3.5384615384615383,2.769230769230769,2.6666666666666665,3.076923076923077,1.0,3.333333333333333,3.4375,2.5,3.8125,3.363636363636364,3.411764705882353,1.2857142857142858,2.75,3.230769230769231,2.588235294117647,3.4166666666666665,1.8461538461538465,4.0,3.5625,2.0,2.7333333333333334,3.5,0.0,3.5,4.0,4.0,0.0,3.75,4.0,3.6,2.7142857142857144,3.75,4.0,3.142857142857143,3.333333333333333,2.533333333333333,3.2,2.75,3.823529411764706,2.642857142857143,3.5,3.4,2.8823529411764706,0.0,3.642857142857143,3.4375,0.8,2.25,3.0,3.8,4.0,2.75,3.0,3.25,3.4,3.0625,2.769230769230769,3.533333333333333,4.0,3.8,3.75,3.75,3.6,3.6,3.571428571428572,4.0,2.4615384615384617,4.0,3.8,2.0,3.7857142857142856,2.1666666666666665,0.9230769230769232,2.75,2.25,3.7142857142857135,2.5,3.076923076923077,3.625,3.0,3.55,3.8125,2.6875,0.0,3.230769230769231,2.6,2.6,1.25,3.2,3.4285714285714284,0.0,2.875,3.625,2.357142857142857,4.0,3.692307692307693,0.0,1.125,3.272727272727273,1.25,4.0,2.75,3.2,2.0,0.0,4.0,2.6,3.636363636363636,4.0,3.6,3.25,3.142857142857143,4.0,3.375,2.75,3.076923076923077,3.3125,2.875,4.0,3.8,4.0,4.0,3.0,3.75,4.0,4.0,3.5,2.0,2.642857142857143,4.0,3.4705882352941178,4.0,0.5,3.5384615384615383,3.4,3.6666666666666665,4.0,1.5,3.8,0.5294117647058824,1.6875,3.7857142857142856,3.5384615384615383,2.75,3.5454545454545454,3.8125,2.923076923076923,3.4,2.25,0.5454545454545454,0.0,1.625,3.2,0.0,3.5,1.6666666666666667,2.25,3.8,3.230769230769231,3.0,2.272727272727273,4.0,4.0,3.0,3.0,2.6923076923076925,2.25,3.6666666666666665,3.5,4.0,4.0,2.076923076923077,3.4,4.0,3.25,2.923076923076923,2.857142857142857,3.769230769230769,2.6923076923076925,3.5,2.375,0.4615384615384615,3.8125,0.0,3.5,3.333333333333333,2.25,3.75,3.75,1.25,4.0,2.75,3.25,3.5,1.9285714285714288,4.0,2.6875,4.0,3.0,3.0,2.875,4.0,0.0,4.0,1.0,4.0,3.25,2.857142857142857,3.5,3.25,0.7142857142857143,3.5625,3.8,0.0,3.333333333333333,3.4,3.5,3.2142857142857144,2.0625,0.7,3.176470588235294,0.8333333333333334,2.8461538461538463,2.25,3.7857142857142856,3.4375,3.8,3.5,2.8,0.0,4.0,4.0,2.6153846153846154,2.6,0.4615384615384615,3.8,1.5714285714285714,4.0,3.625,3.8125,3.0625,3.769230769230769,3.25,3.1538461538461537,3.25,2.5294117647058822,3.6666666666666665,3.5,0.4,3.733333333333333,0.5625,4.0,3.75,1.75,0.4615384615384615,2.6666666666666665,3.1333333333333333,3.25,1.9285714285714288,2.5,3.25,2.5,0.2,0.0,3.6,4.0,1.2,3.3846153846153846,4.0,3.5625,3.6,0.8,3.2222222222222223,3.6875,2.923076923076923,3.0,3.75,4.0,3.2857142857142856,3.076923076923077,4.0,0.0,3.2857142857142856,3.230769230769231,0.1875,3.235294117647059,3.3529411764705883,3.230769230769231,2.5,4.0,4.0,1.4375,3.6666666666666665,3.571428571428572,2.647058823529412,0.0,2.0,4.0,3.6,3.583333333333333,4.0,1.5,3.8,0.9166666666666666,3.333333333333333,3.5625,4.0,1.5,2.5,0.5,3.4,0.7142857142857143,4.0,0.0,3.0,3.25,4.0,4.0,0.6666666666666666,4.0,4.0,4.0,3.2,2.2,3.625,3.692307692307693,1.75,3.0,4.0,2.333333333333333,3.0625,4.0,4.0,2.8461538461538463,4.0,3.75,4.0,3.5,4.0,2.2,3.6666666666666665,3.0,3.411764705882353,4.0,3.5384615384615383,2.9166666666666665,2.875,1.5,2.5,3.8125,3.8,3.857142857142857,2.4375,0.6666666666666666,3.727272727272727,3.25,3.5625,2.1818181818181817,2.2,1.6666666666666667,4.0,3.3076923076923075,3.0,3.625,2.75,2.625,3.5,3.411764705882353,3.0,3.8125,3.5294117647058822,4.0,2.5,4.0,1.0,3.8125,3.076923076923077,2.636363636363636,3.75,3.1538461538461537,4.0,1.5,3.625,2.769230769230769,2.0,1.0,3.1538461538461537,2.769230769230769,3.4444444444444446,3.333333333333333,3.0,4.0,4.0,3.4375,1.5384615384615383,3.25,1.1538461538461535,1.8,0.6923076923076923,2.333333333333333,2.75,4.0,3.25,2.933333333333333,1.5,3.230769230769231,0.0,3.4375,2.375,4.0,3.8125,1.9230769230769231,2.6,3.0,4.0,2.769230769230769,2.833333333333333,4.0,2.5,3.0588235294117645,2.0,1.5,4.0,2.25,3.0,0.0,3.8461538461538463,3.0,0.0,0.0,3.25,3.6,2.6,3.6,4.0,3.6,3.0,4.0,3.076923076923077,4.0,4.0,1.2941176470588236,1.6153846153846154,3.4,2.333333333333333,2.8,4.0,3.8,3.7142857142857135,3.333333333333333,3.466666666666667,4.0,1.3333333333333333,3.4,2.875,2.25,0.0,3.8,3.4615384615384617,3.0,2.333333333333333,2.5,2.4375,4.0,2.0,3.7777777777777777,3.6666666666666665,3.25,3.75,3.4615384615384617,3.2857142857142856,3.230769230769231,3.6666666666666665,2.076923076923077,1.2727272727272727,3.1875,3.0,4.0,3.25,2.2142857142857144,4.0,3.272727272727273,0.7857142857142857,0.4,3.0,1.1538461538461535,2.7857142857142856,3.5384615384615383,2.6,3.25,3.333333333333333,4.0,4.0,3.272727272727273,3.3846153846153846,4.0,3.375,0.0,2.75,3.0,3.769230769230769,3.8333333333333335,3.25,2.764705882352941,3.8,2.5454545454545454,0.0,4.0,3.8,4.0,3.2,2.75,2.0,4.0,3.0,3.7857142857142856,3.230769230769231,2.571428571428572,1.5833333333333333,1.4,4.0,3.1333333333333333,2.25,2.857142857142857,3.3076923076923075,3.8,3.2,3.7142857142857135,3.588235294117647,3.4,3.8333333333333335,1.4,2.571428571428572,4.0,4.0,4.0,3.6,2.4375,2.5,1.5384615384615383,2.727272727272727,3.7142857142857135,3.8,4.0,3.8125,4.0,3.6,0.75,2.1,3.4,2.8125,0.6923076923076923,3.6,3.0,0.0,2.642857142857143,3.0,2.5,3.0588235294117645,3.5,2.571428571428572,2.769230769230769,3.7142857142857135,3.076923076923077,3.0,2.857142857142857,2.333333333333333,3.769230769230769,3.2,1.8125,3.0,3.4615384615384617,3.2,3.823529411764706,3.933333333333333,3.25,0.0,3.3076923076923075,2.5294117647058822,1.5,4.0,2.333333333333333,2.727272727272727,3.2142857142857144,4.0,2.5,4.0,2.5,3.764705882352941,2.9285714285714284,3.5625,2.466666666666667,2.5,0.5454545454545454,0.0,3.5384615384615383,4.0,4.0,3.4,3.6,3.0,2.625,2.8,4.0,3.2,1.0,3.6,0.0,3.25,3.769230769230769,0.0,3.692307692307693,3.2,3.2857142857142856,4.0,3.0,2.909090909090909,4.0,1.75,3.1538461538461537,3.4375,3.25,1.0,3.5,3.230769230769231,0.0,4.0,1.4,3.466666666666667,3.75,2.2666666666666666,2.5,2.1,3.6,3.769230769230769,2.9285714285714284,4.0,4.0,3.5,2.6153846153846154,4.0,3.3846153846153846,2.923076923076923,3.4615384615384617,3.375,1.3846153846153846,2.875,2.375,1.5,4.0,4.0,4.0,4.0,1.5,3.4,2.071428571428572,0.0,3.769230769230769,3.769230769230769,3.375,2.25,3.4285714285714284,0.4,2.75,4.0,1.6,1.25,2.076923076923077,3.8333333333333335,1.9230769230769231,3.0,3.692307692307693,3.8,3.0,3.0,2.1875,3.2142857142857144,2.5,2.0,3.0,3.5,2.25,2.6,3.5294117647058822,4.0,4.0,2.0,2.9166666666666665,4.0,3.75,3.0,2.0,3.3846153846153846,4.0,4.0,4.0,4.0,2.7333333333333334,3.5,1.125,2.375,1.25,4.0,2.4615384615384617,3.0,2.1538461538461537,3.0,2.8,3.8,4.0,3.4,3.25,3.6,0.0,1.2307692307692308,3.6,3.727272727272727,3.25,3.75,0.375,3.5,3.0625,3.75,1.0,3.5,2.5,2.909090909090909,4.0,3.2,3.3846153846153846,3.5294117647058822,3.3125,0.7,4.0,3.6,2.5384615384615383,3.692307692307693,4.0,3.555555555555556,3.75,3.25,3.0,3.625,3.5625,0.1538461538461538,3.2,3.25,2.333333333333333,4.0,2.0,4.0,3.533333333333333,3.75,3.8125,3.8,4.0,2.0,4.0,3.0625,3.625,3.5,2.5384615384615383,1.3333333333333333,3.4,4.0,3.230769230769231,3.692307692307693,3.8,3.75,4.0,2.923076923076923,1.5,2.3529411764705883,0.75,3.4285714285714284,3.7857142857142856,1.25,4.0,3.75,3.4,3.769230769230769,4.0,1.5,3.6,2.111111111111111,4.0,4.0,1.4,4.0,2.076923076923077,4.0,0.0,3.1666666666666665,2.7,3.727272727272727,3.083333333333333,3.6,3.4615384615384617,3.5,2.642857142857143,3.333333333333333,2.7142857142857144,2.4,4.0,2.333333333333333,1.25,3.6,4.0,3.4444444444444446,4.0,3.294117647058824,1.8333333333333333,3.0,4.0,2.5,4.0,0.0,3.769230769230769,4.0,3.071428571428572,3.75,3.5,1.9285714285714288,3.111111111111111,2.555555555555556,3.5384615384615383,4.0,2.0,2.6153846153846154,3.5,3.6,4.0,2.7142857142857144,3.0,3.375,3.5,3.8125,1.75,2.083333333333333,4.0,3.0,0.0,1.6666666666666667,3.076923076923077,1.1428571428571428,0.0,2.923076923076923,3.764705882352941,3.5,3.0,3.071428571428572,4.0,3.7857142857142856,3.6,3.7058823529411766,3.6,4.0,2.333333333333333,3.0,3.25,2.9411764705882355,3.4,2.5,3.0,3.357142857142857,2.4166666666666665,3.625,1.7142857142857142,3.2,2.5,2.6,3.0,2.8125,4.0,3.333333333333333,0.0,0.0,3.692307692307693,3.4375,3.769230769230769,3.7857142857142856,4.0,3.4285714285714284,2.5,2.0,1.7857142857142858,2.1538461538461537,3.1538461538461537,3.8,3.8,3.2,3.4,3.8125,4.0,2.909090909090909,4.0,2.5625,4.0,3.8461538461538463,3.625,3.4166666666666665,3.0,3.4285714285714284,2.8,0.0,4.0,0.2,2.25,0.0,3.375,3.375,3.75,3.692307692307693,3.2,3.4375,4.0,4.0,2.25,3.0,3.8,4.0,3.6,3.4166666666666665,3.764705882352941,4.0,0.5,4.0,3.272727272727273,4.0,3.357142857142857,3.769230769230769,3.8125,2.5,3.25,4.0,2.6666666666666665,2.2,4.0,3.25,3.25,4.0,3.375,4.0,2.8461538461538463,4.0,3.25,3.4,2.4615384615384617,2.25,3.142857142857143,4.0,2.25,4.0,3.2,3.75,3.25,3.7142857142857135,0.0,2.4285714285714284,3.25,3.692307692307693,2.642857142857143,2.333333333333333,3.769230769230769,3.769230769230769,2.333333333333333,2.625,3.6,3.769230769230769,0.25,4.0,2.0,2.0,2.75,3.7,3.5454545454545454,3.5384615384615383,1.0,1.3333333333333333,4.0,1.9230769230769231,2.75,0.4615384615384615,4.0,4.0,2.3076923076923075,1.0,3.769230769230769,1.5,2.076923076923077,3.0,2.333333333333333,3.727272727272727,3.076923076923077,1.5,0.0,3.5625,3.230769230769231,3.6,3.4375,2.769230769230769,2.3125,3.4285714285714284,2.0,3.692307692307693,2.933333333333333,2.230769230769231,4.0,2.357142857142857,3.7142857142857135,3.7857142857142856,1.75,2.5,2.9285714285714284,4.0,2.6666666666666665,2.923076923076923,3.6666666666666665,2.5454545454545454,3.0,2.0,1.3333333333333333,4.0,3.6,4.0,2.8461538461538463,3.3529411764705883,2.642857142857143,3.0,4.0,1.0,2.4285714285714284,3.4285714285714284,3.8,1.6666666666666667,3.363636363636364,2.375,3.2,2.6,2.333333333333333,0.0,3.4615384615384617,4.0,3.2,3.1875,3.5,3.75,3.3076923076923075,1.0,1.8461538461538465,2.8,3.764705882352941,3.5,3.0,2.1538461538461537,1.6923076923076923,4.0,2.4615384615384617,4.0,3.3076923076923075,2.0,3.4,0.625,4.0,3.6,3.4285714285714284,3.230769230769231,1.5714285714285714,1.75,3.3125,2.3076923076923075,3.0,4.0,3.333333333333333,0.2857142857142857,3.2,2.6,3.0,3.5,2.125,2.375,3.1538461538461537,4.0,4.0,3.4375,3.3846153846153846,3.0625,3.4,0.0,4.0,0.0,2.7142857142857144,0.7272727272727273,4.0,3.733333333333333,4.0,2.5,3.25,3.0,4.0,2.769230769230769,0.0,3.8,3.2,0.0,3.8,4.0,0.9,3.4,3.333333333333333,3.4,2.3076923076923075,1.9230769230769231,2.909090909090909,1.4,2.6153846153846154,3.142857142857143,3.5,2.875,3.615384615384616,3.0,4.0,3.823529411764706,4.0,3.4,3.25,2.8,3.142857142857143,3.5,0.0,2.0,3.5,3.6,2.6153846153846154,2.0,3.4375,3.7857142857142856,2.25,3.4375,3.7,2.6923076923076925,3.8125,2.833333333333333,4.0,3.4375,3.125,3.4,3.25,2.9285714285714284,3.6,3.7142857142857135,4.0,3.4,2.8,3.8125,3.0,3.4166666666666665,3.2,3.5,3.8333333333333335,2.25,0.0,3.625,0.2,4.0,3.6,3.5,4.0,3.5,2.923076923076923,3.25,4.0,4.0,0.0,3.0,2.25,2.272727272727273,0.3636363636363636,3.769230769230769,3.2,3.692307692307693,3.8,2.25,3.8,3.5384615384615383,3.769230769230769,2.4285714285714284,3.5,4.0,3.692307692307693,1.6428571428571428,3.4,4.0,3.875,2.0,4.0,2.230769230769231,3.727272727272727,4.0,3.2,1.125,4.0,2.6,1.5,2.6875,3.4,2.4,0.0,3.0,1.6,4.0,4.0,3.2,2.333333333333333,2.5,3.6,1.0,1.375,3.3076923076923075,4.0,4.0,3.8,3.411764705882353,2.375,3.769230769230769,2.9166666666666665,3.333333333333333,3.25,2.909090909090909,3.625,3.2,2.0,3.2142857142857144,3.8,3.6,3.0,2.357142857142857,2.0,3.5,2.0,3.25,4.0,1.8571428571428568,3.75,0.3333333333333333,3.076923076923077,1.6,3.7142857142857135,3.2,4.0,4.0,1.25,2.0,4.0,2.083333333333333,4.0,3.2,2.0,4.0,3.764705882352941,3.076923076923077,2.8461538461538463,3.769230769230769,1.5454545454545454,2.2,2.6153846153846154,1.0,3.2142857142857144,3.8333333333333335,4.0,3.3846153846153846,1.0,4.0,3.4,3.2,3.0588235294117645,0.0,3.636363636363636,4.0,4.0,2.909090909090909,3.0,0.0,3.0,3.4,0.25,4.0,4.0,2.6,3.3125,3.7857142857142856,3.0,4.0,0.0,3.8,3.5384615384615383,3.3,1.7058823529411764,4.0,4.0,3.6,0.0,3.0,0.6666666666666666,3.75,4.0,3.333333333333333,3.5,4.0,2.230769230769231,1.0,2.3846153846153846,3.8125,3.1333333333333333,3.6,2.3846153846153846,4.0,2.6,4.0,2.230769230769231,2.769230769230769,3.0,1.0,4.0,3.5384615384615383,4.0,3.8125,3.8125,0.0,4.0,3.7857142857142856,3.6,3.5625,0.0,2.0,3.076923076923077,3.2,3.75,3.8,3.357142857142857,3.75,3.692307692307693,2.4285714285714284,3.25,0.25,3.0,0.0,3.2142857142857144,4.0,3.75,3.25,3.5625,1.75,2.833333333333333,3.727272727272727,4.0,3.1875,1.1111111111111112,3.333333333333333,3.4,2.769230769230769,2.0,2.647058823529412,3.4,4.0,3.769230769230769,2.6666666666666665,3.0,2.1,4.0,3.764705882352941,3.230769230769231,2.4615384615384617,3.0,3.0,2.0,3.2,3.533333333333333,2.75,2.411764705882353,3.5,3.692307692307693,3.0,2.5,4.0,3.230769230769231,0.6428571428571429,3.333333333333333,2.7857142857142856,4.0,2.25,3.363636363636364,2.2,4.0,3.9375,1.0,3.2666666666666666,3.25,3.5384615384615383,2.5384615384615383,2.833333333333333,4.0,3.0,1.4,3.5,3.823529411764706,2.333333333333333,3.0,3.769230769230769,1.0,2.0,0.2307692307692307,2.4,4.0,3.1538461538461537,3.8333333333333335,4.0,3.8125,3.4,4.0,0.25,4.0,4.0,4.0,2.6923076923076925,1.2727272727272727,3.7142857142857135,4.0,2.5,2.5454545454545454,1.75,3.0625,1.25,3.6666666666666665,4.0,3.5,3.4615384615384617,3.4375,4.0,4.0,3.75,2.625,3.5,2.75,4.0,2.357142857142857,4.0,4.0,3.769230769230769,2.769230769230769,1.8,3.4,3.6666666666666665,4.0,4.0,1.75,1.6666666666666667,4.0,2.923076923076923,2.083333333333333,3.2,3.076923076923077,2.25,2.3076923076923075,4.0,4.0,1.3333333333333333,3.5,3.636363636363636,1.9285714285714288,3.0,2.8666666666666667,3.5384615384615383,0.9230769230769232,1.0,4.0,4.0,4.0,3.5384615384615383,4.0,4.0,3.4285714285714284,3.769230769230769,0.0,1.9,3.8,3.4375,3.25,2.375,3.75,3.8,0.3333333333333333,3.8,4.0,3.6,3.692307692307693,3.076923076923077,4.0,3.0,2.0,2.6666666666666665,3.0,3.25,3.4,4.0,3.5384615384615383,1.25,1.4166666666666667,1.2,4.0,0.0,3.5625,3.076923076923077,3.333333333333333,0.0,3.25,2.9166666666666665,2.4166666666666665,2.0,1.25,4.0,2.5,1.5384615384615383,1.8461538461538465,3.0,3.692307692307693,3.2857142857142856,4.0,3.0,3.571428571428572,2.25,3.3125,2.5625,1.8666666666666667,0.4,2.9375,2.5384615384615383,2.0,3.230769230769231,3.769230769230769,3.8,3.7142857142857135,3.3076923076923075,1.0,3.3846153846153846,1.6153846153846154,0.6,3.25,3.0625,1.0,2.5,3.25,3.25,0.0,4.0,4.0,3.615384615384616,4.0,3.0,3.733333333333333,3.8,3.8333333333333335,3.5625,2.3846153846153846,0.0,0.6,3.6,3.4,3.7857142857142856,3.2,0.0,3.466666666666667,4.0,1.0,2.076923076923077,4.0,4.0,3.2142857142857144,1.0,4.0,3.5,1.3333333333333333,3.4285714285714284,3.8,0.8,2.25,0.5384615384615384,1.2727272727272727,3.8,3.8125,3.25,0.0,3.5,0.5,4.0,3.3529411764705883,4.0,4.0,3.3076923076923075,2.6666666666666665,2.6923076923076925,2.4,2.6923076923076925,2.0,3.0,3.0,3.75,1.1538461538461535,4.0,4.0,3.75,1.8125,0.0,1.5,4.0,3.8,2.6666666666666665,3.076923076923077,4.0,3.4166666666666665,3.8125,3.75,3.3076923076923075,3.25,3.230769230769231,2.769230769230769,3.333333333333333,0.8,2.5454545454545454,3.8,2.636363636363636,2.75,3.8125,2.0,0.5,0.0,2.4,4.0,0.0,3.2,3.7857142857142856,2.9,3.111111111111111,3.0625,2.8125,4.0,2.8181818181818183,2.9166666666666665,1.5,0.0,3.533333333333333,3.25,2.230769230769231,1.5,3.357142857142857,3.75,3.636363636363636,2.0,4.0,4.0,0.0,3.5,3.1875,0.0,0.0,3.0,1.3333333333333333,3.357142857142857,3.4,2.923076923076923,2.8461538461538463,3.8125,0.3333333333333333,3.8333333333333335,3.6,3.6,2.4,2.875,3.5,3.0,1.6666666666666667,3.5,3.230769230769231,1.5714285714285714,2.071428571428572,2.833333333333333,3.4,3.5,3.8125,2.25,4.0,3.2,1.9230769230769231,3.230769230769231,2.076923076923077,3.7142857142857135,3.4615384615384617,2.6,3.6,3.75,3.0,2.5,2.9375,4.0,2.8,0.0,3.5384615384615383,3.0,2.2,3.0,3.7857142857142856,3.0,2.7142857142857144,2.8,0.25,3.6666666666666665,0.3333333333333333,0.4,4.0,3.625,3.4,1.5,0.6,3.0625,3.4285714285714284,2.333333333333333,3.5,3.4375,4.0,3.0,4.0,0.0,0.6666666666666666,3.769230769230769,2.7857142857142856,3.25,1.6666666666666667,2.142857142857143,4.0,3.25,2.4285714285714284,3.6666666666666665,2.333333333333333,3.571428571428572,3.933333333333333,3.636363636363636,0.2222222222222222,3.75,0.6923076923076923,2.4615384615384617,4.0,2.5625,4.0,4.0,3.6666666666666665,3.1538461538461537,3.0625,2.4,2.25,3.6,3.5,4.0,4.0,4.0,2.1538461538461537,3.8333333333333335,4.0,4.0,0.0,3.333333333333333,3.8,2.0,3.0,3.875,2.75,2.333333333333333,4.0,0.6666666666666666,3.076923076923077,4.0,1.5,2.4285714285714284,3.2857142857142856,4.0,3.333333333333333,4.0,3.0625,0.4285714285714285,2.75,3.4615384615384617,3.8,3.7142857142857135,2.4285714285714284,3.8125,3.357142857142857,4.0,3.0,3.8333333333333335,2.25,3.5,3.0,3.5,3.5384615384615383,2.25,4.0,3.2,3.5384615384615383,4.0,3.4615384615384617,3.0,3.8,2.4,3.8,3.0,3.4285714285714284,4.0,3.5384615384615383,1.0,2.7857142857142856,2.75,3.3846153846153846,2.1875,3.5,3.0,4.0,4.0,4.0,3.4615384615384617,3.8,3.6,3.8333333333333335,4.0,3.6,4.0,3.6666666666666665,3.230769230769231,3.0,3.083333333333333,3.25,1.6875,4.0,3.3846153846153846,2.5,2.4615384615384617,3.0,3.8125,3.25,2.25,3.0,2.3846153846153846,3.5,0.0,4.0,3.2,3.5625,4.0,3.25,3.6,3.375,2.6666666666666665,4.0,3.636363636363636,3.75,3.75,3.235294117647059,4.0,3.0,4.0,2.6666666666666665,3.8,1.4285714285714286,3.647058823529412,3.4,2.2,2.75,3.8,3.6,3.6,4.0,4.0,0.0,3.3076923076923075,3.4285714285714284,3.4,2.25,3.0,3.769230769230769,4.0,3.5625,3.4166666666666665,3.5,3.0,3.333333333333333,2.5,1.2857142857142858,4.0,2.6923076923076925,2.909090909090909,3.7,0.6666666666666666,2.4285714285714284,3.333333333333333,3.647058823529412,2.0,3.25,2.333333333333333,2.5,3.5,2.5,0.0,3.1333333333333333,3.625,3.388888888888889,3.4375,4.0,0.0,3.4,4.0,3.176470588235294,4.0,3.571428571428572,4.0,3.692307692307693,3.8125,3.823529411764706,2.3076923076923075,0.0,3.4,3.5,4.0,0.1875,2.5384615384615383,4.0,3.5,2.75,3.692307692307693,3.8,2.6153846153846154,3.4,0.5,4.0,3.8125,3.0,3.5384615384615383,3.8,2.3846153846153846,2.9166666666666665,3.333333333333333,3.071428571428572,2.4615384615384617,2.375,3.055555555555556,2.466666666666667,3.066666666666667,3.526315789473684,0.9166666666666666,3.6,2.875,4.0,4.0,1.25,4.0,3.733333333333333,3.636363636363636,2.727272727272727,3.6666666666666665,4.0,3.25,3.692307692307693,4.0,4.0,3.1538461538461537,1.4,3.0,3.75,4.0,2.625,3.230769230769231,2.071428571428572,1.6,0.4615384615384615,3.0,2.75,3.6666666666666665,3.4,2.3,3.6,3.4,4.0,2.4285714285714284,2.533333333333333,4.0,3.625,1.3333333333333333,3.375,2.857142857142857,3.533333333333333,2.933333333333333,2.066666666666667,2.4615384615384617,3.6,4.0,2.75,3.6666666666666665,2.769230769230769,4.0,3.25,0.4285714285714285,4.0,3.3125,2.230769230769231,2.6666666666666665,0.2307692307692307,3.25,2.4,2.8461538461538463,4.0,3.3076923076923075,3.733333333333333,0.0,2.333333333333333,3.6,2.5,0.0,1.625,0.9,4.0,3.076923076923077,3.588235294117647,3.0625,4.0,4.0,2.8,3.692307692307693,3.0,2.0625,2.8461538461538463,2.923076923076923,2.6666666666666665,0.0,3.25,3.0,2.888888888888889,2.25,4.0,3.692307692307693,3.1538461538461537,2.857142857142857,3.25,3.5,3.615384615384616,4.0,4.0,3.6666666666666665,2.636363636363636,2.8,3.769230769230769,3.8125,2.8,4.0,3.769230769230769,3.2,4.0,1.8461538461538465,3.4,2.230769230769231,4.0,2.0,3.4,3.2666666666666666,3.2,3.6,3.333333333333333,3.5,3.571428571428572,2.533333333333333,3.75,0.0,3.5,3.2222222222222223,2.25,0.8125,3.363636363636364,4.0,3.0,3.823529411764706,2.636363636363636,1.4615384615384617,2.4615384615384617,2.6,3.0,3.684210526315789,3.0,3.647058823529412,3.4285714285714284,3.0,3.0,2.230769230769231,3.4375,1.25,2.230769230769231,2.727272727272727,2.6,1.5,2.8666666666666667,2.230769230769231,3.769230769230769,3.4,3.375,1.5,1.25,3.1875,3.4,4.0,2.875,3.1875,1.4,3.5,3.5384615384615383,3.7777777777777777,3.6666666666666665,2.230769230769231,4.0,3.8,3.8125,1.5,2.769230769230769,3.2,2.636363636363636,1.2222222222222223,2.1538461538461537,3.8125,0.25,3.642857142857143,0.0,1.3846153846153846,4.0,4.0,3.2777777777777777,2.0,0.8,3.769230769230769,0.6363636363636364,3.2,4.0,4.0,2.230769230769231,3.0,3.823529411764706,0.0,3.2857142857142856,2.0,2.769230769230769,2.5384615384615383,4.0,3.0,0.4,3.2,1.75,3.0,2.0,3.8,3.230769230769231,2.8461538461538463,4.0,3.5,3.647058823529412,1.6428571428571428,3.4,0.0,2.25,3.4375,2.4166666666666665,0.0,4.0,3.8333333333333335,1.4,3.25,2.75,3.0,2.75,3.6,4.0,2.0,3.0,3.8125,1.5,4.0,3.0,3.615384615384616,3.5,0.0,1.3076923076923077,1.25,3.411764705882353,2.25,3.5,2.4285714285714284,4.0,2.2,3.5,4.0,3.75,4.0,2.75,2.8,2.3,4.0,4.0,2.2,3.3846153846153846,3.4285714285714284,3.75,3.6,2.5625,3.533333333333333,4.0,2.6153846153846154,3.769230769230769,3.5,4.0,4.0,4.0,2.090909090909091,3.1875,2.6923076923076925,2.6923076923076925,3.5,2.923076923076923,3.4,3.8,4.0,3.733333333333333,3.615384615384616,3.6,3.8,1.0,2.4285714285714284,4.0,3.6,1.5,2.230769230769231,4.0,3.0,3.8,3.75,1.0,4.0,3.769230769230769,1.4,4.0,3.0,3.692307692307693,3.636363636363636,1.0,3.0,3.0625,3.2857142857142856,3.4375,3.4705882352941178,4.0,2.625,0.0,3.6,3.5625,3.8666666666666663,3.3846153846153846,3.823529411764706,3.5,3.0,2.5,4.0,2.0,3.333333333333333,4.0,3.2142857142857144,3.25,3.2142857142857144,3.0,3.0,2.571428571428572,4.0,4.0,0.4,3.5454545454545454,3.8,3.4285714285714284,3.2,0.8571428571428571,3.5,3.8333333333333335,3.0,3.0625,2.769230769230769,4.0,3.25,4.0,3.0,0.0,2.5625,3.4,2.5384615384615383,3.0,3.8,4.0,3.75,2.875,1.25,2.3,4.0,3.2142857142857144,3.0,4.0,3.2666666666666666,3.25,2.375,3.8125,3.5625,3.4285714285714284,2.0,2.230769230769231,1.5,3.636363636363636,2.909090909090909,3.5,4.0,2.75,4.0,4.0,0.25,3.25,2.0,0.0,2.5,0.0,0.0,2.75,3.125,3.0,4.0,3.6,4.0,2.5,3.6,3.5384615384615383,4.0,2.5,4.0,4.0,2.375,1.25,3.142857142857143,3.25,2.5,4.0,4.0,3.411764705882353,3.8125,3.375,2.4615384615384617,4.0,3.8,3.375,0.0,3.25,4.0,4.0,3.7142857142857135,4.0,3.5384615384615383,2.9285714285714284,3.25,3.125,3.692307692307693,3.6,2.6153846153846154,3.3846153846153846,3.636363636363636,0.25,4.0,3.2857142857142856,3.1666666666666665,4.0,3.25,4.0,2.4,3.8333333333333335,4.0,3.272727272727273,3.0,3.333333333333333,3.25,3.6,3.25,0.4,3.8,3.0,3.5,3.4285714285714284,3.176470588235294,3.0,3.571428571428572,2.4,3.75,3.5,2.923076923076923,3.0,2.7857142857142856,3.0,0.7692307692307693,2.076923076923077,3.6,2.4,0.0,1.125,3.8125,3.0625,3.0,4.0,3.1875,2.6666666666666665,4.0,4.0,3.625,2.0,2.0,4.0,4.0,4.0,0.5,3.6666666666666665,4.0,3.4375,2.8,3.75,0.4285714285714285,0.7692307692307693,0.75,0.0,4.0,3.6,3.25,3.9375,3.5,2.6923076923076925,3.1875,2.75,3.4,2.1538461538461537,4.0,2.7,0.1,3.0,3.7142857142857135,3.6,3.1538461538461537,2.769230769230769,3.333333333333333,4.0,4.0,3.692307692307693,3.0,2.6666666666666665,2.625,3.571428571428572,0.3333333333333333,3.692307692307693,3.6666666666666665,3.8125,3.25,3.230769230769231,1.6923076923076923,3.692307692307693,3.1875,1.8,4.0,4.0,3.25,0.5833333333333334,3.0,2.0,3.75,2.8,4.0,3.375,3.625,3.3529411764705883,3.6,4.0,3.823529411764706,0.4285714285714285,4.0,2.6923076923076925,4.0,3.8666666666666663,3.7142857142857135,2.2,2.75,3.5,2.4615384615384617,2.7777777777777777,4.0,3.4,3.230769230769231,3.8,4.0,2.75,3.7857142857142856,0.2307692307692307,3.0,4.0,3.769230769230769,3.2857142857142856,0.625,3.733333333333333,4.0,0.0,3.7142857142857135,3.4,3.083333333333333,1.0,4.0,3.7857142857142856,3.2,4.0,4.0,4.0,3.0,4.0,3.0,4.0,4.0,4.0,3.1875,2.6666666666666665,0.0,4.0,3.076923076923077,1.9090909090909087,3.375,0.4,4.0,3.2666666666666666,3.5,2.75,3.375,3.4,4.0,3.6,2.875,2.2,2.4615384615384617,3.0,4.0,3.571428571428572,4.0,3.083333333333333,3.8,2.833333333333333,1.0,3.5384615384615383,2.357142857142857,2.2222222222222223,3.0,3.2857142857142856,3.2,3.0,2.2857142857142856,0.9090909090909092,4.0,3.6,2.0,4.0,3.8125,3.125,1.9230769230769231,2.6666666666666665,3.5,4.0,2.5625,3.5,2.923076923076923,4.0,2.8,3.0588235294117645,2.7857142857142856,3.625,3.25,3.2857142857142856,3.272727272727273,4.0,4.0,4.0,2.5,3.5,1.5294117647058822,3.2142857142857144,2.5384615384615383,3.4375,3.333333333333333,4.0,3.8125,3.3076923076923075,3.8125,3.625,2.25,3.625,3.769230769230769,3.0,1.0,3.4375,3.0,3.25,2.75,3.3125,2.076923076923077,2.888888888888889,3.75,4.0,2.4,2.625,3.0,3.769230769230769,0.7647058823529411,3.8,3.5384615384615383,4.0,3.5625,3.076923076923077,0.5384615384615384,3.0,3.4,1.6875,2.2,0.6666666666666666,3.375,2.2,1.8571428571428568,0.0,2.5,3.3125,0.8333333333333334,3.8125,2.6,3.3076923076923075,4.0,2.5,3.571428571428572,3.0,2.230769230769231,2.6,3.8,3.0,3.3076923076923075,3.25,3.692307692307693,2.083333333333333,3.0,2.857142857142857,2.75,2.727272727272727,4.0,0.0,3.2,3.0625,3.8,0.0,3.25,2.3,1.0,3.625,2.9285714285714284,2.333333333333333,4.0,1.5,0.4,2.6923076923076925,4.0,3.0,3.076923076923077,0.0,3.8125,3.0,1.8461538461538465,3.0,2.0,3.8,4.0,3.4285714285714284,1.5,3.333333333333333,0.5,3.8125,4.0,3.6,3.25,2.0,3.2,3.5384615384615383,4.0,3.7857142857142856,4.0,2.642857142857143,1.5,3.5294117647058822,3.071428571428572,3.0,4.0,0.0,3.333333333333333,2.5,2.75,2.230769230769231,4.0,0.0,4.0,4.0,3.0,2.0,3.4,3.823529411764706,4.0,3.3076923076923075,3.75,3.8125,4.0,4.0,3.0,3.0,3.5,4.0,3.692307692307693,3.5,3.6666666666666665,3.692307692307693,3.75,3.5,3.0,2.4,2.8125,3.5,3.5,3.764705882352941,0.9333333333333332,3.6666666666666665,3.4,3.8,4.0,3.75,3.0,3.25,3.6666666666666665,2.0,2.5625,4.0,2.636363636363636,3.75,3.6,3.0,3.411764705882353,3.4615384615384617,2.5,3.0,3.6,2.75,2.8,3.6666666666666665,3.8,3.8125,3.25,3.3076923076923075,2.764705882352941,3.5,4.0,2.6666666666666665,4.0,3.8,3.6,0.0,1.0,3.636363636363636,3.5,3.4375,2.625,4.0,2.083333333333333,2.0,3.75,4.0,1.5,3.2857142857142856,4.0,3.411764705882353,3.3076923076923075,4.0,3.25,4.0,2.923076923076923,3.230769230769231,0.3333333333333333,3.4,3.0,3.75,3.8,2.9166666666666665,3.4285714285714284,3.375,4.0,3.6666666666666665,2.4,3.25,3.3,0.875,3.2857142857142856,3.1875,4.0,0.3333333333333333,3.357142857142857,3.25,0.0,3.625,3.6,2.75,3.7,3.0,3.636363636363636,3.5,3.4,0.75,4.0,3.0,4.0,3.5,2.4,2.75,2.7142857142857144,1.3333333333333333,1.0,3.8125,2.4,3.0,1.5,2.3125,4.0,2.333333333333333,2.5,3.7142857142857135,3.0,4.0,3.5,3.071428571428572,3.4,2.8125,2.7142857142857144,2.7333333333333334,3.8,3.5,4.0,1.6666666666666667,3.75,3.4,3.2,3.4375,3.8,3.230769230769231,3.7142857142857135,3.6,2.2,3.4285714285714284,2.6666666666666665,3.5,3.8,3.25,4.0,4.0,0.0,1.5294117647058822,3.0,2.75,4.0,4.0,2.333333333333333,3.333333333333333,3.8,2.0,3.5,2.875,3.1666666666666665,3.2857142857142856,2.8,4.0,2.6666666666666665,2.923076923076923,3.5384615384615383,1.5,3.769230769230769,2.4,4.0,3.0,3.0,1.4,2.923076923076923,3.0,1.5,3.625,4.0,4.0,2.0,4.0,2.6666666666666665,1.0625,4.0,2.833333333333333,2.6875,3.6,1.75,3.8125,3.769230769230769,2.6666666666666665,4.0,1.4285714285714286,4.0,0.2307692307692307,2.8,3.8,3.071428571428572,4.0,3.4285714285714284,2.5384615384615383,4.0,3.769230769230769,1.1428571428571428,0.7692307692307693,1.0,2.4285714285714284,3.0,2.857142857142857,3.3846153846153846,1.5,1.25,4.0,3.727272727272727,4.0,0.2727272727272727,4.0,3.5,2.3125,2.0,4.0,3.375,3.1538461538461537,4.0,4.0,3.5,0.4615384615384615,4.0,1.5,2.7142857142857144,2.333333333333333,3.25,0.5384615384615384,2.0,3.636363636363636,2.6153846153846154,0.75,2.833333333333333,3.4444444444444446,2.6153846153846154,3.111111111111111,0.0,3.75,4.0,0.0,2.0,3.692307692307693,2.5384615384615383,1.6666666666666667,1.3333333333333333,2.3846153846153846,2.857142857142857,3.6,3.375,2.6923076923076925,1.5,3.4,3.2,3.625,3.0,3.4615384615384617,3.466666666666667,3.8125,3.625,4.0,3.25,4.0,4.0,2.4,1.75,3.5,3.0,3.5625,3.466666666666667,2.857142857142857,1.5714285714285714,0.0,3.5,4.0,2.5,1.2,1.0909090909090908,3.0,0.75,2.5,3.1538461538461537,2.6153846153846154,3.25,3.25,4.0,1.3333333333333333,4.0,1.5,3.466666666666667,3.0,3.2,3.5,2.6,4.0,4.0,2.25,4.0,3.8,3.071428571428572,2.8,1.125,2.909090909090909,2.7,3.8,2.071428571428572,2.6,2.0,3.294117647058824,0.0,3.0,3.25,3.533333333333333,3.8125,3.230769230769231,2.6,4.0,3.6,2.4,2.25,3.235294117647059,2.7333333333333334,3.2142857142857144,0.0,3.333333333333333,3.2,3.692307692307693,3.363636363636364,2.6875,3.5384615384615383,3.2666666666666666,3.8,3.8,3.4166666666666665,3.636363636363636,0.0,2.5,3.6,3.75,2.764705882352941,2.625,3.5625,4.0,3.4615384615384617,1.8333333333333333,1.0,0.6666666666666666,3.4615384615384617,3.75,1.5,2.769230769230769,2.571428571428572,4.0,3.8333333333333335,3.0,3.75,1.375,2.769230769230769,3.0,0.0,0.5,2.5,3.076923076923077,1.3333333333333333,3.75,1.3,3.7857142857142856,3.0,4.0,3.4375,2.8181818181818183,2.9285714285714284,1.9285714285714288,4.0,3.8,2.333333333333333,2.375,1.2307692307692308,3.25,3.6,2.588235294117647,3.533333333333333,4.0,0.5,1.6923076923076923,2.933333333333333,1.1875,2.8,2.769230769230769,4.0,3.75,2.9285714285714284,4.0,3.5,1.9230769230769231,2.642857142857143,2.75,2.4285714285714284,3.5384615384615383,3.4615384615384617,4.0,2.8666666666666667,3.8,2.0625,1.0,3.5,3.272727272727273,2.6875,3.769230769230769,3.3076923076923075,2.857142857142857,3.5384615384615383,3.333333333333333,0.3,2.4615384615384617,2.2,3.8461538461538463,3.8125,3.25,4.0,4.0,3.6,3.4375,3.4,3.6666666666666665,3.333333333333333,3.75,4.0,3.4615384615384617,3.7142857142857135,1.6470588235294117,3.1875,2.1538461538461537,4.0,3.625,0.8571428571428571,0.0,4.0,1.5333333333333334,0.4375,2.333333333333333,3.769230769230769,3.6666666666666665,3.5,4.0,3.0,3.333333333333333,3.6875,3.769230769230769,3.0,3.076923076923077,3.0,3.4,1.7142857142857142,3.25,4.0,4.0,1.75,2.3846153846153846,4.0,1.6666666666666667,3.6,2.647058823529412,4.0,4.0,3.3846153846153846,3.2,1.6,3.571428571428572,3.076923076923077,2.0,3.0,3.3076923076923075,1.5,1.4375,2.0,2.125,3.75,4.0,2.25,3.75,3.3846153846153846,2.2,3.3076923076923075,3.0625,3.1666666666666665,3.2666666666666666,3.75,3.75,3.3076923076923075,3.0588235294117645,3.230769230769231,3.533333333333333,4.0,2.4,3.4375,4.0,2.2142857142857144,3.823529411764706,3.2,2.2,2.8461538461538463,3.0,3.076923076923077,3.230769230769231,3.75,3.0,4.0,3.6,3.333333333333333,3.0,3.75,4.0,3.8333333333333335,3.4375,0.5,3.882352941176471,3.2857142857142856,1.5,1.3333333333333333,3.4,2.6,2.6,3.8125,2.6923076923076925,2.4,3.5,3.6666666666666665,3.6,2.6666666666666665,2.9375,2.230769230769231,2.25,2.8461538461538463,4.0,0.75,4.0,3.0,3.5,1.75,4.0,2.8125,3.8125,3.6,2.5,1.2857142857142858,3.4375,2.6923076923076925,3.4,4.0,0.0,3.769230769230769,0.5,0.2857142857142857,3.2,4.0,3.6666666666666665,0.6,3.2,2.7,2.4615384615384617,4.0,3.3125,3.6666666666666665,3.25,0.0,4.0,0.0,4.0,2.75,4.0,3.8,2.0,4.0,4.0,4.0,3.6875,3.5384615384615383,2.357142857142857,3.25,1.8461538461538465,3.6,3.8,3.6666666666666665,4.0,4.0,1.3076923076923077,1.5,3.1333333333333333,3.8125,4.0,3.8,3.625,2.769230769230769,3.769230769230769,3.75,4.0,4.0,3.0,0.0,3.6666666666666665,3.5,2.571428571428572,3.333333333333333,0.5,3.75,3.2142857142857144,3.25,3.75,4.0,2.8125,2.923076923076923,3.4,3.076923076923077,3.75,2.6,4.0,2.8461538461538463,0.0,3.0,3.0,3.272727272727273,3.066666666666667,0.1875,2.923076923076923,4.0,3.2142857142857144,3.4375,3.0625,3.625,3.076923076923077,1.875,1.25,3.75,3.5,3.1875,3.142857142857143,4.0,3.0,4.0,2.555555555555556,3.5,2.875,3.142857142857143,3.375,4.0,4.0,3.3076923076923075,3.75,4.0,1.0,2.125,3.6,1.8,2.0,4.0,1.0,4.0,2.5,3.0,4.0,3.823529411764706,2.6,4.0,3.3076923076923075,2.5384615384615383,3.571428571428572,4.0,3.8181818181818175,2.1666666666666665,2.0,3.727272727272727,4.0,2.25,3.1875,3.75,3.2,1.6666666666666667,3.5,1.1538461538461535,3.230769230769231,3.5,3.75,4.0,3.0,2.8,3.8,2.8,3.4,3.6666666666666665,1.5,3.375,3.2857142857142856,1.7142857142857142,4.0,3.6,3.0,2.75,2.5384615384615383,2.764705882352941,1.2666666666666666,4.0,3.375,3.928571428571429,4.0,1.7272727272727273,3.4615384615384617,2.4285714285714284,3.1538461538461537,3.636363636363636,3.2857142857142856,4.0,4.0,3.4615384615384617,3.769230769230769,0.0,3.5384615384615383,0.375,2.75,3.75,3.1538461538461537,0.8571428571428571,3.4615384615384617,0.9333333333333332,4.0,2.8666666666666667,3.0625,1.9230769230769231,0.5,3.4,3.75,3.5,4.0,3.4375,2.0,0.75,3.071428571428572,2.923076923076923,3.4375,3.8333333333333335,1.2307692307692308,2.0,3.0,3.75,4.0,2.6666666666666665,3.8,3.4,4.0,1.3333333333333333,3.5294117647058822,4.0,3.25,3.2,3.8,3.0625,4.0,3.0,3.3846153846153846,0.0,3.4,4.0,1.5,0.0,4.0,1.5,2.0,3.0,0.0,2.0,4.0,2.375,3.5,3.625,2.8,3.1538461538461537,3.5454545454545454,3.4,2.6923076923076925,3.5,3.230769230769231,1.588235294117647,3.769230769230769,3.5294117647058822,0.0,3.6666666666666665,3.2857142857142856,3.4,4.0,2.375,4.0,2.3846153846153846,3.117647058823529,3.75,3.2,3.5384615384615383,3.230769230769231,3.230769230769231,3.0,0.0,3.25,3.6,3.5384615384615383,2.4,2.923076923076923,3.0,3.0588235294117645,2.0,3.533333333333333,0.2307692307692307,3.4375,2.9285714285714284,3.583333333333333,2.333333333333333,2.8461538461538463,3.6,3.2857142857142856,4.0,3.5384615384615383,2.6,2.2,3.5,3.0,1.0,3.6,2.3076923076923075,2.75,3.0,4.0,4.0,3.0,1.8461538461538465,3.4285714285714284,0.5,2.857142857142857,2.357142857142857,3.0,4.0,3.75,3.6,3.75,0.5,3.125,2.625,4.0,4.0,3.4,3.0,2.533333333333333,3.5,3.4444444444444446,4.0,1.3333333333333333,0.0,3.4,3.769230769230769,3.0,0.0,0.4444444444444444,0.0,3.3,0.1875,3.5,4.0,0.75,0.0,4.0,3.7857142857142856,2.0,3.625,4.0,3.6666666666666665,2.1538461538461537,3.0,3.8,3.8,1.2142857142857142,3.3076923076923075,2.769230769230769,3.071428571428572,4.0,3.6,3.75,3.272727272727273,3.0,4.0,2.9285714285714284,3.4,3.2,4.0,3.3,0.75,2.3076923076923075,2.6666666666666665,3.684210526315789,0.0,1.8461538461538465,3.0,0.0,1.5384615384615383,3.5,4.0,3.25,2.7857142857142856,3.6,2.7,3.6,3.5,0.3333333333333333,3.5,3.769230769230769,3.230769230769231,3.636363636363636,3.25,4.0,3.75,3.8,3.764705882352941,2.636363636363636,3.5625,1.5,3.0,4.0,3.2,4.0,3.4,3.5384615384615383,2.4,0.25,3.0,3.8,3.25,3.0,3.25,2.6,1.7142857142857142,1.1875,3.6,4.0,2.8,4.0,2.7857142857142856,2.75,0.0,2.4285714285714284,1.25,4.0,4.0,3.583333333333333,2.5,4.0,0.2142857142857142,2.3076923076923075,4.0,2.5,4.0,1.8,3.2142857142857144,3.4,1.0,3.25,3.0,4.0,1.25,4.0,2.6666666666666665,3.2,3.142857142857143,2.2,3.0,2.875,3.7857142857142856,3.692307692307693,4.0,3.5,2.2,0.0,0.25,2.25,3.733333333333333,0.0,2.75,1.2857142857142858,1.6363636363636365,4.0,3.4,3.25,3.764705882352941,2.230769230769231,2.75,3.636363636363636,3.769230769230769,0.5,0.5714285714285714,2.75,2.533333333333333,3.3076923076923075,4.0,3.3076923076923075,2.857142857142857,2.75,3.25,3.2,2.5,4.0,2.1,1.4285714285714286,2.6923076923076925,3.8125,3.2857142857142856,1.5333333333333334,3.9375,2.4285714285714284,4.0,4.0,3.25,3.333333333333333,4.0,1.9,3.4615384615384617,0.6,3.2,0.0,3.0,3.9166666666666665,3.076923076923077,3.769230769230769,2.5625,2.1666666666666665,1.2727272727272727,3.0,4.0,2.75,3.4375,2.333333333333333,2.5,2.7857142857142856,4.0,4.0,2.0,3.4615384615384617,4.0,0.0,3.5625,3.25,3.75,3.25,4.0,3.769230769230769,3.4,4.0,4.0,3.2,2.142857142857143,4.0,3.25,3.4,4.0,3.9,4.0,3.6666666666666665,3.2,2.875,0.2,2.625,2.8,3.5294117647058822,2.636363636363636,3.272727272727273,4.0,3.692307692307693,0.0,4.0,2.8461538461538463,3.3125,3.4375,3.0,3.75,3.5,1.6666666666666667,3.4,3.25,3.2,4.0,3.0,3.0,3.4375,3.1538461538461537,0.0,2.0,3.6,2.9166666666666665,3.0625,4.0,0.1538461538461538,2.375,2.4615384615384617,4.0,3.0,3.25,4.0,0.6666666666666666,3.8333333333333335,1.75,3.0,4.0,4.0,2.875,3.769230769230769,2.5384615384615383,3.25,3.8,3.636363636363636,3.5,3.0,3.5384615384615383,2.4615384615384617,4.0,2.0,0.0,1.0,4.0,3.5,4.0,2.4,2.636363636363636,2.6,2.9285714285714284,3.75,3.625,2.333333333333333,3.75,3.25,2.25,3.1875,3.0588235294117645,3.333333333333333,1.1428571428571428,3.625,3.692307692307693,3.0625,4.0,2.6,1.75,1.6666666666666667,3.4375,3.588235294117647,3.571428571428572,1.6153846153846154,2.8461538461538463,3.357142857142857,4.0,3.4615384615384617,4.0,1.25,3.0,3.769230769230769,1.0,3.6,2.9285714285714284,3.230769230769231,3.4,3.4285714285714284,3.7142857142857135,4.0,3.333333333333333,3.0,3.0,3.076923076923077,3.5625,3.888888888888889,1.4615384615384617,4.0,3.75,2.5,3.8125,4.0,3.2,4.0,3.75,3.3529411764705883,2.0,4.0,2.875,2.9375,0.3333333333333333,0.3333333333333333,3.647058823529412,3.3,2.923076923076923,2.9375,2.2142857142857144,0.8181818181818182,3.076923076923077,4.0,3.0,0.0,2.6,4.0,2.75,4.0,3.8,3.0,3.6,3.533333333333333,3.7,3.1875,2.6,1.4615384615384617,2.090909090909091,3.625,4.0,3.3846153846153846,3.1333333333333333,3.5384615384615383,4.0,3.0,3.8,3.6666666666666665,3.0,3.75,2.875,3.75,3.6666666666666665,2.2,0.0,3.142857142857143,1.4,3.1875,3.4615384615384617,4.0,3.8,1.0,3.1538461538461537,4.0,3.8,2.1666666666666665,1.5625,2.25,2.8461538461538463,4.0,3.5,3.25,2.823529411764706,2.6923076923076925,3.0,3.333333333333333,3.4,1.6153846153846154,2.7142857142857144,3.692307692307693,3.0,3.0,4.0,2.75,3.727272727272727,4.0,3.066666666666667,3.5,0.6666666666666666,4.0,3.0,1.5454545454545454,3.25,3.6666666666666665,3.5,3.142857142857143,4.0,3.6666666666666665,3.6666666666666665,3.5384615384615383,2.333333333333333,4.0,4.0,3.5,0.0,3.0,1.6428571428571428,1.0,2.5384615384615383,1.125,1.2,1.6923076923076923,2.6666666666666665,3.75,4.0,0.0,3.0,4.0,1.5,1.7692307692307692,2.333333333333333,2.9285714285714284,3.8125,2.6923076923076925,0.0,3.0,3.5,3.625,3.6,3.1538461538461537,1.5,3.25,3.75,2.0,3.2,2.9375,3.0,3.0,3.6,3.0,3.842105263157895,4.0,3.6666666666666665,3.1875,1.1538461538461535,4.0,2.6,3.4375,0.0,3.0,4.0,2.5,2.0625,4.0,4.0,4.0,1.25,3.75,3.1,4.0,3.142857142857143,4.0,3.5,3.0,3.5,3.571428571428572,3.6666666666666665,3.625,2.5625,2.7142857142857144,2.75,2.230769230769231,3.076923076923077,3.7142857142857135,4.0,1.0909090909090908,3.4705882352941178,3.5384615384615383,3.4285714285714284,4.0,3.4285714285714284,3.8,3.0,0.8888888888888888,3.764705882352941,3.75,3.333333333333333,2.4285714285714284,3.2142857142857144,0.6666666666666666,3.1666666666666665,3.6,2.6666666666666665,1.3333333333333333,1.4705882352941178,2.5,3.692307692307693,3.2857142857142856,3.692307692307693,3.7222222222222223,2.0,3.4,3.75,4.0,2.8,3.25,0.0,4.0,0.0,3.3,3.272727272727273,3.6,4.0,0.0,3.0625,4.0,3.636363636363636,2.4285714285714284,3.5,3.692307692307693,2.923076923076923,2.857142857142857,2.0,1.0769230769230769,1.3333333333333333,1.0,2.4615384615384617,3.7142857142857135,3.142857142857143,4.0,4.0,3.6,3.2,4.0,0.25,2.076923076923077,0.0,3.6,4.0,3.764705882352941,1.0909090909090908,3.230769230769231,1.8888888888888888,4.0,3.2857142857142856,4.0,4.0,3.0,3.6,3.2,2.75,3.25,3.0,3.8,4.0,3.230769230769231,3.5,3.0,3.2,1.375,4.0,3.0,3.875,2.5,3.0,2.923076923076923,0.0,3.769230769230769,3.75,2.0,0.5,3.083333333333333,3.375,2.636363636363636,3.5,3.375,0.6923076923076923,4.0,3.3076923076923075,4.0,2.6,3.333333333333333,3.5625,0.2,4.0,3.6,4.0,2.076923076923077,3.388888888888889,3.4,3.25,2.8461538461538463,2.6923076923076925,3.6666666666666665,1.25,2.75,0.3333333333333333,2.083333333333333,2.7142857142857144,2.6153846153846154,3.533333333333333,3.642857142857143,3.0,4.0,2.6666666666666665,4.0,2.6153846153846154,4.0,0.0,4.0,4.0,3.647058823529412,0.8181818181818182,3.6,4.0,3.25,4.0,2.8,3.4,3.0,2.333333333333333,3.5,3.769230769230769,4.0,4.0,3.333333333333333,3.0625,3.7,3.230769230769231,2.142857142857143,4.0,3.25,2.6923076923076925,2.857142857142857,2.2666666666666666,4.0,2.5384615384615383,0.0,1.7692307692307692,4.0,2.642857142857143,2.0,3.5625,2.142857142857143,3.0,2.6,3.6,2.5,3.6875,4.0,3.4375,3.4,4.0,3.8,1.2307692307692308,3.571428571428572,2.0,2.5,3.25,4.0,2.0,3.8125,3.0,3.5,4.0,4.0,4.0,2.9375,4.0,4.0,4.0,3.4166666666666665,2.5,3.8,3.692307692307693,2.7142857142857144,3.5,3.769230769230769,2.8,3.2,3.0,3.333333333333333,4.0,3.0,4.0,3.2142857142857144,1.9166666666666667,3.375,0.0,3.0,3.6666666666666665,3.6,3.4,3.4,3.4,3.6,0.1333333333333333,1.6153846153846154,3.823529411764706,3.071428571428572,3.076923076923077,1.3333333333333333,4.0,0.0,1.2,3.5,2.071428571428572,3.2,2.3076923076923075,3.533333333333333,1.6666666666666667,4.0,0.7692307692307693,3.8125,3.6666666666666665,2.0,4.0,3.0,2.75,3.071428571428572,2.0,2.0,4.0,3.6,3.5,4.0,3.642857142857143,4.0,2.8,2.9375,3.4,3.076923076923077,3.5,3.533333333333333,1.0769230769230769,0.0,1.3,3.5,1.3333333333333333,2.5,2.4,3.0588235294117645,3.5,2.2,2.25,2.3,2.9,3.7857142857142856,3.636363636363636,3.8,2.0,3.1818181818181817,3.5294117647058822,2.25,3.0,3.571428571428572,2.875,3.0,3.2,3.5625,2.333333333333333,3.8125,3.8125,2.833333333333333,1.3,4.0,2.5,2.75,3.5,1.9230769230769231,3.8,3.235294117647059,2.3076923076923075,3.0,3.0,1.6875,3.1875,3.2666666666666666,4.0,2.333333333333333,1.9285714285714288,2.4615384615384617,3.5,2.5,3.75,3.75,1.0769230769230769,3.230769230769231,4.0,4.0,3.3846153846153846,2.333333333333333,2.636363636363636,3.769230769230769,2.75,4.0,3.235294117647059,4.0,3.25,3.769230769230769,3.8125,2.25,3.0,3.230769230769231,0.6,2.8125,4.0,3.2,3.5454545454545454,3.363636363636364,1.9285714285714288,4.0,4.0,3.733333333333333,3.076923076923077,2.4615384615384617,3.5,3.0,3.6,4.0,4.0,1.1,2.8,2.5,2.7142857142857144,3.8125,0.0,3.5625,1.0,3.2,4.0,4.0,4.0,4.0,3.071428571428572,2.5454545454545454,0.0,3.625,2.333333333333333,2.8,3.1333333333333333,3.9,2.0,2.111111111111111,3.363636363636364,2.4285714285714284,1.6,2.2142857142857144,3.3529411764705883,3.142857142857143,0.25,2.571428571428572,1.5,3.4,2.9285714285714284,3.0,2.9375,2.6,2.6,3.4375,3.6,0.6666666666666666,1.0769230769230769,0.0,4.0,3.769230769230769,1.2,1.0,2.25,1.8,1.75,2.4,0.0,3.769230769230769,4.0,3.25,0.0,3.8,2.5625,3.0,4.0,3.2,4.0,2.1875,3.0,1.2,2.533333333333333,0.0,3.8333333333333335,4.0,0.0,2.8125,4.0,2.0,0.25,0.0,3.4166666666666665,4.0,4.0,4.0,3.0,2.0,2.8,3.615384615384616,3.235294117647059,3.25,1.4285714285714286,3.8,3.75,1.7333333333333334,3.1333333333333333,2.6666666666666665,3.75,2.6,3.25,2.75,3.588235294117647,2.642857142857143,2.875,2.066666666666667,4.0,3.4,4.0,2.6923076923076925,2.25,3.8,3.8,0.0,3.769230769230769,3.1333333333333333,3.5384615384615383,2.9285714285714284,2.0,3.5384615384615383,3.2,3.25,1.6153846153846154,4.0,3.3,3.0,2.0,2.0,2.0,1.25,3.571428571428572,4.0,3.6,4.0,2.4,3.4,2.75,2.588235294117647,1.8,2.411764705882353,4.0,3.466666666666667,4.0,2.7333333333333334,3.0,2.5,4.0,1.5,3.769230769230769,2.5,2.7857142857142856,3.5384615384615383,0.0,2.333333333333333,3.8125,3.1875,3.357142857142857,2.857142857142857,2.333333333333333,3.0,3.0,3.5,1.875,3.25,4.0,2.5,3.6,4.0,1.0,4.0,3.727272727272727,4.0,4.0,2.75,4.0,3.0,2.909090909090909,3.8125,2.076923076923077,2.6,1.8,3.0,3.0,4.0,4.0,1.0,3.75,1.9230769230769231,3.8125,3.4166666666666665,1.7,4.0,4.0,3.8125,3.0,4.0,4.0,3.2,4.0,4.0,4.0,3.083333333333333,4.0,2.857142857142857,3.333333333333333,3.8125,3.4,2.75,2.333333333333333,3.8,0.875,1.75,4.0,3.1538461538461537,2.2,2.583333333333333,1.5454545454545454,2.375,4.0,1.8,3.625,3.75,2.6,3.8,2.857142857142857,3.3076923076923075,2.8,2.25,3.125,2.7857142857142856,4.0,3.533333333333333,2.1538461538461537,4.0,4.0,2.294117647058824,1.5,2.2,4.0,3.8125,1.3333333333333333,2.9166666666666665,3.0,4.0,1.1428571428571428,3.363636363636364,2.0,0.0,2.333333333333333,2.75,2.5454545454545454,3.0,3.230769230769231,1.4444444444444444,0.6,3.076923076923077,2.6153846153846154,4.0,3.1875,3.75,3.5,3.636363636363636,3.4,3.4615384615384617,3.25,2.6153846153846154,3.0,4.0,4.0,0.0,3.230769230769231,3.5384615384615383,3.4,3.4375,4.0,2.3076923076923075,2.0,0.0,2.1,3.6666666666666665,4.0,2.769230769230769,2.833333333333333,3.5384615384615383,4.0,4.0,2.25,3.6666666666666665,4.0,2.642857142857143,3.5,2.9,0.0,0.0,3.3125,3.142857142857143,3.625,3.0,4.0,3.769230769230769,3.6,2.1875,1.75,2.066666666666667,4.0,3.8125,2.8,3.75,3.7142857142857135,0.8,2.1538461538461537,2.2,3.411764705882353,4.0,3.8,2.0588235294117645,4.0,4.0,3.8,0.75,2.6,3.533333333333333,3.75,3.8,4.0,1.7142857142857142,3.75,3.2142857142857144,3.25,3.5,2.25,1.6428571428571428,3.6,3.75,3.5,1.5454545454545454,3.764705882352941,3.923076923076923,4.0,3.769230769230769,2.6,4.0,0.9166666666666666,3.3125,1.6,2.333333333333333,0.4,3.333333333333333,2.75,0.5,2.571428571428572,3.4,3.3125,1.2307692307692308,0.0,4.0,4.0,2.6,4.0,0.0,4.0,3.6,2.235294117647059,1.5,3.0,0.5,2.4615384615384617,3.4615384615384617,3.7,2.8,3.230769230769231,4.0,1.0833333333333333,2.75,3.6,2.0,4.0,3.692307692307693,3.0,4.0,3.4444444444444446,3.2142857142857144,4.0,3.6,3.4,1.0769230769230769,2.6666666666666665,3.25,3.3846153846153846,3.0625,0.0,0.0,4.0,3.294117647058824,3.8125,2.0,2.5,3.5384615384615383,2.0,0.0,3.3529411764705883,2.3076923076923075,4.0,3.2,3.6,4.0,3.4,3.6,3.0,3.4375,4.0,0.0,4.0,3.4,2.4615384615384617,3.4,3.8125,2.6666666666666665,3.5384615384615383,4.0,3.2,3.1875,4.0,3.25,1.1,3.571428571428572,2.2,3.6666666666666665,3.2857142857142856,3.6,4.0,3.125,4.0,2.6923076923076925,0.0,3.333333333333333,4.0,1.3333333333333333,4.0,2.769230769230769,0.9230769230769232,4.0,0.0,2.9375,0.0,0.0,3.25,3.6,2.642857142857143,2.9285714285714284,2.75,3.090909090909091,3.4,3.727272727272727,3.25,2.75,2.076923076923077,2.857142857142857,4.0,4.0,1.75,2.5,3.6,0.5,1.75,4.0,3.75,4.0,3.8,4.0,3.6666666666666665,4.0,2.0,3.0,2.8,3.769230769230769,2.230769230769231,3.769230769230769,3.1538461538461537,2.8,2.1666666666666665,0.0,4.0,2.5,4.0,2.0,2.5,4.0,2.571428571428572,3.0,3.583333333333333,4.0,3.4705882352941178,3.363636363636364,0.0,2.9,1.8,3.769230769230769,2.4,1.375,2.466666666666667,3.3076923076923075,3.769230769230769,3.8125,3.066666666666667,3.6666666666666665,3.6,3.230769230769231,1.0,2.4444444444444446,3.684210526315789,4.0,2.1818181818181817,4.0,2.6875,0.375,3.142857142857143,3.0,1.3333333333333333,3.0,3.6666666666666665,2.4166666666666665,0.0,4.0,3.0,3.2222222222222223,3.7857142857142856,3.4615384615384617,3.5,3.25,4.0,3.0625,3.8,2.4166666666666665,1.2857142857142858,3.3125,3.8125,2.7333333333333334,3.571428571428572,3.3076923076923075,4.0,3.4,3.466666666666667,4.0,0.0,0.0,3.533333333333333,3.3846153846153846,2.4285714285714284,3.5625,2.0,3.6666666666666665,2.769230769230769,3.4705882352941178,4.0,3.625,3.769230769230769,1.0,3.642857142857143,0.25,3.75,4.0,2.0,2.75,2.5,3.5,4.0,4.0,4.0,1.2142857142857142,2.8125,0.0,3.333333333333333,3.2,4.0,3.4615384615384617,1.4285714285714286,3.769230769230769,3.5,3.090909090909091,3.3,3.5294117647058822,3.769230769230769,1.7142857142857142,3.2,2.4,3.7142857142857135,4.0,4.0,2.933333333333333,3.75,3.75,3.5384615384615383,3.142857142857143,0.0,3.7142857142857135,4.0,3.066666666666667,3.2142857142857144,3.5,3.692307692307693,4.0,3.733333333333333,4.0,4.0,2.3,3.25,3.5,3.3125,4.0,3.2666666666666666,3.6,4.0,3.0,3.142857142857143,3.230769230769231,3.4,3.466666666666667,3.333333333333333,0.0,0.0,3.0,3.8,2.5,3.5,2.5625,3.5384615384615383,4.0,3.125,0.0,3.6,2.75,1.1111111111111112,1.0,3.125,3.0,4.0,3.5294117647058822,2.5,2.5625,2.5,3.142857142857143,4.0,2.076923076923077,0.75,2.6666666666666665,3.466666666666667,2.75,0.5,4.0,3.4166666666666665,3.1666666666666665,3.4615384615384617,3.1538461538461537,3.7142857142857135,2.6923076923076925,3.0,3.3076923076923075,3.0,3.0,2.2,4.0,0.8,3.0,3.75,3.5,3.5,3.0,0.0,3.75,2.764705882352941,1.4166666666666667,2.5384615384615383,0.5,0.4615384615384615,2.4166666666666665,3.7857142857142856,3.230769230769231,3.8125,3.0,0.0,2.6875,3.0,2.8,2.1875,4.0,3.4,3.8,1.0,2.769230769230769,3.5384615384615383,3.4,3.636363636363636,3.5,2.9375,0.5,2.8461538461538463,4.0,3.6,0.0,3.5384615384615383,3.076923076923077,2.4,3.5384615384615383,4.0,3.1333333333333333,2.25,1.5,3.25,2.8,3.4705882352941178,4.0,3.0,4.0,0.0,2.769230769230769,4.0,3.2142857142857144,2.6,3.090909090909091,4.0,2.6666666666666665,3.8125,0.0,3.75,4.0,3.75,3.4375,3.25,3.692307692307693,4.0,3.823529411764706,3.8,3.25,3.583333333333333,3.769230769230769,3.3076923076923075,3.4,4.0,3.75,3.933333333333333,3.076923076923077,3.5,4.0,3.411764705882353,0.0,2.125,4.0,4.0,2.6923076923076925,4.0,2.769230769230769,0.6666666666666666,3.5384615384615383,2.857142857142857,3.75,3.25,1.3333333333333333,2.75,1.7692307692307692,3.75,3.642857142857143,4.0,4.0,2.888888888888889,4.0,0.0,3.6,3.636363636363636,4.0,3.1538461538461537,4.0,2.4,4.0,0.0,4.0,3.8,1.4,3.75,1.0909090909090908,0.6428571428571429,1.2,4.0,3.230769230769231,3.6,3.0,2.9166666666666665,1.9,3.388888888888889,2.857142857142857,1.6153846153846154,3.8,0.0,2.4,4.0,4.0,3.5384615384615383,3.076923076923077,0.5,1.0,3.2,3.4615384615384617,1.75,4.0,3.764705882352941,4.0,3.0,3.0,2.764705882352941,3.25,1.2727272727272727,3.272727272727273,3.647058823529412,2.8,3.6,3.0,3.8,2.7142857142857144,2.8,3.076923076923077,2.8461538461538463,0.0,3.0,3.7142857142857135,2.25,1.6,0.0,1.875,3.5,3.692307692307693,4.0,0.6,4.0,4.0,2.272727272727273,4.0,3.466666666666667,3.769230769230769,0.6666666666666666,2.8,0.5,2.5,2.6666666666666665,3.333333333333333,2.4285714285714284,3.230769230769231,3.3125,3.3846153846153846,3.571428571428572,4.0,4.0,3.0,0.0,2.0,2.6923076923076925,4.0,2.6666666666666665,4.0,3.4375,0.3333333333333333,4.0,3.2,3.125,2.8461538461538463,3.8,3.375,2.333333333333333,3.4,3.733333333333333,3.5,3.0,2.7,3.692307692307693,3.0,4.0,3.7857142857142856,3.2,1.375,3.4,3.0,3.375,2.75,2.4615384615384617,4.0,2.823529411764706,1.2,4.0,3.769230769230769,3.0,2.875,3.2,3.25,3.7857142857142856,4.0,2.7777777777777777,4.0,3.692307692307693,0.0,3.6,4.0,3.6,2.0,0.5,0.0,2.4,2.25,4.0,2.25,4.0,2.1538461538461537,3.2142857142857144,4.0,4.0,2.4615384615384617,4.0,4.0,2.25,0.0,4.0,2.6666666666666665,4.0,4.0,0.0,3.5,4.0,3.8125,3.692307692307693,3.0,0.2307692307692307,3.5384615384615383,4.0,2.769230769230769,4.0,4.0,3.0,0.0,3.25,4.0,3.25,2.7142857142857144,3.25,2.333333333333333,3.5384615384615383,0.0,1.5,2.6666666666666665,2.076923076923077,3.4,0.0,3.75,2.4,3.8,4.0,2.4285714285714284,1.4615384615384617,3.3125,3.4615384615384617,4.0,3.5,4.0,3.333333333333333,3.625,4.0,2.5,3.0,2.8125,2.5,4.0,2.4615384615384617,3.8125,2.75,3.5,2.7142857142857144,2.923076923076923,3.071428571428572,2.75,3.75,3.5,2.25,3.4,4.0,3.5454545454545454,3.2857142857142856,3.4,3.25,3.0,3.0,3.733333333333333,3.733333333333333,2.555555555555556,2.923076923076923,3.75,2.6923076923076925,3.5,1.3846153846153846,0.0,3.5,0.0,3.6875,2.466666666666667,3.4,4.0,0.0,2.7857142857142856,4.0,3.4,4.0,3.066666666666667,3.4615384615384617,4.0,3.75,3.5384615384615383,2.625,3.0,3.75,3.6666666666666665,1.3333333333333333,3.6,3.692307692307693,3.25,2.25,2.75,1.2857142857142858,0.6666666666666666,3.2,0.0,2.8,3.647058823529412,2.6153846153846154,3.0,2.25,3.142857142857143,2.7142857142857144,4.0,2.5,2.5,3.25,2.0,2.571428571428572,3.8461538461538463,2.2142857142857144,3.0,2.823529411764706,3.2666666666666666,3.571428571428572,0.875,3.25,4.0,3.4615384615384617,3.0,2.6923076923076925,2.75,3.2,4.0,2.6153846153846154,0.0,2.066666666666667,3.2142857142857144,0.0,2.9375,2.5625,0.25,4.0,4.0,2.3529411764705883,3.2,3.0,0.0,3.0,3.8125,3.5454545454545454,2.769230769230769,2.25,2.0,3.692307692307693,4.0,2.8461538461538463,4.0,3.1875,2.333333333333333,3.5,1.9375,0.3846153846153846,3.5,3.6,4.0,2.5,4.0,3.333333333333333,3.4,4.0,0.4,3.3076923076923075,3.7142857142857135,3.625,3.692307692307693,3.625,3.8,3.5,2.875,4.0,3.7,4.0,3.083333333333333,3.4615384615384617,3.25,3.0625,2.8461538461538463,4.0,2.2857142857142856,4.0,3.8125,3.5,3.0,2.571428571428572,4.0,2.875,1.5,2.3076923076923075,3.6666666666666665,4.0,3.4615384615384617,3.5,3.2857142857142856,3.769230769230769,4.0,2.8,3.0,2.8,4.0,3.25,1.8333333333333333,3.0,3.25,3.3076923076923075,0.0,3.6666666666666665,4.0,3.7,2.75,3.071428571428572,3.533333333333333,2.6666666666666665,3.75,3.230769230769231,4.0,0.0,3.3076923076923075,4.0,3.0,3.4285714285714284,2.4,1.4375,4.0,2.2,3.5,3.75,3.615384615384616,4.0,3.2142857142857144,3.769230769230769,4.0,2.0,4.0,1.2857142857142858,3.2142857142857144,2.75,3.8,0.8,3.2,3.571428571428572,4.0,1.0,3.0,1.6428571428571428,2.2,1.0,1.5,2.7142857142857144,3.625,1.0,3.0,3.0,3.764705882352941,1.5,3.25,3.0,3.5,2.2,1.8461538461538465,3.75,4.0,3.357142857142857,2.8666666666666667,3.769230769230769,3.2857142857142856,3.230769230769231,4.0,0.6666666666666666,3.0,2.769230769230769,3.0,3.375,3.7142857142857135,4.0,3.0,3.076923076923077,3.6,4.0,0.3333333333333333,3.75,2.75,0.0,1.25,3.727272727272727,3.411764705882353,2.333333333333333,0.0,3.8,3.75,2.8125,4.0,3.615384615384616,0.5833333333333334,2.9285714285714284,3.75,3.8125,2.0,3.2,3.7,4.0,2.230769230769231,3.8,3.6666666666666665,4.0,4.0,3.4,2.75,3.076923076923077,2.3846153846153846,2.7777777777777777,4.0,3.733333333333333,3.117647058823529,4.0,2.142857142857143,3.5,0.0,2.071428571428572,3.823529411764706,0.0,3.4615384615384617,2.4615384615384617,4.0,0.0,3.8125,3.625,0.4615384615384615,2.0,4.0,4.0,3.142857142857143,4.0,4.0,1.7142857142857142,3.8125,1.8333333333333333,2.6153846153846154,2.4615384615384617,4.0,3.75,3.25,4.0,1.6153846153846154,1.4666666666666666,3.733333333333333,4.0,4.0,3.75,2.4375,4.0,3.1333333333333333,3.4166666666666665,3.0526315789473686,2.8,3.0,0.0,3.333333333333333,2.571428571428572,3.4615384615384617,3.4375,3.3846153846153846,0.25,4.0,2.8,3.0,2.1538461538461537,0.0,3.6,0.75,3.1538461538461537,3.2,0.5,3.071428571428572,3.8125,3.2,2.6,3.5,3.3125,2.909090909090909,2.75,4.0,2.8125,3.2142857142857144,2.2,0.0,3.076923076923077,3.4285714285714284,1.0,4.0,0.2307692307692307,3.0,2.8,4.0,4.0,3.7142857142857135,3.571428571428572,2.0,2.3125,4.0,2.230769230769231,2.6666666666666665,0.3076923076923077,3.571428571428572,2.833333333333333,3.5,3.411764705882353,2.375,0.0,4.0,3.642857142857143,3.357142857142857,3.0625,3.6,0.0,3.272727272727273,3.6666666666666665,4.0,2.0,2.0,2.769230769230769,3.75,4.0,1.75,4.0,2.0,1.3333333333333333,3.625,2.111111111111111,0.1875,3.5384615384615383,3.272727272727273,2.933333333333333,3.0,3.5294117647058822,4.0,3.0,3.625,4.0,3.2,2.6,3.6666666666666665,3.0,2.9166666666666665,3.0,0.9230769230769232,2.0,3.8,3.8125,3.5,2.4,3.857142857142857,4.0,3.0,1.3333333333333333,3.5,4.0,3.5,2.4615384615384617,3.076923076923077,3.2142857142857144,3.7857142857142856,4.0,2.142857142857143,1.8125,1.5,1.8333333333333333,3.727272727272727,2.9166666666666665,3.3076923076923075,0.3076923076923077,3.0588235294117645,3.7857142857142856,3.4,2.75,2.176470588235294,3.75,3.5,2.0,3.6,3.5,2.9375,2.571428571428572,4.0,1.6153846153846154,3.6,4.0,3.8,3.625,1.8666666666666667,4.0,2.0625,0.6923076923076923,1.3636363636363635,0.0,3.6,2.9166666666666665,3.5625,3.4615384615384617,1.8461538461538465,4.0,0.2142857142857142,4.0,1.6,2.8,1.25,3.4285714285714284,3.625,1.9375,4.0,0.0,3.733333333333333,3.4615384615384617,4.0,1.6,2.1538461538461537,3.230769230769231,2.2142857142857144,4.0,3.3846153846153846,4.0,3.083333333333333,0.6,3.6666666666666665,3.2142857142857144,3.5,4.0,1.0,0.5,4.0,0.25,3.25,0.0,0.0,3.2,4.0,3.375,2.857142857142857,0.6666666666666666,2.333333333333333,3.5625,3.2,3.2857142857142856,2.090909090909091,3.272727272727273,1.6923076923076923,4.0,3.3076923076923075,3.4,2.642857142857143,3.0,4.0,2.6153846153846154,3.8,1.8571428571428568,3.6,4.0,3.2,3.733333333333333,3.1875,4.0,3.692307692307693,2.6153846153846154,2.933333333333333,2.875,0.0,3.0625,3.7142857142857135,0.0,4.0,3.2,1.1538461538461535,2.5454545454545454,4.0,2.7857142857142856,2.75,1.2307692307692308,3.0,2.642857142857143,4.0,3.5,3.8125,3.4,3.6,3.4,4.0,2.2857142857142856,3.071428571428572,4.0,2.25,3.2,3.333333333333333,1.6,1.8461538461538465,3.3529411764705883,2.294117647058824,3.0,4.0,3.75,3.692307692307693,3.6,3.375,3.25,3.8125,2.3,2.8125,4.0,3.0,3.333333333333333,3.5,4.0,2.75,3.2142857142857144,1.6666666666666667,3.3529411764705883,3.6666666666666665,3.8125,0.0,3.75,1.1538461538461535,3.375,3.823529411764706,1.6666666666666667,3.769230769230769,3.8125,2.5384615384615383,3.588235294117647,2.5,0.9230769230769232,3.8125,2.6666666666666665,0.0,3.230769230769231,3.625,3.625,3.5,0.0,0.0,3.4,3.0,2.8461538461538463,3.5,1.6,0.0,3.692307692307693,3.5384615384615383,2.2857142857142856,3.769230769230769,3.692307692307693,3.5,4.0,3.4375,2.909090909090909,3.0,3.7,3.2,3.692307692307693,3.0,4.0,4.0,4.0,0.0,2.8125,0.4285714285714285,4.0,3.5294117647058822,3.692307692307693,3.0,3.6,3.4,3.5454545454545454,3.6,2.2,1.7692307692307692,2.9411764705882355,3.0,3.4285714285714284,3.375,3.466666666666667,1.6363636363636365,0.75,1.7,3.4615384615384617,1.8461538461538465,3.7857142857142856,4.0,3.636363636363636,3.0,1.9230769230769231,0.0,2.625,3.5625,2.5,1.8571428571428568,2.363636363636364,3.8125,4.0,2.4,2.5,2.7142857142857144,3.0,3.230769230769231,3.3846153846153846,3.142857142857143,0.0,2.6,3.0,3.8,4.0,3.6666666666666665,2.5,1.9090909090909087,3.0,3.8125,3.142857142857143,0.2727272727272727,3.0,3.2,3.4285714285714284,2.769230769230769,4.0,1.0,2.25,3.8,4.0,4.0,4.0,4.0,4.0,2.8461538461538463,2.4375,0.0,3.2,3.090909090909091,4.0,3.625,0.8888888888888888,0.0,3.4285714285714284,2.5384615384615383,3.533333333333333,3.8125,0.0,2.923076923076923,3.2,3.375,3.5,0.0,0.7692307692307693,4.0,3.333333333333333,4.0,3.1666666666666665,1.7692307692307692,3.8666666666666663,3.8125,3.25,2.4545454545454546,3.8,3.2666666666666666,3.769230769230769,3.8,3.0,3.6666666666666665,4.0,0.0,3.333333333333333,0.0,3.6666666666666665,2.6153846153846154,3.0,1.3333333333333333,2.6666666666666665,3.75,2.0,2.8125,3.2857142857142856,3.6,0.0,3.4,4.0,2.833333333333333,3.692307692307693,3.5,3.625,2.75,4.0,1.3846153846153846,3.5,3.6,3.1538461538461537,3.0,3.8333333333333335,3.8333333333333335,3.5,4.0,3.375,3.7142857142857135,3.357142857142857,3.0,4.0,2.9,3.1666666666666665,0.0,3.8,3.0,2.8461538461538463,3.571428571428572,1.8571428571428568,3.8,3.75,3.357142857142857,2.25,3.111111111111111,2.1,2.8,4.0,3.8125,3.769230769230769,3.1,2.333333333333333,4.0,4.0,3.75,2.076923076923077,3.076923076923077,2.4166666666666665,3.5,1.5,4.0,1.9230769230769231,0.6923076923076923,0.0,3.25,3.25,4.0,2.25,3.2,2.769230769230769,3.5,3.6,3.0,3.75,2.5625,3.3,4.0,3.5,3.25,3.625,4.0,1.3333333333333333,0.9090909090909092,4.0,3.5384615384615383,3.625,1.25,2.75,2.4705882352941178,2.8,3.5294117647058822,2.857142857142857,3.2857142857142856,3.0,1.6666666666666667,3.357142857142857,0.75,3.583333333333333,2.75,3.8,4.0,2.6923076923076925,2.6923076923076925,1.6666666666666667,1.0,3.5625,3.6666666666666665,4.0,2.5,0.6666666666666666,3.8,3.2666666666666666,3.375,1.8333333333333333,3.7777777777777777,2.642857142857143,3.466666666666667,3.5625,4.0,1.75,3.25,2.0,3.2,0.0,3.4545454545454546,3.625,3.076923076923077,2.3,2.0,4.0,3.4,3.625,4.0,3.25,0.0,4.0,3.0,0.0,2.0,4.0,3.25,3.75,0.0,4.0,2.6,2.5,2.6153846153846154,3.4,4.0,1.5454545454545454,0.0,3.8,3.6,1.4166666666666667,4.0,3.4615384615384617,2.833333333333333,3.0,1.7692307692307692,4.0,3.6,3.769230769230769,3.230769230769231,3.076923076923077,3.9375,2.090909090909091,3.4,2.4,3.0,3.75,0.0,0.0,4.0,4.0,4.0,3.0,3.692307692307693,2.8,2.4,2.071428571428572,3.076923076923077,4.0,3.0,2.076923076923077,3.4705882352941178,3.571428571428572,3.363636363636364,4.0,4.0,2.6666666666666665,2.0,4.0,2.3076923076923075,3.6,4.0,3.4,3.8,3.625,2.6,0.5,2.4,3.4,4.0,4.0,2.75,3.25,3.0,1.5,3.3,3.5384615384615383,3.1,3.0,4.0,2.0,4.0,3.5625,3.176470588235294,3.6666666666666665,3.4285714285714284,4.0,3.8125,2.7857142857142856,3.333333333333333,0.0,4.0,4.0,2.25,3.3076923076923075,4.0,4.0,2.9285714285714284,1.3,2.142857142857143,2.25,3.230769230769231,2.0,3.8,4.0,1.6666666666666667,3.411764705882353,3.4615384615384617,2.75,4.0,2.75,1.5384615384615383,3.625,2.764705882352941,3.533333333333333,3.5,2.1875,1.9375,2.6,2.75,0.25,1.25,2.6875,3.769230769230769,3.25,3.4285714285714284,1.818181818181818,4.0,2.6153846153846154,3.1333333333333333,3.0,3.1875,3.4615384615384617,3.25,3.4,0.0,3.0,0.75,2.6,2.5,3.5384615384615383,3.4,3.4,3.625,3.4,2.1818181818181817,2.875,0.0,3.8,2.7,1.1538461538461535,3.5,3.8125,4.0,3.8125,3.823529411764706,4.0,0.8,0.0,3.4375,3.75,4.0,3.3846153846153846,2.3846153846153846,3.625,0.5,2.1,4.0,3.5384615384615383,3.7142857142857135,0.0,2.5,2.8461538461538463,4.0,3.769230769230769,3.4375,3.2857142857142856,3.176470588235294,3.083333333333333,3.8,4.0,4.0,2.8,1.3333333333333333,2.9285714285714284,4.0,4.0,2.0,2.6,3.4615384615384617,3.8,3.8,3.5,4.0,4.0,2.5,3.8125,4.0,2.8,3.4615384615384617,3.4615384615384617,4.0,2.923076923076923,3.4615384615384617,3.4,1.3636363636363635,4.0,2.0,3.4166666666666665,3.333333333333333,4.0,3.5,3.533333333333333,3.75,3.1875,2.8,3.0,4.0,3.1666666666666665,2.7,3.0,2.2,2.4166666666666665,3.0,3.2666666666666666,3.75,4.0,0.0,4.0,3.0,3.0,3.071428571428572,1.0,3.357142857142857,3.533333333333333,3.75,4.0,2.0,2.5,3.0,2.6,3.0,0.0,2.8125,4.0,0.0,2.6875,3.8,1.9230769230769231,3.083333333333333,0.0,3.6666666666666665,3.5,4.0,4.0,1.0,4.0,3.4,3.125,3.625,3.25,2.8823529411764706,0.1538461538461538,3.75,4.0,3.692307692307693,4.0,4.0,0.0,3.5625,4.0,3.2666666666666666,1.1538461538461535,2.0,1.9166666666666667,2.5,4.0,3.0,3.588235294117647,3.5,3.3076923076923075,3.333333333333333,1.3333333333333333,0.5,2.5625,0.6666666666666666,3.25,3.8,3.0,4.0,3.5,1.6666666666666667,2.923076923076923,3.0,3.071428571428572,3.5,3.2142857142857144,2.571428571428572,2.7857142857142856,2.230769230769231,4.0,1.75,3.7,3.333333333333333,4.0,4.0,2.333333333333333,0.75,1.9444444444444444,3.4,1.0,3.1875,3.4,4.0,3.375,2.7,2.6,3.8125,3.363636363636364,3.0,3.75,3.2142857142857144,2.6153846153846154,1.3333333333333333,2.0,2.642857142857143,1.25,2.2,3.533333333333333,3.4375,3.2142857142857144,4.0,3.5,1.3333333333333333,4.0,3.2,2.6923076923076925,0.0,0.0,1.7333333333333334,4.0,2.8,4.0,1.5,0.4,2.6666666666666665,4.0,0.6428571428571429,3.125,4.0,3.0,2.3846153846153846,0.0,2.923076923076923,3.4285714285714284,2.0,3.4,2.769230769230769,2.8,2.75,3.7142857142857135,2.8,2.4285714285714284,3.8125,1.0769230769230769,2.0,1.2,4.0,4.0,3.0,1.5,3.5,4.0,2.0,3.8,2.75,3.230769230769231,2.857142857142857,4.0,3.933333333333333,3.230769230769231,3.1818181818181817,4.0,2.6,4.0,2.0,3.692307692307693,2.8,2.5,3.823529411764706,4.0,3.5,3.25,3.272727272727273,0.0,3.4,2.8181818181818183,3.8125,4.0,4.0,3.75,3.764705882352941,1.5,1.3333333333333333,3.5,3.3076923076923075,3.5,3.375,4.0,3.769230769230769,1.3846153846153846,2.375,3.6,2.5384615384615383,3.083333333333333,3.2,3.6,2.4,2.0,4.0,2.090909090909091,3.5625,3.363636363636364,1.0,3.25,4.0,3.75,0.8461538461538461,4.0,2.5,2.3076923076923075,1.9166666666666667,0.8461538461538461,2.25,4.0,4.0,1.0,2.8461538461538463,2.5,3.0,1.8461538461538465,0.0,2.111111111111111,3.1875,3.6,4.0,3.769230769230769,1.0,2.0,3.75,2.625,2.333333333333333,4.0,3.8,2.0,3.5,3.733333333333333,3.0,4.0,4.0,4.0,3.2,4.0,3.0,1.5,3.333333333333333,0.75,3.75,3.75,3.0,3.4,2.9375,3.0,4.0,2.9375,3.6,3.7142857142857135,4.0,3.5384615384615383,2.0,3.4,3.6666666666666665,3.5,2.25,3.6666666666666665,4.0,4.0,2.923076923076923,3.0,0.0,3.8,3.4285714285714284,3.5384615384615383,4.0,4.0,4.0,1.5,2.75,3.583333333333333,4.0,2.0,3.25,4.0,2.6923076923076925,0.0,3.2857142857142856,1.625,4.0,4.0,2.1818181818181817,4.0,3.375,4.0],\"xaxis\":\"x\",\"y\":[15.0,12.0,11.0,16.0,14.0,15.0,0.0,6.0,12.0,9.0,16.0,9.0,13.0,3.0,0.0,13.0,16.0,0.0,15.0,10.0,16.0,17.0,14.0,16.0,17.0,13.0,12.0,13.0,12.0,6.0,12.0,6.0,14.0,16.0,13.0,12.0,16.0,0.0,15.0,13.0,13.0,12.0,15.0,7.0,16.0,8.0,17.0,6.0,16.0,15.0,11.0,5.0,16.0,16.0,2.0,6.0,4.0,17.0,14.0,8.0,0.0,12.0,15.0,15.0,16.0,13.0,9.0,10.0,3.0,9.0,16.0,13.0,16.0,13.0,17.0,3.0,15.0,13.0,17.0,13.0,10.0,13.0,16.0,6.0,16.0,6.0,0.0,12.0,13.0,16.0,0.0,12.0,14.0,15.0,14.0,12.0,14.0,14.0,10.0,11.0,15.0,12.0,17.0,14.0,16.0,15.0,14.0,0.0,14.0,16.0,3.0,9.0,14.0,15.0,14.0,12.0,15.0,12.0,15.0,12.0,13.0,15.0,15.0,15.0,15.0,12.0,15.0,15.0,14.0,12.0,13.0,16.0,15.0,11.0,14.0,12.0,3.0,12.0,6.0,14.0,7.0,13.0,8.0,12.0,20.0,16.0,16.0,0.0,13.0,12.0,15.0,6.0,15.0,14.0,0.0,13.0,16.0,14.0,12.0,13.0,0.0,6.0,11.0,6.0,17.0,12.0,15.0,3.0,0.0,15.0,15.0,12.0,16.0,15.0,16.0,14.0,12.0,16.0,12.0,13.0,16.0,13.0,3.0,15.0,15.0,16.0,11.0,14.0,16.0,12.0,14.0,7.0,11.0,12.0,17.0,12.0,0.0,16.0,15.0,15.0,12.0,7.0,15.0,4.0,8.0,14.0,13.0,9.0,11.0,16.0,13.0,15.0,9.0,1.0,0.0,10.0,15.0,0.0,12.0,12.0,12.0,15.0,13.0,12.0,7.0,18.0,16.0,13.0,15.0,13.0,9.0,12.0,13.0,12.0,12.0,7.0,10.0,15.0,15.0,13.0,14.0,13.0,13.0,12.0,16.0,0.0,16.0,0.0,12.0,9.0,12.0,12.0,12.0,6.0,16.0,12.0,12.0,13.0,11.0,14.0,16.0,16.0,13.0,14.0,16.0,16.0,0.0,12.0,6.0,13.0,12.0,14.0,16.0,12.0,2.0,16.0,15.0,3.0,9.0,15.0,15.0,14.0,11.0,0.0,17.0,3.0,13.0,9.0,14.0,16.0,15.0,13.0,15.0,0.0,15.0,13.0,10.0,10.0,3.0,16.0,7.0,13.0,16.0,16.0,16.0,13.0,12.0,13.0,12.0,17.0,12.0,16.0,4.0,15.0,3.0,12.0,16.0,6.0,3.0,12.0,16.0,16.0,8.0,12.0,16.0,15.0,0.0,0.0,15.0,12.0,6.0,13.0,16.0,16.0,16.0,4.0,18.0,16.0,14.0,12.0,15.0,12.0,14.0,13.0,12.0,0.0,14.0,13.0,0.0,17.0,17.0,13.0,12.0,8.0,16.0,7.0,9.0,15.0,17.0,0.0,9.0,14.0,15.0,12.0,17.0,1.0,15.0,5.0,9.0,16.0,16.0,6.0,6.0,3.0,15.0,1.0,18.0,0.0,12.0,12.0,13.0,9.0,0.0,16.0,13.0,12.0,15.0,7.0,16.0,13.0,7.0,9.0,13.0,12.0,16.0,12.0,12.0,13.0,9.0,12.0,14.0,12.0,15.0,13.0,12.0,6.0,17.0,14.0,13.0,13.0,16.0,6.0,9.0,16.0,15.0,14.0,13.0,4.0,12.0,16.0,16.0,6.0,12.0,3.0,15.0,13.0,16.0,16.0,12.0,16.0,14.0,17.0,14.0,16.0,17.0,13.0,10.0,10.0,3.0,16.0,13.0,11.0,17.0,13.0,13.0,3.0,16.0,10.0,12.0,4.0,13.0,10.0,18.0,0.0,15.0,15.0,13.0,16.0,5.0,15.0,4.0,9.0,3.0,15.0,12.0,16.0,15.0,15.0,6.0,13.0,0.0,16.0,16.0,16.0,16.0,10.0,12.0,15.0,13.0,13.0,12.0,6.0,13.0,14.0,4.0,9.0,15.0,0.0,15.0,0.0,13.0,3.0,0.0,0.0,16.0,15.0,12.0,15.0,13.0,15.0,16.0,14.0,13.0,13.0,13.0,6.0,9.0,12.0,9.0,15.0,17.0,16.0,14.0,12.0,15.0,6.0,6.0,15.0,16.0,9.0,0.0,15.0,13.0,5.0,11.0,9.0,16.0,11.0,10.0,13.0,9.0,16.0,12.0,13.0,14.0,13.0,6.0,10.0,7.0,16.0,12.0,12.0,12.0,7.0,17.0,11.0,5.0,0.0,6.0,6.0,14.0,13.0,15.0,12.0,13.0,16.0,15.0,11.0,13.0,12.0,16.0,0.0,13.0,12.0,13.0,18.0,12.0,17.0,10.0,11.0,0.0,16.0,15.0,11.0,15.0,15.0,3.0,16.0,11.0,14.0,13.0,12.0,4.0,7.0,15.0,15.0,10.0,14.0,13.0,15.0,15.0,14.0,17.0,10.0,18.0,9.0,9.0,6.0,14.0,15.0,15.0,16.0,12.0,7.0,12.0,14.0,15.0,13.0,16.0,13.0,15.0,3.0,9.0,10.0,16.0,3.0,15.0,11.0,0.0,14.0,9.0,12.0,17.0,12.0,11.0,13.0,14.0,13.0,16.0,14.0,12.0,13.0,15.0,7.0,9.0,13.0,15.0,17.0,15.0,12.0,0.0,13.0,12.0,10.0,13.0,6.0,11.0,14.0,15.0,9.0,16.0,6.0,17.0,14.0,16.0,12.0,11.0,3.0,0.0,13.0,13.0,14.0,10.0,15.0,12.0,12.0,15.0,17.0,15.0,4.0,15.0,0.0,12.0,13.0,0.0,13.0,15.0,15.0,11.0,15.0,11.0,4.0,6.0,13.0,16.0,12.0,3.0,16.0,13.0,0.0,16.0,6.0,13.0,12.0,15.0,12.0,7.0,15.0,13.0,15.0,12.0,13.0,12.0,9.0,13.0,13.0,14.0,14.0,16.0,3.0,16.0,17.0,12.0,17.0,13.0,13.0,16.0,3.0,15.0,11.0,0.0,14.0,13.0,17.0,0.0,8.0,3.0,12.0,12.0,9.0,3.0,10.0,0.0,9.0,13.0,13.0,15.0,15.0,10.0,13.0,14.0,9.0,9.0,14.0,12.0,13.0,15.0,17.0,13.0,11.0,9.0,12.0,15.0,12.0,16.0,9.0,13.0,16.0,18.0,14.0,15.0,12.0,12.0,9.0,16.0,7.0,13.0,13.0,12.0,11.0,15.0,15.0,15.0,17.0,15.0,13.0,15.0,0.0,6.0,15.0,11.0,12.0,15.0,0.0,12.0,16.0,16.0,6.0,18.0,12.0,13.0,13.0,12.0,13.0,17.0,16.0,0.0,16.0,15.0,13.0,13.0,13.0,13.0,15.0,12.0,12.0,16.0,17.0,0.0,15.0,16.0,13.0,12.0,9.0,13.0,15.0,12.0,16.0,15.0,14.0,11.0,17.0,16.0,16.0,12.0,10.0,9.0,15.0,16.0,13.0,13.0,15.0,12.0,17.0,13.0,6.0,10.0,3.0,14.0,14.0,3.0,15.0,12.0,15.0,13.0,12.0,6.0,15.0,7.0,10.0,6.0,6.0,16.0,9.0,16.0,0.0,12.0,10.0,12.0,12.0,15.0,13.0,14.0,10.0,12.0,15.0,12.0,16.0,11.0,10.0,15.0,12.0,9.0,15.0,17.0,7.0,6.0,16.0,10.0,14.0,0.0,13.0,17.0,14.0,16.0,12.0,7.0,9.0,15.0,13.0,13.0,6.0,16.0,12.0,15.0,6.0,14.0,12.0,16.0,12.0,16.0,9.0,9.0,12.0,16.0,0.0,4.0,13.0,9.0,0.0,13.0,17.0,9.0,14.0,14.0,12.0,14.0,10.0,17.0,15.0,12.0,6.0,12.0,12.0,17.0,16.0,9.0,15.0,14.0,8.0,16.0,7.0,15.0,10.0,15.0,15.0,16.0,13.0,12.0,0.0,0.0,13.0,16.0,13.0,14.0,14.0,14.0,9.0,3.0,6.0,8.0,13.0,15.0,15.0,15.0,15.0,16.0,13.0,11.0,16.0,16.0,13.0,13.0,16.0,12.0,16.0,14.0,15.0,0.0,16.0,0.0,12.0,0.0,8.0,16.0,17.0,13.0,10.0,15.0,15.0,3.0,8.0,12.0,15.0,10.0,15.0,12.0,17.0,16.0,3.0,8.0,11.0,15.0,14.0,13.0,16.0,13.0,12.0,10.0,8.0,12.0,12.0,16.0,12.0,12.0,16.0,12.0,14.0,12.0,12.0,10.0,13.0,9.0,13.0,16.0,8.0,12.0,15.0,16.0,12.0,14.0,0.0,14.0,12.0,13.0,10.0,12.0,13.0,13.0,8.0,16.0,15.0,14.0,0.0,6.0,3.0,3.0,16.0,13.0,12.0,13.0,3.0,6.0,15.0,10.0,12.0,4.0,13.0,14.0,9.0,1.0,13.0,9.0,9.0,16.0,9.0,12.0,13.0,6.0,0.0,16.0,13.0,15.0,17.0,10.0,13.0,7.0,12.0,13.0,15.0,10.0,7.0,9.0,14.0,14.0,6.0,9.0,14.0,12.0,15.0,13.0,9.0,12.0,10.0,3.0,3.0,14.0,15.0,15.0,16.0,17.0,10.0,9.0,16.0,0.0,7.0,15.0,15.0,0.0,11.0,16.0,15.0,11.0,9.0,0.0,13.0,15.0,15.0,16.0,12.0,16.0,13.0,3.0,7.0,10.0,17.0,12.0,15.0,7.0,3.0,12.0,10.0,18.0,13.0,6.0,15.0,2.0,14.0,15.0,14.0,13.0,5.0,3.0,16.0,13.0,15.0,15.0,12.0,0.0,15.0,15.0,15.0,12.0,14.0,13.0,13.0,13.0,15.0,16.0,13.0,16.0,15.0,0.0,15.0,0.0,14.0,0.0,12.0,15.0,12.0,12.0,12.0,15.0,13.0,13.0,0.0,15.0,15.0,0.0,15.0,16.0,3.0,15.0,9.0,15.0,9.0,7.0,11.0,6.0,16.0,7.0,15.0,12.0,13.0,18.0,13.0,17.0,13.0,15.0,12.0,15.0,7.0,12.0,0.0,12.0,12.0,15.0,14.0,6.0,16.0,14.0,12.0,16.0,13.0,13.0,16.0,9.0,17.0,16.0,16.0,15.0,13.0,14.0,15.0,14.0,18.0,15.0,13.0,16.0,15.0,12.0,15.0,12.0,18.0,12.0,3.0,16.0,0.0,13.0,15.0,15.0,12.0,12.0,13.0,16.0,12.0,15.0,0.0,12.0,9.0,11.0,3.0,13.0,15.0,13.0,15.0,9.0,15.0,13.0,13.0,14.0,14.0,15.0,13.0,6.0,15.0,14.0,16.0,12.0,11.0,13.0,11.0,13.0,15.0,4.0,12.0,12.0,6.0,16.0,15.0,12.0,0.0,12.0,9.0,16.0,18.0,15.0,6.0,12.0,15.0,6.0,6.0,13.0,12.0,16.0,15.0,17.0,16.0,14.0,12.0,9.0,13.0,11.0,16.0,15.0,12.0,14.0,15.0,15.0,12.0,11.0,9.0,16.0,15.0,12.0,13.0,6.0,12.0,0.0,16.0,9.0,15.0,15.0,13.0,16.0,6.0,12.0,12.0,12.0,12.0,15.0,3.0,17.0,17.0,13.0,13.0,13.0,3.0,6.0,9.0,3.0,14.0,18.0,12.0,13.0,3.0,12.0,15.0,15.0,17.0,0.0,12.0,13.0,13.0,12.0,13.0,0.0,16.0,15.0,0.0,12.0,16.0,8.0,16.0,14.0,12.0,15.0,0.0,15.0,13.0,10.0,14.0,15.0,15.0,15.0,0.0,15.0,3.0,13.0,13.0,9.0,12.0,7.0,13.0,6.0,9.0,16.0,15.0,15.0,13.0,12.0,15.0,12.0,10.0,13.0,13.0,1.0,14.0,13.0,12.0,16.0,16.0,0.0,12.0,14.0,15.0,17.0,0.0,3.0,13.0,16.0,12.0,15.0,14.0,12.0,13.0,15.0,13.0,0.0,12.0,0.0,14.0,12.0,12.0,12.0,16.0,10.0,12.0,11.0,17.0,16.0,4.0,9.0,15.0,10.0,12.0,14.0,15.0,12.0,13.0,9.0,3.0,6.0,12.0,17.0,16.0,13.0,3.0,12.0,10.0,15.0,15.0,12.0,17.0,12.0,13.0,16.0,14.0,15.0,13.0,3.0,12.0,14.0,13.0,9.0,11.0,15.0,12.0,16.0,3.0,15.0,12.0,13.0,12.0,15.0,15.0,15.0,7.0,15.0,17.0,6.0,6.0,13.0,6.0,6.0,1.0,5.0,10.0,13.0,0.0,14.0,16.0,15.0,15.0,0.0,15.0,16.0,15.0,13.0,8.0,15.0,12.0,13.0,11.0,6.0,16.0,6.0,10.0,15.0,15.0,13.0,16.0,18.0,16.0,12.0,8.0,12.0,12.0,15.0,10.0,13.0,15.0,13.0,13.0,9.0,15.0,12.0,14.0,14.0,9.0,6.0,12.0,13.0,8.0,15.0,16.0,12.0,13.0,14.0,13.0,6.0,14.0,12.0,11.0,12.0,15.0,13.0,1.0,6.0,15.0,15.0,12.0,16.0,13.0,15.0,14.0,13.0,0.0,6.0,15.0,16.0,4.0,16.0,12.0,15.0,0.0,15.0,17.0,15.0,13.0,13.0,13.0,15.0,13.0,9.0,12.0,12.0,15.0,13.0,13.0,6.0,7.0,4.0,12.0,0.0,16.0,13.0,9.0,0.0,12.0,12.0,8.0,12.0,3.0,13.0,15.0,8.0,9.0,13.0,13.0,14.0,16.0,16.0,14.0,9.0,16.0,17.0,11.0,0.0,16.0,13.0,9.0,13.0,16.0,15.0,15.0,13.0,3.0,13.0,6.0,3.0,13.0,16.0,3.0,10.0,13.0,13.0,0.0,15.0,16.0,13.0,12.0,15.0,15.0,15.0,18.0,17.0,13.0,0.0,3.0,15.0,15.0,14.0,15.0,0.0,15.0,15.0,4.0,10.0,13.0,12.0,14.0,3.0,15.0,12.0,7.0,14.0,15.0,1.0,9.0,0.0,3.0,15.0,16.0,12.0,0.0,12.0,0.0,15.0,17.0,18.0,12.0,13.0,13.0,13.0,13.0,13.0,14.0,13.0,12.0,12.0,6.0,12.0,18.0,13.0,9.0,0.0,7.0,14.0,15.0,9.0,9.0,7.0,13.0,16.0,12.0,16.0,12.0,11.0,13.0,9.0,3.0,11.0,15.0,12.0,12.0,16.0,13.0,3.0,0.0,10.0,13.0,0.0,15.0,14.0,11.0,9.0,16.0,16.0,17.0,11.0,12.0,9.0,0.0,15.0,13.0,10.0,9.0,14.0,12.0,13.0,7.0,15.0,13.0,0.0,6.0,16.0,0.0,0.0,9.0,4.0,14.0,15.0,13.0,13.0,16.0,0.0,18.0,15.0,15.0,15.0,16.0,16.0,8.0,9.0,12.0,13.0,7.0,6.0,14.0,10.0,12.0,16.0,9.0,13.0,16.0,9.0,13.0,7.0,14.0,13.0,15.0,10.0,12.0,12.0,6.0,16.0,12.0,15.0,0.0,13.0,6.0,7.0,16.0,14.0,15.0,11.0,15.0,0.0,13.0,0.0,3.0,13.0,16.0,15.0,3.0,3.0,16.0,14.0,7.0,12.0,16.0,12.0,12.0,15.0,0.0,3.0,17.0,14.0,16.0,9.0,10.0,10.0,12.0,14.0,12.0,12.0,14.0,15.0,12.0,1.0,13.0,3.0,13.0,18.0,16.0,8.0,6.0,9.0,13.0,16.0,10.0,13.0,15.0,9.0,14.0,18.0,13.0,10.0,18.0,12.0,16.0,0.0,12.0,15.0,7.0,15.0,16.0,12.0,9.0,13.0,3.0,13.0,15.0,3.0,10.0,14.0,15.0,9.0,9.0,16.0,0.0,13.0,13.0,15.0,14.0,15.0,16.0,14.0,15.0,12.0,18.0,15.0,12.0,12.0,16.0,12.0,13.0,17.0,15.0,13.0,15.0,13.0,9.0,15.0,10.0,15.0,16.0,14.0,12.0,16.0,3.0,14.0,15.0,16.0,14.0,12.0,13.0,12.0,15.0,11.0,13.0,15.0,15.0,18.0,14.0,15.0,14.0,18.0,13.0,11.0,12.0,12.0,9.0,13.0,13.0,13.0,13.0,15.0,16.0,16.0,9.0,12.0,11.0,12.0,0.0,16.0,10.0,16.0,15.0,16.0,15.0,16.0,15.0,13.0,12.0,12.0,12.0,17.0,15.0,12.0,16.0,9.0,15.0,6.0,17.0,15.0,15.0,9.0,15.0,10.0,15.0,16.0,13.0,9.0,13.0,7.0,15.0,5.0,16.0,13.0,13.0,16.0,12.0,12.0,13.0,9.0,8.0,7.0,14.0,13.0,12.0,13.0,3.0,10.0,9.0,17.0,7.0,12.0,10.0,12.0,12.0,12.0,0.0,15.0,8.0,18.0,16.0,14.0,0.0,15.0,14.0,17.0,15.0,14.0,14.0,14.0,16.0,17.0,13.0,0.0,15.0,12.0,15.0,0.0,10.0,14.0,13.0,12.0,13.0,15.0,9.0,15.0,0.0,12.0,16.0,12.0,13.0,15.0,9.0,13.0,15.0,14.0,10.0,13.0,18.0,12.0,14.0,19.0,3.0,15.0,13.0,18.0,16.0,7.0,16.0,15.0,12.0,14.0,10.0,16.0,12.0,13.0,17.0,16.0,13.0,7.0,12.0,12.0,13.0,10.0,13.0,9.0,8.0,3.0,4.0,12.0,13.0,15.0,10.0,15.0,15.0,3.0,7.0,12.0,15.0,16.0,6.0,16.0,15.0,15.0,15.0,15.0,14.0,15.0,15.0,9.0,18.0,13.0,15.0,12.0,1.0,12.0,16.0,13.0,12.0,0.0,12.0,12.0,13.0,15.0,13.0,15.0,0.0,9.0,15.0,12.0,0.0,6.0,4.0,15.0,13.0,17.0,16.0,12.0,15.0,12.0,13.0,12.0,12.0,10.0,13.0,6.0,0.0,12.0,16.0,18.0,13.0,16.0,13.0,13.0,14.0,12.0,14.0,13.0,15.0,14.0,15.0,13.0,12.0,13.0,16.0,15.0,14.0,16.0,15.0,3.0,9.0,15.0,14.0,14.0,16.0,15.0,15.0,13.0,15.0,12.0,12.0,14.0,11.0,12.0,0.0,14.0,9.0,14.0,5.0,12.0,15.0,14.0,17.0,11.0,6.0,13.0,16.0,3.0,19.0,9.0,17.0,14.0,9.0,15.0,13.0,16.0,3.0,10.0,12.0,15.0,7.0,15.0,13.0,13.0,15.0,16.0,6.0,3.0,16.0,15.0,15.0,16.0,16.0,6.0,14.0,13.0,18.0,12.0,13.0,15.0,15.0,16.0,9.0,13.0,15.0,12.0,4.0,13.0,16.0,0.0,14.0,0.0,6.0,17.0,13.0,18.0,10.0,4.0,13.0,0.0,15.0,14.0,16.0,13.0,6.0,17.0,0.0,14.0,14.0,13.0,13.0,13.0,16.0,0.0,15.0,12.0,12.0,10.0,15.0,13.0,13.0,16.0,6.0,17.0,8.0,15.0,0.0,12.0,16.0,12.0,0.0,15.0,18.0,9.0,12.0,12.0,3.0,12.0,15.0,17.0,3.0,15.0,16.0,6.0,17.0,15.0,13.0,12.0,0.0,4.0,6.0,17.0,9.0,14.0,7.0,16.0,7.0,18.0,15.0,12.0,15.0,12.0,16.0,14.0,13.0,16.0,6.0,13.0,14.0,14.0,10.0,16.0,15.0,15.0,13.0,13.0,12.0,17.0,16.0,14.0,11.0,16.0,14.0,14.0,12.0,13.0,15.0,16.0,16.0,15.0,13.0,15.0,15.0,0.0,14.0,15.0,15.0,9.0,8.0,15.0,15.0,15.0,16.0,3.0,12.0,13.0,9.0,13.0,16.0,13.0,11.0,4.0,16.0,16.0,15.0,16.0,17.0,13.0,16.0,0.0,15.0,16.0,15.0,13.0,17.0,0.0,16.0,12.0,14.0,3.0,12.0,15.0,14.0,12.0,14.0,13.0,12.0,11.0,14.0,13.0,3.0,11.0,15.0,14.0,15.0,6.0,12.0,18.0,15.0,16.0,13.0,15.0,12.0,15.0,12.0,0.0,16.0,15.0,13.0,12.0,15.0,16.0,12.0,16.0,3.0,10.0,12.0,15.0,9.0,6.0,15.0,12.0,13.0,16.0,16.0,15.0,6.0,8.0,6.0,12.0,12.0,14.0,16.0,12.0,13.0,14.0,1.0,12.0,9.0,0.0,12.0,0.0,0.0,8.0,16.0,15.0,12.0,15.0,16.0,12.0,15.0,13.0,12.0,11.0,12.0,13.0,16.0,6.0,0.0,12.0,10.0,14.0,14.0,17.0,16.0,17.0,13.0,12.0,15.0,16.0,0.0,16.0,15.0,5.0,15.0,13.0,13.0,14.0,16.0,8.0,14.0,15.0,10.0,13.0,12.0,0.0,12.0,14.0,12.0,15.0,16.0,14.0,12.0,18.0,16.0,12.0,13.0,9.0,16.0,15.0,12.0,3.0,15.0,12.0,12.0,14.0,17.0,15.0,7.0,7.0,13.0,18.0,13.0,12.0,14.0,15.0,3.0,9.0,15.0,15.0,0.0,6.0,16.0,12.0,8.0,16.0,16.0,9.0,14.0,12.0,16.0,9.0,4.0,12.0,6.0,14.0,3.0,18.0,9.0,17.0,15.0,12.0,2.0,0.0,3.0,0.0,13.0,15.0,12.0,18.0,14.0,13.0,16.0,9.0,15.0,9.0,17.0,10.0,0.0,12.0,14.0,16.0,13.0,16.0,0.0,15.0,15.0,13.0,13.0,15.0,13.0,14.0,0.0,16.0,18.0,16.0,15.0,14.0,9.0,13.0,16.0,9.0,13.0,13.0,12.0,1.0,15.0,12.0,16.0,15.0,14.0,15.0,16.0,16.0,10.0,12.0,17.0,3.0,12.0,13.0,16.0,15.0,14.0,15.0,12.0,18.0,13.0,9.0,13.0,15.0,14.0,15.0,12.0,15.0,14.0,0.0,14.0,15.0,13.0,14.0,0.0,15.0,16.0,0.0,14.0,15.0,12.0,3.0,12.0,14.0,15.0,16.0,6.0,12.0,9.0,15.0,12.0,12.0,17.0,15.0,16.0,10.0,3.0,14.0,16.0,6.0,16.0,4.0,14.0,15.0,12.0,12.0,16.0,15.0,17.0,15.0,16.0,9.0,13.0,12.0,18.0,14.0,13.0,12.0,15.0,18.0,3.0,13.0,14.0,5.0,12.0,15.0,15.0,14.0,10.0,5.0,1.0,10.0,12.0,15.0,16.0,16.0,6.0,12.0,12.0,12.0,16.0,12.0,13.0,13.0,15.0,17.0,14.0,16.0,12.0,14.0,12.0,17.0,12.0,16.0,12.0,15.0,11.0,14.0,13.0,16.0,10.0,13.0,16.0,13.0,16.0,16.0,9.0,16.0,13.0,12.0,3.0,16.0,9.0,15.0,12.0,16.0,10.0,18.0,12.0,15.0,15.0,16.0,12.0,13.0,3.0,15.0,13.0,15.0,16.0,16.0,1.0,9.0,15.0,10.0,12.0,3.0,16.0,15.0,4.0,0.0,12.0,16.0,3.0,16.0,15.0,13.0,12.0,6.0,14.0,16.0,13.0,15.0,15.0,17.0,13.0,12.0,13.0,9.0,18.0,0.0,16.0,11.0,12.0,15.0,15.0,16.0,15.0,0.0,12.0,10.0,3.0,16.0,14.0,9.0,14.0,6.0,0.0,13.0,16.0,13.0,13.0,1.0,16.0,14.0,8.0,16.0,7.0,15.0,14.0,14.0,6.0,12.0,3.0,16.0,11.0,15.0,12.0,9.0,15.0,13.0,13.0,14.0,15.0,10.0,7.0,17.0,14.0,15.0,12.0,1.0,15.0,10.0,13.0,13.0,17.0,0.0,15.0,12.0,17.0,9.0,15.0,17.0,14.0,13.0,13.0,17.0,13.0,11.0,12.0,15.0,6.0,21.0,13.0,18.0,18.0,13.0,16.0,12.0,14.0,13.0,13.0,13.0,14.0,17.0,2.0,13.0,15.0,15.0,12.0,13.0,9.0,12.0,12.0,9.0,16.0,13.0,11.0,12.0,15.0,16.0,17.0,14.0,9.0,17.0,15.0,12.0,6.0,18.0,16.0,16.0,12.0,13.0,14.0,18.0,13.0,12.0,15.0,15.0,15.0,0.0,3.0,12.0,12.0,16.0,13.0,16.0,12.0,4.0,15.0,16.0,6.0,14.0,14.0,17.0,13.0,13.0,12.0,16.0,13.0,14.0,0.0,15.0,15.0,12.0,15.0,14.0,14.0,16.0,12.0,16.0,7.0,12.0,10.0,4.0,14.0,16.0,16.0,0.0,14.0,15.0,0.0,16.0,16.0,12.0,10.0,12.0,12.0,13.0,15.0,0.0,13.0,15.0,15.0,12.0,10.0,12.0,10.0,3.0,3.0,16.0,12.0,12.0,6.0,12.0,15.0,9.0,12.0,14.0,15.0,13.0,12.0,14.0,15.0,16.0,14.0,15.0,15.0,12.0,14.0,6.0,16.0,10.0,16.0,16.0,15.0,13.0,14.0,15.0,12.0,14.0,13.0,12.0,15.0,12.0,12.0,16.0,0.0,10.0,15.0,16.0,12.0,15.0,6.0,12.0,15.0,9.0,14.0,16.0,12.0,14.0,12.0,15.0,9.0,13.0,13.0,6.0,16.0,12.0,10.0,3.0,12.0,9.0,13.0,9.0,6.0,16.0,17.0,12.0,7.0,16.0,18.0,7.0,13.0,12.0,16.0,15.0,6.0,16.0,13.0,12.0,12.0,7.0,16.0,0.0,12.0,15.0,14.0,14.0,14.0,13.0,7.0,13.0,6.0,3.0,3.0,14.0,10.0,10.0,13.0,6.0,3.0,12.0,11.0,15.0,1.0,14.0,12.0,12.0,12.0,15.0,16.0,13.0,16.0,13.0,12.0,3.0,12.0,6.0,14.0,9.0,15.0,1.0,15.0,13.0,10.0,3.0,15.0,9.0,9.0,9.0,0.0,12.0,14.0,0.0,6.0,13.0,13.0,6.0,3.0,13.0,10.0,15.0,8.0,13.0,7.0,15.0,15.0,16.0,16.0,13.0,15.0,16.0,16.0,12.0,15.0,13.0,13.0,12.0,6.0,12.0,12.0,16.0,15.0,7.0,4.0,0.0,13.0,10.0,12.0,6.0,3.0,14.0,2.0,16.0,13.0,13.0,12.0,12.0,16.0,3.0,14.0,7.0,15.0,12.0,15.0,6.0,5.0,12.0,13.0,12.0,12.0,15.0,14.0,12.0,7.0,11.0,13.0,15.0,6.0,12.0,3.0,17.0,0.0,12.0,12.0,15.0,16.0,14.0,12.0,13.0,10.0,12.0,7.0,17.0,11.0,14.0,0.0,9.0,15.0,13.0,12.0,16.0,13.0,15.0,15.0,15.0,12.0,12.0,0.0,12.0,15.0,12.0,17.0,16.0,16.0,16.0,13.0,8.0,3.0,3.0,19.0,15.0,5.0,13.0,14.0,12.0,18.0,12.0,12.0,4.0,13.0,14.0,0.0,3.0,9.0,13.0,3.0,15.0,3.0,14.0,15.0,16.0,16.0,12.0,12.0,10.0,13.0,15.0,6.0,8.0,3.0,12.0,16.0,17.0,15.0,13.0,0.0,6.0,15.0,7.0,12.0,16.0,17.0,12.0,14.0,16.0,15.0,9.0,14.0,13.0,10.0,13.0,13.0,17.0,16.0,15.0,11.0,6.0,12.0,11.0,16.0,13.0,13.0,8.0,14.0,15.0,0.0,13.0,12.0,13.0,16.0,16.0,16.0,18.0,16.0,16.0,15.0,18.0,15.0,16.0,16.0,14.0,14.0,11.0,16.0,9.0,15.0,16.0,5.0,0.0,15.0,9.0,0.0,9.0,13.0,9.0,12.0,16.0,15.0,12.0,16.0,13.0,16.0,0.0,15.0,15.0,7.0,15.0,13.0,12.0,9.0,14.0,16.0,6.0,15.0,17.0,14.0,12.0,14.0,15.0,6.0,14.0,13.0,10.0,13.0,16.0,9.0,7.0,5.0,10.0,12.0,15.0,12.0,16.0,13.0,12.0,13.0,16.0,15.0,15.0,12.0,12.0,13.0,17.0,13.0,15.0,6.0,9.0,13.0,13.0,11.0,17.0,15.0,11.0,9.0,13.0,13.0,14.0,16.0,6.0,17.0,15.0,9.0,12.0,12.0,12.0,18.0,16.0,0.0,17.0,14.0,6.0,6.0,15.0,10.0,15.0,16.0,13.0,12.0,12.0,12.0,15.0,9.0,16.0,13.0,9.0,10.0,16.0,3.0,14.0,12.0,12.0,6.0,14.0,13.0,16.0,15.0,14.0,6.0,16.0,13.0,10.0,16.0,0.0,13.0,0.0,0.0,15.0,11.0,15.0,0.0,15.0,10.0,13.0,16.0,16.0,9.0,13.0,0.0,13.0,0.0,15.0,12.0,12.0,15.0,12.0,14.0,14.0,16.0,16.0,13.0,10.0,12.0,9.0,15.0,15.0,9.0,14.0,12.0,7.0,6.0,15.0,16.0,10.0,15.0,16.0,13.0,13.0,12.0,16.0,15.0,12.0,0.0,18.0,12.0,14.0,18.0,4.0,12.0,14.0,16.0,12.0,16.0,16.0,14.0,15.0,13.0,16.0,12.0,14.0,10.0,0.0,13.0,15.0,13.0,15.0,0.0,13.0,14.0,14.0,16.0,16.0,16.0,13.0,5.0,6.0,12.0,13.0,16.0,7.0,16.0,13.0,14.0,9.0,16.0,16.0,14.0,16.0,15.0,15.0,13.0,16.0,16.0,0.0,12.0,15.0,9.0,4.0,17.0,6.0,16.0,9.0,15.0,13.0,17.0,15.0,9.0,11.0,8.0,12.0,15.0,11.0,15.0,9.0,12.0,13.0,12.0,16.0,16.0,15.0,6.0,12.0,7.0,13.0,15.0,12.0,12.0,14.0,15.0,15.0,13.0,15.0,15.0,7.0,16.0,15.0,8.0,15.0,15.0,13.0,12.0,13.0,17.0,5.0,12.0,16.0,15.0,12.0,8.0,13.0,15.0,13.0,14.0,15.0,14.0,17.0,13.0,13.0,0.0,13.0,2.0,12.0,12.0,13.0,2.0,13.0,4.0,16.0,16.0,16.0,7.0,0.0,10.0,12.0,12.0,15.0,16.0,3.0,1.0,13.0,13.0,16.0,18.0,3.0,6.0,15.0,12.0,9.0,9.0,15.0,15.0,13.0,6.0,17.0,12.0,12.0,15.0,16.0,16.0,18.0,16.0,13.0,0.0,15.0,17.0,3.0,0.0,13.0,10.0,3.0,8.0,0.0,11.0,16.0,13.0,15.0,16.0,15.0,13.0,12.0,10.0,13.0,10.0,13.0,10.0,13.0,18.0,1.0,18.0,14.0,10.0,12.0,13.0,15.0,14.0,17.0,12.0,15.0,13.0,13.0,13.0,9.0,0.0,12.0,15.0,13.0,15.0,13.0,12.0,17.0,9.0,15.0,0.0,16.0,15.0,12.0,9.0,10.0,15.0,14.0,16.0,13.0,12.0,12.0,12.0,15.0,3.0,15.0,12.0,12.0,15.0,16.0,13.0,15.0,8.0,14.0,3.0,14.0,11.0,12.0,13.0,16.0,15.0,12.0,3.0,16.0,16.0,13.0,16.0,15.0,6.0,15.0,12.0,12.0,18.0,8.0,0.0,15.0,13.0,15.0,0.0,1.0,0.0,10.0,0.0,12.0,18.0,3.0,0.0,12.0,14.0,15.0,16.0,16.0,18.0,10.0,12.0,15.0,15.0,4.0,13.0,13.0,14.0,13.0,15.0,16.0,13.0,15.0,12.0,14.0,15.0,15.0,16.0,10.0,3.0,10.0,12.0,19.0,0.0,8.0,14.0,0.0,10.0,14.0,14.0,13.0,14.0,10.0,10.0,15.0,12.0,0.0,14.0,13.0,16.0,11.0,12.0,14.0,12.0,15.0,17.0,7.0,16.0,9.0,12.0,12.0,15.0,14.0,15.0,13.0,16.0,0.0,12.0,15.0,16.0,12.0,12.0,12.0,3.0,6.0,15.0,13.0,15.0,16.0,14.0,9.0,0.0,7.0,6.0,12.0,12.0,13.0,6.0,14.0,0.0,9.0,17.0,13.0,16.0,12.0,14.0,12.0,3.0,12.0,13.0,14.0,6.0,13.0,9.0,15.0,14.0,6.0,12.0,16.0,14.0,13.0,12.0,12.0,12.0,0.0,0.0,12.0,15.0,0.0,9.0,0.0,7.0,15.0,15.0,12.0,17.0,13.0,9.0,12.0,13.0,2.0,0.0,12.0,11.0,13.0,16.0,13.0,15.0,12.0,12.0,15.0,6.0,15.0,10.0,7.0,13.0,16.0,14.0,0.0,16.0,8.0,12.0,16.0,12.0,18.0,16.0,6.0,13.0,3.0,15.0,0.0,6.0,13.0,13.0,13.0,13.0,7.0,3.0,16.0,16.0,12.0,16.0,9.0,12.0,15.0,15.0,14.0,9.0,14.0,10.0,0.0,16.0,16.0,12.0,12.0,16.0,13.0,15.0,10.0,13.0,15.0,10.0,10.0,12.0,15.0,16.0,10.0,16.0,12.0,15.0,16.0,0.0,16.0,15.0,17.0,11.0,13.0,15.0,13.0,0.0,12.0,13.0,16.0,16.0,9.0,12.0,18.0,6.0,15.0,15.0,15.0,15.0,10.0,16.0,16.0,13.0,0.0,9.0,15.0,12.0,16.0,15.0,1.0,11.0,13.0,13.0,9.0,16.0,13.0,3.0,18.0,0.0,15.0,12.0,14.0,13.0,16.0,13.0,12.0,15.0,12.0,14.0,15.0,13.0,13.0,13.0,3.0,0.0,3.0,0.0,8.0,12.0,12.0,11.0,11.0,14.0,12.0,16.0,8.0,12.0,12.0,5.0,16.0,17.0,9.0,4.0,8.0,13.0,16.0,13.0,15.0,6.0,3.0,16.0,17.0,14.0,8.0,13.0,14.0,15.0,13.0,15.0,6.0,12.0,13.0,0.0,15.0,14.0,13.0,15.0,15.0,14.0,13.0,12.0,13.0,11.0,9.0,15.0,18.0,6.0,12.0,12.0,12.0,16.0,18.0,15.0,14.0,15.0,17.0,9.0,16.0,12.0,16.0,0.0,0.0,17.0,10.0,13.0,16.0,11.0,3.0,13.0,13.0,12.0,0.0,16.0,14.0,12.0,16.0,15.0,12.0,15.0,16.0,14.0,16.0,15.0,6.0,11.0,17.0,9.0,13.0,16.0,13.0,16.0,15.0,15.0,9.0,12.0,12.0,16.0,12.0,13.0,15.0,0.0,14.0,7.0,16.0,16.0,13.0,15.0,3.0,13.0,12.0,15.0,10.0,9.0,12.0,14.0,12.0,16.0,12.0,17.0,13.0,12.0,9.0,15.0,6.0,7.0,13.0,10.0,15.0,17.0,12.0,11.0,15.0,15.0,12.0,3.0,16.0,13.0,5.0,12.0,12.0,16.0,14.0,12.0,9.0,12.0,13.0,6.0,16.0,16.0,12.0,0.0,13.0,3.0,3.0,13.0,5.0,6.0,9.0,10.0,15.0,8.0,0.0,18.0,13.0,6.0,7.0,12.0,14.0,16.0,13.0,0.0,15.0,14.0,16.0,10.0,13.0,3.0,12.0,15.0,12.0,11.0,16.0,16.0,12.0,10.0,13.0,19.0,16.0,12.0,16.0,3.0,13.0,10.0,16.0,0.0,9.0,15.0,6.0,10.0,12.0,15.0,12.0,4.0,12.0,10.0,13.0,14.0,16.0,12.0,13.0,14.0,14.0,12.0,16.0,16.0,14.0,12.0,13.0,13.0,15.0,14.0,4.0,17.0,14.0,14.0,13.0,15.0,15.0,12.0,4.0,17.0,12.0,15.0,7.0,14.0,1.0,12.0,15.0,16.0,6.0,4.0,6.0,13.0,14.0,14.0,18.0,10.0,12.0,16.0,12.0,15.0,16.0,0.0,14.0,0.0,10.0,13.0,15.0,4.0,0.0,16.0,13.0,12.0,7.0,0.0,14.0,13.0,14.0,9.0,4.0,6.0,3.0,13.0,14.0,14.0,16.0,13.0,15.0,6.0,15.0,0.0,7.0,0.0,15.0,14.0,17.0,6.0,13.0,5.0,15.0,14.0,13.0,12.0,15.0,10.0,15.0,12.0,12.0,9.0,15.0,13.0,13.0,15.0,12.0,15.0,7.0,17.0,12.0,16.0,9.0,10.0,14.0,0.0,13.0,12.0,4.0,1.0,12.0,16.0,11.0,15.0,16.0,3.0,12.0,13.0,16.0,15.0,9.0,16.0,0.0,15.0,15.0,13.0,9.0,18.0,16.0,12.0,9.0,10.0,9.0,4.0,9.0,0.0,9.0,10.0,13.0,15.0,15.0,12.0,21.0,7.0,16.0,9.0,6.0,0.0,15.0,17.0,17.0,3.0,15.0,13.0,12.0,12.0,12.0,15.0,16.0,9.0,13.0,13.0,18.0,14.0,12.0,16.0,10.0,13.0,10.0,14.0,12.0,14.0,14.0,10.0,13.0,10.0,0.0,8.0,13.0,11.0,9.0,16.0,10.0,14.0,15.0,15.0,12.0,17.0,3.0,16.0,15.0,12.0,15.0,3.0,14.0,9.0,9.0,16.0,13.0,6.0,16.0,3.0,15.0,12.0,15.0,18.0,16.0,6.0,12.0,16.0,12.0,6.0,15.0,13.0,14.0,12.0,16.0,15.0,15.0,15.0,9.0,15.0,3.0,15.0,14.0,7.0,16.0,0.0,12.0,18.0,15.0,15.0,15.0,10.0,15.0,1.0,6.0,17.0,14.0,13.0,6.0,14.0,0.0,6.0,12.0,11.0,15.0,9.0,15.0,6.0,13.0,2.0,19.0,9.0,12.0,16.0,15.0,13.0,14.0,8.0,7.0,11.0,15.0,14.0,15.0,15.0,13.0,15.0,16.0,15.0,13.0,12.0,15.0,7.0,0.0,4.0,17.0,6.0,10.0,12.0,17.0,12.0,15.0,15.0,10.0,10.0,14.0,13.0,15.0,12.0,11.0,17.0,9.0,8.0,14.0,16.0,12.0,15.0,16.0,6.0,16.0,16.0,12.0,4.0,14.0,14.0,12.0,16.0,6.0,15.0,17.0,13.0,17.0,15.0,9.0,16.0,15.0,16.0,9.0,10.0,8.0,12.0,9.0,12.0,16.0,7.0,13.0,15.0,14.0,13.0,12.0,11.0,13.0,12.0,16.0,17.0,14.0,17.0,13.0,16.0,9.0,15.0,13.0,3.0,16.0,13.0,12.0,12.0,12.0,9.0,16.0,15.0,15.0,13.0,10.0,12.0,12.0,15.0,13.0,13.0,4.0,12.0,12.0,14.0,16.0,0.0,16.0,6.0,13.0,9.0,15.0,13.0,15.0,11.0,7.0,0.0,16.0,9.0,12.0,15.0,11.0,9.0,5.0,12.0,14.0,9.0,10.0,17.0,7.0,0.0,11.0,6.0,15.0,9.0,13.0,16.0,15.0,15.0,16.0,15.0,0.0,3.0,0.0,12.0,13.0,3.0,0.0,9.0,6.0,6.0,12.0,0.0,13.0,12.0,12.0,0.0,15.0,13.0,9.0,16.0,11.0,9.0,12.0,9.0,3.0,11.0,0.0,18.0,16.0,0.0,16.0,13.0,7.0,0.0,0.0,12.0,15.0,15.0,3.0,11.0,16.0,15.0,13.0,17.0,12.0,9.0,15.0,12.0,12.0,15.0,11.0,15.0,15.0,16.0,12.0,17.0,14.0,16.0,10.0,14.0,15.0,13.0,13.0,9.0,15.0,15.0,0.0,13.0,15.0,13.0,14.0,12.0,14.0,15.0,12.0,7.0,16.0,10.0,13.0,10.0,8.0,9.0,6.0,14.0,16.0,15.0,12.0,7.0,15.0,16.0,17.0,9.0,14.0,16.0,15.0,13.0,15.0,15.0,12.0,15.0,10.0,13.0,16.0,14.0,13.0,0.0,12.0,16.0,16.0,14.0,10.0,6.0,13.0,13.0,12.0,9.0,15.0,16.0,12.0,15.0,12.0,0.0,13.0,14.0,16.0,15.0,12.0,16.0,15.0,12.0,16.0,9.0,10.0,9.0,13.0,8.0,16.0,13.0,3.0,12.0,6.0,16.0,12.0,7.0,16.0,15.0,16.0,12.0,15.0,16.0,15.0,12.0,15.0,16.0,12.0,18.0,7.0,12.0,16.0,15.0,16.0,4.0,15.0,4.0,6.0,13.0,13.0,13.0,9.0,3.0,16.0,15.0,12.0,16.0,12.0,15.0,15.0,10.0,13.0,15.0,9.0,16.0,11.0,15.0,15.0,9.0,12.0,13.0,11.0,6.0,13.0,15.0,16.0,3.0,12.0,15.0,14.0,6.0,12.0,9.0,0.0,18.0,9.0,11.0,10.0,14.0,2.0,3.0,13.0,12.0,9.0,16.0,16.0,16.0,12.0,15.0,13.0,12.0,13.0,16.0,13.0,12.0,0.0,13.0,13.0,15.0,16.0,15.0,9.0,12.0,0.0,10.0,9.0,16.0,9.0,18.0,13.0,15.0,8.0,10.0,14.0,13.0,15.0,16.0,10.0,0.0,0.0,16.0,14.0,16.0,13.0,12.0,13.0,15.0,10.0,9.0,11.0,12.0,16.0,15.0,15.0,14.0,3.0,9.0,7.0,17.0,12.0,15.0,14.0,15.0,16.0,17.0,3.0,12.0,15.0,16.0,15.0,12.0,3.0,15.0,14.0,12.0,14.0,9.0,10.0,15.0,12.0,12.0,4.0,17.0,13.0,14.0,13.0,12.0,12.0,4.0,16.0,6.0,9.0,0.0,15.0,16.0,0.0,14.0,10.0,16.0,3.0,0.0,15.0,15.0,12.0,12.0,0.0,16.0,15.0,17.0,6.0,12.0,3.0,13.0,13.0,10.0,15.0,13.0,14.0,4.0,12.0,15.0,7.0,12.0,14.0,12.0,15.0,17.0,14.0,13.0,15.0,15.0,4.0,13.0,16.0,13.0,16.0,0.0,0.0,14.0,17.0,16.0,9.0,13.0,16.0,3.0,0.0,17.0,9.0,15.0,15.0,15.0,14.0,15.0,15.0,12.0,16.0,15.0,0.0,15.0,16.0,13.0,15.0,16.0,9.0,13.0,18.0,15.0,16.0,8.0,12.0,4.0,14.0,12.0,13.0,14.0,15.0,15.0,16.0,15.0,13.0,0.0,9.0,12.0,6.0,12.0,13.0,3.0,13.0,0.0,16.0,0.0,0.0,12.0,15.0,14.0,14.0,12.0,11.0,15.0,11.0,16.0,12.0,8.0,14.0,16.0,13.0,9.0,12.0,15.0,3.0,4.0,14.0,16.0,16.0,15.0,15.0,15.0,10.0,12.0,6.0,15.0,13.0,13.0,13.0,13.0,15.0,8.0,0.0,14.0,12.0,10.0,8.0,9.0,13.0,11.0,13.0,13.0,16.0,17.0,11.0,0.0,10.0,6.0,13.0,10.0,4.0,15.0,16.0,13.0,16.0,15.0,18.0,15.0,13.0,3.0,12.0,19.0,17.0,8.0,13.0,13.0,0.0,14.0,8.0,3.0,6.0,12.0,9.0,0.0,12.0,15.0,9.0,14.0,13.0,12.0,15.0,12.0,16.0,15.0,7.0,4.0,16.0,16.0,15.0,14.0,13.0,12.0,15.0,15.0,13.0,1.0,0.0,15.0,13.0,10.0,16.0,10.0,15.0,13.0,17.0,0.0,16.0,16.0,7.0,14.0,0.0,15.0,13.0,12.0,16.0,9.0,14.0,15.0,13.0,17.0,6.0,16.0,0.0,12.0,15.0,13.0,13.0,6.0,13.0,12.0,11.0,10.0,17.0,13.0,10.0,15.0,12.0,15.0,18.0,16.0,12.0,13.0,12.0,13.0,14.0,0.0,15.0,13.0,15.0,14.0,14.0,13.0,15.0,15.0,13.0,13.0,10.0,12.0,16.0,16.0,17.0,15.0,15.0,13.0,15.0,14.0,13.0,15.0,16.0,9.0,0.0,0.0,9.0,15.0,12.0,12.0,16.0,13.0,13.0,16.0,0.0,15.0,12.0,5.0,6.0,16.0,13.0,13.0,17.0,9.0,14.0,12.0,14.0,16.0,7.0,3.0,10.0,15.0,12.0,0.0,15.0,12.0,13.0,13.0,13.0,14.0,13.0,15.0,13.0,7.0,12.0,9.0,12.0,4.0,15.0,12.0,15.0,12.0,16.0,0.0,12.0,17.0,5.0,13.0,3.0,1.0,9.0,14.0,13.0,16.0,14.0,0.0,13.0,15.0,15.0,13.0,12.0,16.0,15.0,4.0,13.0,13.0,15.0,12.0,6.0,16.0,3.0,13.0,16.0,11.0,0.0,13.0,13.0,15.0,13.0,14.0,15.0,10.0,3.0,16.0,15.0,18.0,14.0,15.0,14.0,0.0,13.0,15.0,14.0,10.0,11.0,15.0,6.0,16.0,0.0,12.0,3.0,12.0,16.0,12.0,13.0,15.0,17.0,15.0,16.0,12.0,13.0,13.0,15.0,15.0,14.0,15.0,13.0,16.0,16.0,17.0,0.0,10.0,17.0,16.0,13.0,10.0,9.0,0.0,13.0,8.0,16.0,14.0,6.0,13.0,10.0,15.0,14.0,15.0,14.0,18.0,14.0,0.0,15.0,12.0,16.0,14.0,18.0,12.0,17.0,0.0,15.0,15.0,4.0,15.0,4.0,3.0,6.0,15.0,13.0,15.0,12.0,13.0,4.0,18.0,15.0,8.0,15.0,0.0,12.0,16.0,12.0,13.0,13.0,1.0,0.0,15.0,13.0,9.0,15.0,17.0,14.0,13.0,13.0,17.0,12.0,3.0,13.0,17.0,15.0,15.0,17.0,15.0,14.0,15.0,13.0,13.0,4.0,15.0,14.0,9.0,6.0,0.0,0.0,8.0,13.0,17.0,4.0,12.0,12.0,9.0,15.0,15.0,13.0,3.0,15.0,3.0,12.0,12.0,9.0,11.0,13.0,16.0,15.0,7.0,15.0,16.0,13.0,0.0,9.0,13.0,13.0,6.0,12.0,16.0,0.0,17.0,15.0,16.0,13.0,15.0,16.0,12.0,15.0,15.0,12.0,15.0,10.0,13.0,17.0,16.0,14.0,15.0,3.0,16.0,15.0,16.0,12.0,9.0,13.0,17.0,7.0,16.0,13.0,15.0,16.0,15.0,16.0,14.0,13.0,9.0,12.0,13.0,0.0,15.0,13.0,15.0,14.0,1.0,0.0,15.0,12.0,12.0,9.0,15.0,10.0,14.0,13.0,16.0,9.0,6.0,12.0,11.0,0.0,14.0,10.0,17.0,0.0,0.0,13.0,15.0,16.0,14.0,15.0,0.0,13.0,13.0,13.0,12.0,13.0,13.0,1.0,12.0,10.0,12.0,11.0,16.0,6.0,13.0,0.0,10.0,9.0,10.0,15.0,0.0,12.0,12.0,15.0,14.0,14.0,4.0,16.0,13.0,12.0,12.0,13.0,12.0,16.0,9.0,12.0,15.0,16.0,9.0,14.0,13.0,16.0,8.0,14.0,14.0,13.0,14.0,16.0,16.0,16.0,12.0,15.0,16.0,12.0,14.0,15.0,12.0,12.0,15.0,15.0,15.0,9.0,13.0,16.0,13.0,12.0,9.0,0.0,12.0,0.0,16.0,15.0,15.0,15.0,0.0,12.0,12.0,15.0,13.0,15.0,13.0,15.0,15.0,16.0,16.0,15.0,16.0,18.0,3.0,15.0,14.0,12.0,9.0,12.0,4.0,4.0,15.0,0.0,15.0,17.0,10.0,16.0,13.0,10.0,14.0,12.0,15.0,14.0,15.0,9.0,10.0,13.0,14.0,14.0,15.0,15.0,14.0,7.0,15.0,13.0,13.0,15.0,13.0,12.0,15.0,14.0,13.0,0.0,10.0,14.0,0.0,16.0,16.0,0.0,12.0,15.0,14.0,15.0,12.0,0.0,15.0,16.0,12.0,13.0,12.0,13.0,16.0,7.0,13.0,15.0,16.0,6.0,12.0,12.0,1.0,14.0,16.0,13.0,12.0,12.0,9.0,15.0,16.0,3.0,13.0,14.0,16.0,14.0,16.0,15.0,16.0,13.0,9.0,13.0,12.0,12.0,14.0,14.0,16.0,14.0,14.0,4.0,13.0,16.0,12.0,12.0,10.0,12.0,16.0,5.0,13.0,12.0,16.0,13.0,12.0,14.0,13.0,13.0,15.0,12.0,16.0,14.0,12.0,7.0,13.0,16.0,16.0,0.0,12.0,16.0,10.0,12.0,14.0,15.0,9.0,12.0,13.0,16.0,0.0,13.0,17.0,12.0,14.0,12.0,10.0,12.0,12.0,16.0,12.0,13.0,12.0,14.0,14.0,14.0,12.0,12.0,6.0,14.0,12.0,15.0,3.0,15.0,14.0,17.0,6.0,12.0,10.0,6.0,3.0,6.0,10.0,16.0,3.0,15.0,9.0,17.0,6.0,16.0,15.0,18.0,15.0,6.0,12.0,17.0,14.0,15.0,13.0,14.0,13.0,15.0,3.0,17.0,10.0,12.0,16.0,14.0,14.0,9.0,14.0,11.0,11.0,0.0,12.0,13.0,0.0,3.0,11.0,17.0,8.0,0.0,15.0,16.0,16.0,15.0,15.0,1.0,14.0,16.0,16.0,9.0,15.0,10.0,12.0,13.0,15.0,18.0,18.0,16.0,15.0,12.0,13.0,9.0,9.0,11.0,15.0,17.0,14.0,9.0,12.0,0.0,13.0,17.0,0.0,13.0,13.0,12.0,0.0,16.0,16.0,3.0,8.0,19.0,16.0,14.0,18.0,15.0,6.0,16.0,7.0,9.0,13.0,15.0,16.0,16.0,13.0,7.0,8.0,15.0,16.0,12.0,12.0,13.0,14.0,15.0,13.0,19.0,15.0,13.0,0.0,12.0,7.0,13.0,16.0,13.0,0.0,14.0,15.0,15.0,6.0,0.0,15.0,3.0,13.0,15.0,0.0,14.0,17.0,15.0,10.0,12.0,16.0,11.0,9.0,10.0,16.0,14.0,15.0,0.0,16.0,14.0,6.0,15.0,0.0,10.0,15.0,13.0,16.0,15.0,7.0,8.0,16.0,6.0,10.0,9.0,1.0,14.0,12.0,18.0,17.0,16.0,3.0,13.0,14.0,14.0,16.0,15.0,0.0,13.0,9.0,15.0,4.0,9.0,10.0,12.0,17.0,6.0,15.0,5.0,4.0,16.0,5.0,0.0,13.0,11.0,15.0,6.0,17.0,12.0,13.0,16.0,12.0,16.0,15.0,9.0,9.0,12.0,6.0,4.0,4.0,15.0,16.0,15.0,12.0,14.0,15.0,15.0,6.0,6.0,13.0,15.0,13.0,13.0,14.0,14.0,13.0,14.0,10.0,4.0,9.0,11.0,8.0,16.0,0.0,14.0,14.0,15.0,12.0,10.0,12.0,12.0,9.0,15.0,14.0,16.0,7.0,13.0,9.0,15.0,12.0,15.0,16.0,10.0,6.0,12.0,3.0,7.0,0.0,15.0,13.0,16.0,13.0,7.0,9.0,0.0,13.0,9.0,12.0,4.0,7.0,16.0,12.0,12.0,0.0,15.0,13.0,14.0,3.0,9.0,13.0,10.0,16.0,13.0,19.0,12.0,3.0,13.0,14.0,12.0,16.0,3.0,2.0,18.0,0.0,12.0,0.0,0.0,15.0,17.0,17.0,14.0,3.0,9.0,16.0,15.0,14.0,5.0,11.0,9.0,12.0,16.0,17.0,10.0,9.0,14.0,13.0,15.0,6.0,15.0,15.0,15.0,15.0,16.0,18.0,13.0,13.0,15.0,16.0,0.0,16.0,14.0,0.0,16.0,10.0,6.0,8.0,14.0,14.0,12.0,7.0,9.0,14.0,17.0,12.0,16.0,15.0,15.0,15.0,16.0,14.0,14.0,15.0,12.0,15.0,9.0,6.0,6.0,17.0,14.0,13.0,13.0,12.0,13.0,15.0,16.0,12.0,16.0,11.0,16.0,16.0,15.0,18.0,12.0,9.0,12.0,15.0,3.0,17.0,10.0,16.0,0.0,12.0,3.0,16.0,17.0,9.0,13.0,16.0,13.0,17.0,12.0,3.0,16.0,9.0,0.0,13.0,16.0,16.0,14.0,0.0,0.0,14.0,12.0,10.0,12.0,6.0,0.0,13.0,13.0,10.0,13.0,13.0,15.0,13.0,16.0,8.0,15.0,13.0,12.0,13.0,13.0,14.0,15.0,16.0,0.0,16.0,3.0,13.0,17.0,13.0,12.0,15.0,15.0,11.0,15.0,12.0,7.0,17.0,15.0,14.0,16.0,15.0,7.0,4.0,7.0,13.0,6.0,14.0,15.0,12.0,14.0,10.0,0.0,13.0,16.0,11.0,11.0,6.0,16.0,13.0,12.0,9.0,14.0,15.0,13.0,13.0,14.0,0.0,15.0,9.0,15.0,9.0,12.0,6.0,8.0,15.0,16.0,15.0,0.0,7.0,15.0,7.0,13.0,15.0,0.0,13.0,15.0,13.0,14.0,15.0,16.0,15.0,13.0,13.0,0.0,15.0,12.0,0.0,16.0,1.0,0.0,7.0,10.0,15.0,16.0,0.0,13.0,15.0,16.0,12.0,1.0,3.0,13.0,9.0,16.0,18.0,10.0,15.0,16.0,16.0,12.0,15.0,16.0,13.0,15.0,16.0,13.0,12.0,0.0,12.0,0.0,9.0,14.0,18.0,3.0,12.0,12.0,9.0,16.0,14.0,15.0,0.0,15.0,18.0,12.0,14.0,15.0,16.0,16.0,13.0,5.0,13.0,15.0,13.0,9.0,12.0,18.0,12.0,10.0,16.0,14.0,14.0,6.0,12.0,9.0,18.0,0.0,15.0,15.0,13.0,14.0,6.0,15.0,15.0,15.0,12.0,11.0,7.0,15.0,12.0,16.0,13.0,14.0,6.0,15.0,16.0,12.0,9.0,13.0,12.0,12.0,6.0,13.0,6.0,3.0,0.0,15.0,12.0,9.0,12.0,10.0,13.0,15.0,15.0,15.0,16.0,16.0,11.0,13.0,16.0,13.0,16.0,15.0,6.0,4.0,13.0,13.0,16.0,3.0,16.0,14.0,15.0,17.0,14.0,14.0,15.0,12.0,14.0,2.0,12.0,16.0,15.0,18.0,13.0,13.0,7.0,3.0,16.0,18.0,16.0,9.0,3.0,15.0,15.0,16.0,7.0,18.0,14.0,15.0,16.0,18.0,6.0,15.0,9.0,15.0,0.0,11.0,16.0,13.0,10.0,8.0,15.0,15.0,16.0,15.0,12.0,0.0,15.0,15.0,0.0,14.0,12.0,12.0,12.0,0.0,15.0,15.0,12.0,14.0,13.0,14.0,7.0,0.0,15.0,15.0,7.0,15.0,13.0,13.0,10.0,8.0,15.0,15.0,13.0,13.0,13.0,16.0,6.0,15.0,12.0,17.0,12.0,0.0,0.0,18.0,12.0,12.0,6.0,14.0,15.0,12.0,14.0,13.0,15.0,12.0,10.0,17.0,14.0,13.0,12.0,13.0,13.0,9.0,13.0,9.0,15.0,8.0,15.0,15.0,16.0,15.0,3.0,15.0,15.0,15.0,16.0,12.0,12.0,15.0,3.0,10.0,13.0,13.0,12.0,16.0,4.0,15.0,16.0,17.0,15.0,15.0,16.0,16.0,12.0,9.0,0.0,12.0,16.0,12.0,13.0,17.0,9.0,14.0,3.0,11.0,9.0,13.0,9.0,15.0,6.0,6.0,15.0,15.0,9.0,3.0,12.0,7.0,16.0,17.0,15.0,12.0,10.0,9.0,15.0,16.0,0.0,3.0,12.0,13.0,12.0,14.0,7.0,21.0,13.0,13.0,12.0,16.0,13.0,15.0,15.0,0.0,12.0,3.0,10.0,4.0,13.0,15.0,15.0,16.0,15.0,11.0,17.0,0.0,15.0,10.0,6.0,12.0,16.0,15.0,16.0,17.0,12.0,4.0,0.0,16.0,16.0,15.0,13.0,13.0,16.0,3.0,6.0,14.0,14.0,15.0,0.0,6.0,13.0,12.0,14.0,16.0,14.0,17.0,12.0,15.0,12.0,15.0,15.0,6.0,14.0,15.0,6.0,3.0,15.0,13.0,15.0,15.0,12.0,15.0,14.0,14.0,16.0,12.0,10.0,13.0,13.0,13.0,13.0,14.0,15.0,6.0,14.0,12.0,12.0,18.0,12.0,12.0,15.0,12.0,16.0,10.0,16.0,14.0,15.0,10.0,12.0,10.0,9.0,16.0,14.0,12.0,16.0,0.0,16.0,17.0,13.0,11.0,3.0,14.0,15.0,16.0,15.0,9.0,12.0,7.0,15.0,12.0,0.0,16.0,16.0,0.0,17.0,15.0,10.0,12.0,0.0,9.0,6.0,12.0,16.0,0.0,14.0,15.0,16.0,16.0,16.0,17.0,1.0,16.0,0.0,13.0,12.0,16.0,0.0,16.0,16.0,15.0,3.0,6.0,9.0,10.0,16.0,15.0,17.0,12.0,13.0,12.0,4.0,3.0,16.0,3.0,16.0,15.0,15.0,3.0,18.0,9.0,13.0,12.0,14.0,12.0,14.0,10.0,14.0,10.0,12.0,7.0,11.0,18.0,16.0,12.0,9.0,3.0,15.0,15.0,3.0,16.0,15.0,16.0,16.0,11.0,15.0,16.0,13.0,4.0,12.0,15.0,9.0,3.0,0.0,14.0,6.0,12.0,15.0,16.0,14.0,18.0,12.0,6.0,13.0,15.0,13.0,0.0,0.0,11.0,10.0,15.0,15.0,8.0,3.0,13.0,16.0,3.0,16.0,13.0,12.0,7.0,0.0,13.0,14.0,7.0,15.0,16.0,15.0,16.0,15.0,15.0,10.0,16.0,7.0,12.0,3.0,14.0,15.0,12.0,9.0,14.0,3.0,6.0,15.0,12.0,13.0,8.0,18.0,15.0,13.0,12.0,16.0,15.0,8.0,10.0,13.0,12.0,13.0,17.0,13.0,12.0,13.0,12.0,0.0,15.0,12.0,16.0,12.0,15.0,16.0,17.0,4.0,3.0,12.0,13.0,16.0,16.0,15.0,13.0,7.0,8.0,15.0,10.0,12.0,15.0,16.0,12.0,3.0,13.0,5.0,16.0,11.0,3.0,16.0,16.0,12.0,4.0,11.0,6.0,7.0,10.0,4.0,12.0,17.0,15.0,1.0,10.0,12.0,15.0,9.0,0.0,5.0,16.0,15.0,16.0,16.0,3.0,9.0,12.0,16.0,10.0,12.0,15.0,9.0,12.0,15.0,6.0,12.0,12.0,6.0,15.0,9.0,14.0,6.0,9.0,3.0,16.0,16.0,15.0,15.0,16.0,15.0,12.0,16.0,11.0,14.0,16.0,13.0,9.0,15.0,13.0,12.0,12.0,18.0,15.0,13.0,13.0,15.0,0.0,15.0,14.0,16.0,16.0,12.0,13.0,6.0,16.0,12.0,12.0,10.0,13.0,15.0,13.0,0.0,14.0,10.0,15.0,18.0,11.0,15.0,16.0,17.0],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"GPA_2\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UNITS_COMPLETED_2\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"SEM_2_STATUS\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.scatter(data_frame=Xy_build_c,y='UNITS_COMPLETED_2',x='GPA_2',color='SEM_2_STATUS')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "be1df4c8-874c-4cf4-8e97-d38a785f413c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nx-qlY6egylO"
      },
      "source": [
        "###Feature Enginering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ac557725-fce6-4406-a744-c048bd256a21",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "A0OT1AOYgylO"
      },
      "source": [
        "To build a model with maximum predictive ability, it is necessary to transform existing variables so that they are in a format scikit learn models can use effectively.\n",
        "There are three closely intertwined considerations\n",
        "\n",
        "1. Domain distinctives - Are some of the variables in our data inherently devoid of predictive value? Are there new variables we can create that are likely to be predictive?\n",
        "\n",
        "2. Statistical subtleties - Are there further transformations that need to take place on existing features to minimize the possibility of poor model performance?\n",
        "\n",
        "3. Coding customization - Is the data presented in a format tailored to scikit learn's predictive modeling infrastructure to readily move through prescribed train-test-validate workflows?\n",
        "\n",
        "\n",
        "We first create the new variables and view them in new dataframes, **X_build_c1** and **X_test_c1**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "acce5b01-67c3-4eda-9fe3-3440dc24e67c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "iFFllymmgylO"
      },
      "source": [
        "#####Domain distinctives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2a66e2b4-d4e3-4348-b82a-83b0e794c63f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ERNrPb03gylO"
      },
      "source": [
        "Given our knowledge of higher ed data, what variables are devoid of information for predicting dropout? Are there new variables we can create that could be informative? To address point 1., we dropped identifier variables above. These variables identify (as opposed to characterize) the \"who\" and \"when\", and in this context provide no information about academic performance. Aditionally, we removed variables that were redundant or unobservable in the third term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ecf932ef-9ffe-4d7e-a8d1-b63e89b7eda9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "l4XZ5IfRgylO"
      },
      "source": [
        "Exploratory data analysis seemed to indicate strong relationships between academic performance leading up to term 3 and retention. Let's create variables for DFW rate as well as grade points for each of the first two terms, for both the train and the test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "81b012c4-89bc-44d8-94e3-540ba3cc8876",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rHw19EcggylO"
      },
      "outputs": [],
      "source": [
        "#Initialize the new dataframe\n",
        "X_build_c1 = X_build_c.copy()\n",
        "\n",
        "\n",
        "X_build_c1['DFW_RATE_1'] = (X_build_c1['UNITS_ATTEMPTED_1']-X_build_c1['UNITS_COMPLETED_1'])/X_build_c1['UNITS_ATTEMPTED_1']\n",
        "\n",
        "#DFW Rate Term 2\n",
        "X_build_c1['DFW_RATE_2'] = (X_build_c1['UNITS_ATTEMPTED_2']-X_build_c1['UNITS_COMPLETED_2'])/X_build_c1['UNITS_ATTEMPTED_2']\n",
        "\n",
        "#Grade points Term 1\n",
        "X_build_c1['GRADE_POINTS_1'] = X_build_c1['UNITS_ATTEMPTED_1']*X_build_c1['GPA_1']\n",
        "\n",
        "#Grade points Term 2\n",
        "X_build_c1['GRADE_POINTS_2'] = X_build_c1['UNITS_ATTEMPTED_2']*X_build_c1['GPA_2']\n",
        "\n",
        "\n",
        "#Repeat the above for the Validation set\n",
        "X_val_c1 = X_val_c.copy()\n",
        "\n",
        "X_val_c1['DFW_RATE_1'] = (X_val_c1['UNITS_ATTEMPTED_1']-X_val_c1['UNITS_COMPLETED_1'])/X_val_c1['UNITS_ATTEMPTED_1']\n",
        "\n",
        "#DFW Rate Term 2\n",
        "X_val_c1['DFW_RATE_2'] = (X_val_c1['UNITS_ATTEMPTED_2']-X_val_c1['UNITS_COMPLETED_2'])/X_val_c1['UNITS_ATTEMPTED_2']\n",
        "\n",
        "#Grade points Term 1\n",
        "X_val_c1['GRADE_POINTS_1'] = X_val_c1['UNITS_ATTEMPTED_1']*X_val_c1['GPA_1']\n",
        "\n",
        "#Grade points Term 2\n",
        "X_val_c1['GRADE_POINTS_2'] = X_val_c1['UNITS_ATTEMPTED_2']*X_val_c1['GPA_2']\n",
        "\n",
        "\n",
        "#Repeat the above for the Test set\n",
        "X_test_c1 = X_test_c.copy()\n",
        "\n",
        "X_test_c1['DFW_RATE_1'] = (X_test_c1['UNITS_ATTEMPTED_1']-X_test_c1['UNITS_COMPLETED_1'])/X_test_c1['UNITS_ATTEMPTED_1']\n",
        "X_test_c1['DFW_RATE_2'] = (X_test_c1['UNITS_ATTEMPTED_2']-X_test_c1['UNITS_COMPLETED_2'])/X_test_c1['UNITS_ATTEMPTED_2']\n",
        "\n",
        "#Grade points Term 1\n",
        "X_test_c1['GRADE_POINTS_1'] = X_test_c1['UNITS_ATTEMPTED_1']*X_test_c1['GPA_1']\n",
        "\n",
        "#Grade points Term 2\n",
        "X_test_c1['GRADE_POINTS_2'] = X_test_c1['UNITS_ATTEMPTED_2']*X_test_c1['GPA_2']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "71a5f5a7-b7f6-4f3a-95d4-98f4352ce57c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "i5CQ8KH0gylO"
      },
      "source": [
        "Let's calculate the correlation among predictors. Some machine learning models are sensitive to high correlations, so we'll keep in mind as we proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "63611091-ad62-4cc3-b79f-0e7fe75d4740",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8GfpzwHhgylO",
        "outputId": "3fb9a666-196c-458e-e778-614df7f9464e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/spark-5e7b7ea8-cc39-44a2-9105-50/.ipykernel/2597/command-5144643411401066-3949783157:2: FutureWarning:\n\nThe default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HS_MATH_GPA          0.382767\n",
              "HS_ENGL_GPA          0.440109\n",
              "UNITS_ATTEMPTED_1    0.082034\n",
              "UNITS_ATTEMPTED_2    0.234575\n",
              "UNITS_COMPLETED_1    0.540799\n",
              "UNITS_COMPLETED_2    0.812027\n",
              "GPA_1                0.636078\n",
              "GPA_2                1.000000\n",
              "DFW_RATE_1          -0.561511\n",
              "DFW_RATE_2          -0.866516\n",
              "GRADE_POINTS_1       0.610332\n",
              "GRADE_POINTS_2       0.923573\n",
              "Name: GPA_2, dtype: float64"
            ]
          },
          "execution_count": 216,
          "metadata": {}
        }
      ],
      "source": [
        "#Calulating the pairwise Pearson correlation coefficient between the response and each predictor\n",
        "X_build_c1.corr().loc[:,'GPA_2']\n",
        "\n",
        "#Note this is done strictly for exploratory purposes, so it is not necessary to apply to the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f205d545-b791-4bdd-9b30-2f4b8b925af7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6xELDROjgylP"
      },
      "outputs": [],
      "source": [
        "#fig = px.scatter_matrix(Xy_build_c1,\n",
        " #   dimensions=['HS_ENGL_GPA','DFW_RATE_1','UNITS_ATTEMPTED_1','DFW_RATE_1','GRADE_POINTS_1'],\n",
        " #   color='SEM_2_STATUS')\n",
        "#fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0af2d7b8-5b25-409f-a6b2-c1da5057cc87",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6imCUa88gylP"
      },
      "source": [
        "###Addressing Class Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c0d0fdb8-05a1-47a2-b7ab-d9a609a0f00d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GLrm-_CIgylP"
      },
      "source": [
        "\n",
        "The build set and its target have a total of 6,852 observations (6,070 retained and 782 not retained) each. As mentioned above, however, there is severe *class imbalance*, meaning that there are not (approximately) equal amounts of both classes represented in the data. Let's check it out visually:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cbadce71-6810-49ce-966b-14a4b3a14043",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5AH7L9RrgylP",
        "outputId": "2b9d77dd-040c-47f9-c11c-e4fb65241b75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"9dc5aa61-df54-4b46-8f82-543a64176f9f\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9dc5aa61-df54-4b46-8f82-543a64176f9f\")) {                    Plotly.newPlot(                        \"9dc5aa61-df54-4b46-8f82-543a64176f9f\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[0,1],\"xaxis\":\"x\",\"y\":[0.8866100205098154,0.11338997949018459],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Bar graph visualizing class imbalance\n",
        "px.bar(x=np.unique(y_build_c),y=y_build_c.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aa4d99e0-8b26-4f7c-a58f-2121a91fb0e8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QC75VPQlgylP"
      },
      "source": [
        "There are 8 times as many students that are retained after year 1 than are not (i.e. the *odds* of being retained are 8 times higher than not being retained). This is a good thing for the university, but poses challenges for the classification task. This could cause our algorithm to memorize and learn the majority class so well that it performs poorly on the minority examples. Moreover, it could lead to a false sense of security. Suppose we blindly predict that every student in our DataFrame is retained. Then 90% of our predictions would be correct, which seems great, except for the fact that 0% of our predictions would be correct for students that drop out - which is the more important class to predict correctly. By *resampling* from the minority class, we can build an artificially balanced dataset, and typically boost classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "82be518e-05aa-4b58-83a5-234fd698dd62",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PNWCh43ZgylP"
      },
      "outputs": [],
      "source": [
        "#First, again we combine the features and labels into one DataFrame, as the labels will dictate how we use the corresponding features.\n",
        "\n",
        "y_build.name = 'SEM_2_STATUS'\n",
        "Xy_build_c1 = pd.concat([X_build_c1,y_build_c], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "90bb446d-d053-45a3-8b23-4c25e06a83e5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "TWPN52THgylP"
      },
      "source": [
        "Next we use numpy to randomly sample with replacement from the minority class. Sometimes this process is referred to as *bootstrap resampling*. How many additional dropout samples do we need?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "65460599-68c5-46fd-9dc6-58ebc0b2556c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ePJ6ocZIgylP",
        "outputId": "bce290fc-8ca7-4b86-b020-b92d82548216"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5278"
            ]
          },
          "execution_count": 220,
          "metadata": {}
        }
      ],
      "source": [
        "#Difference between number retained and number that leave\n",
        "counts = pd.value_counts(y_build_c)\n",
        "counts[0]-counts[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "13081a57-af32-4c49-9c00-ba27477abd04",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "lcCZudUGgylQ",
        "outputId": "825974b8-320e-4abe-8c61-765d9bee6146"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>GRADE_POINTS_1</th>\n",
              "      <th>GRADE_POINTS_2</th>\n",
              "      <th>SEM_2_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9579</th>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.88</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5611</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.90</td>\n",
              "      <td>Business</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9379</th>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.89</td>\n",
              "      <td>3.39</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>3.454545</td>\n",
              "      <td>C</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>44.625000</td>\n",
              "      <td>48.363636</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.93</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15819</th>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>4.26</td>\n",
              "      <td>3.84</td>\n",
              "      <td>Health</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15565</th>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.02</td>\n",
              "      <td>3.15</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>C</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15149</th>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.23</td>\n",
              "      <td>Health</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>2.250000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>33.750000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14490</th>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.13</td>\n",
              "      <td>3.60</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>47.666667</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15490</th>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.24</td>\n",
              "      <td>3.88</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14439</th>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.64</td>\n",
              "      <td>4.12</td>\n",
              "      <td>Health</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.071429</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12104 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  HS_MATH_GPA  \\\n",
              "9579                       White  Female  Continuing Generation         3.72   \n",
              "5611             Hispanic/Latino    Male  Continuing Generation         3.57   \n",
              "9379                       Asian    Male  Continuing Generation         2.89   \n",
              "5237             Hispanic/Latino  Female  Continuing Generation         3.63   \n",
              "15819              Visa Non-U.S.  Female                Unknown         4.26   \n",
              "...                          ...     ...                    ...          ...   \n",
              "15565  Black or African American  Female  Continuing Generation         3.02   \n",
              "15149                      White  Female  Continuing Generation         4.05   \n",
              "14490                      White    Male  Continuing Generation         3.13   \n",
              "15490            Hispanic/Latino    Male  Continuing Generation         3.24   \n",
              "14439              Visa Non-U.S.  Female  Continuing Generation         3.64   \n",
              "\n",
              "       HS_ENGL_GPA              COLLEGE  UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  \\\n",
              "9579          3.88                 Arts               15.0               15.0   \n",
              "5611          3.90             Business               15.0               12.0   \n",
              "9379          3.39         Liberal Arts               17.0               14.0   \n",
              "5237          3.93         Liberal Arts               16.0               16.0   \n",
              "15819         3.84               Health               13.0               14.0   \n",
              "...            ...                  ...                ...                ...   \n",
              "15565         3.15  University Programs               12.0               16.0   \n",
              "15149         4.23               Health               15.0               15.0   \n",
              "14490         3.60  University Programs               13.0               16.0   \n",
              "15490         3.88  University Programs               16.0               12.0   \n",
              "14439         4.12               Health               15.0               14.0   \n",
              "\n",
              "       UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  \\\n",
              "9579                15.0               15.0  3.800000  3.200000            C   \n",
              "5611                15.0               12.0  3.400000  4.000000            C   \n",
              "9379                14.0               11.0  2.625000  3.454545            C   \n",
              "5237                16.0               16.0  4.000000  2.937500            C   \n",
              "15819               13.0               14.0  4.000000  2.500000            C   \n",
              "...                  ...                ...       ...       ...          ...   \n",
              "15565                0.0                0.0  0.166667  0.333333            C   \n",
              "15149               12.0               12.0  1.800000  2.250000            C   \n",
              "14490               13.0               13.0  3.666667  3.250000            C   \n",
              "15490               16.0               12.0  3.000000  2.750000            C   \n",
              "14439               15.0                9.0  4.000000  2.071429            C   \n",
              "\n",
              "       DFW_RATE_1  DFW_RATE_2  GRADE_POINTS_1  GRADE_POINTS_2  SEM_2_STATUS  \n",
              "9579     0.000000    0.000000       57.000000       48.000000             0  \n",
              "5611     0.000000    0.000000       51.000000       48.000000             0  \n",
              "9379     0.176471    0.214286       44.625000       48.363636             0  \n",
              "5237     0.000000    0.000000       64.000000       47.000000             0  \n",
              "15819    0.000000    0.000000       52.000000       35.000000             1  \n",
              "...           ...         ...             ...             ...           ...  \n",
              "15565    1.000000    1.000000        2.000000        5.333333             1  \n",
              "15149    0.200000    0.200000       27.000000       33.750000             1  \n",
              "14490    0.000000    0.187500       47.666667       52.000000             1  \n",
              "15490    0.000000    0.000000       48.000000       33.000000             1  \n",
              "14439    0.000000    0.357143       60.000000       29.000000             1  \n",
              "\n",
              "[12104 rows x 18 columns]"
            ]
          },
          "execution_count": 221,
          "metadata": {}
        }
      ],
      "source": [
        "#The seed ensures reproducibility\n",
        "rms = 34\n",
        "rng = np.random.RandomState(rms)\n",
        "\n",
        "#\n",
        "Xy_build_c1_pos = Xy_build_c1[Xy_build_c1['SEM_2_STATUS']==1].iloc[list(rng.randint(low=0,high=counts[1],size=counts[0]-counts[1])),:]\n",
        "\n",
        "#Now we concatenate rows to create the new data\n",
        "Xy_build_bal = pd.concat([Xy_build_c1,Xy_build_c1_pos],axis=0)\n",
        "Xy_build_bal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fc283ed4-f3b3-48f4-8bcb-8999c84b9292",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "w8iILXgXgylQ"
      },
      "source": [
        "There are other more complex algorithms for generating brand new, realistic but simulated samples for the minority class. One of the most popular is the synthetic minority over-sampling technique, or SMOTE. It can be implemented as a method in the [imbalanced-learn](https://imbalanced-learn.org/stable/index.html) library. Documentation for SMOTE, and links to similar methods can be found [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html).\n",
        "\n",
        "Let's separate our target variable once again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a15ec5af-e359-47d7-9989-7b79270a9fcc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zqXTss8kgylQ"
      },
      "outputs": [],
      "source": [
        "X_build_bal = Xy_build_bal.drop('SEM_2_STATUS',axis=1)\n",
        "y_build_bal = Xy_build_bal['SEM_2_STATUS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "40a6d2c7-f38f-4aef-ad36-754ec6208785",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hksO1-ZggylQ"
      },
      "source": [
        "We can verify numerically and graphically that the classes are now balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aad3e189-031e-446d-9fc6-8536694d583e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uNZQsEkzgylQ",
        "outputId": "70a317c2-3f77-4be1-a999-c1e3a36aea87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9579     0\n",
              "5611     0\n",
              "9379     0\n",
              "5237     0\n",
              "15819    1\n",
              "        ..\n",
              "15565    1\n",
              "15149    1\n",
              "14490    1\n",
              "15490    1\n",
              "14439    1\n",
              "Name: SEM_2_STATUS, Length: 12104, dtype: int64"
            ]
          },
          "execution_count": 223,
          "metadata": {}
        }
      ],
      "source": [
        "y_build_bal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "862190cc-d960-4a54-8ba3-cb983df686ee",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QYxyZLvhgylQ",
        "outputId": "4888db61-7373-4679-816a-afa836e316aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6052\n",
              "1    6052\n",
              "Name: SEM_2_STATUS, dtype: int64"
            ]
          },
          "execution_count": 224,
          "metadata": {}
        }
      ],
      "source": [
        "y_build_bal.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3d96bde9-201e-40cb-9d5b-b058f1f06b31",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_dpIazbkgylQ",
        "outputId": "4e081ac3-0057-4abb-b434-a4c5940fd128"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"33f31027-f551-45f5-a378-cc129c8ffdb5\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"33f31027-f551-45f5-a378-cc129c8ffdb5\")) {                    Plotly.newPlot(                        \"33f31027-f551-45f5-a378-cc129c8ffdb5\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[0,1],\"xaxis\":\"x\",\"y\":[0.5,0.5],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "px.bar(x=np.unique(y_build_bal),y=y_build_bal.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0593ac77-7a1a-4bb9-a9e6-919877c68609",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "a2THJeU2gylQ"
      },
      "source": [
        "At this point it will be helpful to create a balanced version of the entire train set as well. We won't use it til later on, in the model testing stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e266320c-360e-4f62-909d-ab6d69320087",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2dZEbh6cgylR"
      },
      "outputs": [],
      "source": [
        "#The seed ensures reproducibility\n",
        "rms = 34\n",
        "rng = np.random.RandomState(rms)\n",
        "\n",
        "X_train_c1 = pd.concat([X_build_c1,X_val_c1], axis=0)\n",
        "y_train_c = pd.concat([y_build_c,y_val_c], axis=0)\n",
        "\n",
        "Xy_train_c1 = pd.concat([X_train_c1,y_train_c], axis=1)\n",
        "\n",
        "#Difference between number retained and number that leave\n",
        "counts = pd.value_counts(y_train_c)\n",
        "counts[0]-counts[1]\n",
        "\n",
        "#The following code ensures reproducibility of np.random.randint\n",
        "Xy_train_c1_pos = Xy_train_c1[Xy_train_c1['SEM_2_STATUS']==1].iloc[list(rng.randint(low=0,high=counts[1],size=counts[0]-counts[1])),:]\n",
        "\n",
        "#Now we concatenate rows to create the new data\n",
        "Xy_train_bal = pd.concat([Xy_train_c1,Xy_train_c1_pos],axis=0)\n",
        "\n",
        "X_train_bal = Xy_train_bal.drop('SEM_2_STATUS',axis=1)\n",
        "y_train_bal = Xy_train_bal['SEM_2_STATUS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "94c2b0e0-1b9d-4b49-aada-ab57884d732e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6QEioKEOgylR",
        "outputId": "3ca31975-0cb8-4976-be92-077d35390d08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6922\n",
              "1    6922\n",
              "Name: SEM_2_STATUS, dtype: int64"
            ]
          },
          "execution_count": 227,
          "metadata": {}
        }
      ],
      "source": [
        "pd.value_counts(Xy_train_bal['SEM_2_STATUS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5056a931-bb54-4c13-bf31-3d9b313aefc2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Rsc7MF-8gylR"
      },
      "source": [
        "#####Statistical subtleties and Coding customization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "098b7d31-366b-4b75-a745-480a6900ba54",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "XvtEauoCgylR"
      },
      "source": [
        "As indicated earlier, there are essentially two types of features in our data, and it is best practcie to pre-process each type in a different way to prepare it for effective inclusion in our model.\n",
        "\n",
        "1. Quantitative or numeric variables are typically measured on different scales, and machine learning models can be sensitive to that. For example, while GPA's stay in the \"ones\" place value (between 0 and 4), units per semester are in the \"tens\". Variables such as parental income or financial aid award or student loan amounts would have many values in the tens of thousands or even higher. Values on a larger scale can wield disproportional influnce on model parameters due to their sheer magnitude, regardless their actual importance. Two methods used to level the playing field among predictors are\n",
        "\n",
        "  a. **Standardization.**\n",
        "  Suppose \\\\(x_1,x_2,\\ldots,x_n\\\\) are the values of a given variable in our data. Further let\n",
        "\n",
        "  \\\\(\\bar{x}=mean(x_1,x_2,\\ldots,x_n)\\\\) and\n",
        "  \\\\(s=sd(x_1,x_2,\\ldots,x_n)\\\\)\n",
        "  represent the mean and standard deviation of our variable, respectively. Then the *standardized* version of each value \\\\(x_i\\\\) would be\n",
        "  $$ z_i = \\frac{x_i - \\bar{x}}{s}$$\n",
        "\n",
        "  for \\\\(i = 1,\\ldots,n\\\\). This results in variable values typically between -5 and 5 for all variables.\n",
        "\n",
        "  b. **Min-Max Scaling.**\n",
        "  Suppose \\\\(x_1,x_2,\\ldots,x_n\\\\) are the values of a given variable in our data. Further let\n",
        "\n",
        "  \\\\(min = minimum(x_1,x_2,\\ldots,x_n)\\\\) and\n",
        "  \\\\(max=maximum(x_1,x_2,\\ldots,x_n)\\\\)\n",
        "  represent the smallest and largest values of our variable, respectively. Then the *max-min normalized* version of each value \\\\(x_i\\\\) would be\n",
        "  $$ u_i = \\frac{x_i - min}{max}$$\n",
        "\n",
        "  for \\\\(i=1,\\ldots,n\\\\). This results in variable values between 0 and 1 for all variables.\n",
        "\n",
        "More details on these techniques can be found in *Get Your Data Machine Learn Ready* in Module 3.\n",
        "\n",
        "\n",
        "2. Qualitative or categorical variables are primarily encoded in strings. This is a non-starter for analysis, as all variables input into a predictive model must be numerical. As mentioned above, **One hot encoding** is a technique used to assign numeric values to categorical variables without losing the identification of each observation as falling under a given category. The one hot encoder in scikit learn will create a new variable for each possibility in the original variable. For example, three new variables will be created corresponding to the FIRST_GEN_STATUS variable:\n",
        "\n",
        "  - First Generation indicator: each first gen student will be assigned a value of 1, all others 0.\n",
        "\n",
        "  - Continuing Generation indicator: each continuing gen student will be assigned a value of 1, all others 0.\n",
        "\n",
        "  - Unknown indicator: each student with unknown first gen status will be assigned a value of 1, all others 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b9f16f1d-1215-4cac-89bd-6cdb20d4f290",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "S1ubbhD5gylR"
      },
      "source": [
        "###Preparing the Preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "41e823fd-296b-414f-b4bc-297fcca10916",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QatF0Ox5gylS"
      },
      "source": [
        "At this stage we introduce yet another powerful tool within scikit learn - *Preprocessors*. These provide us the ability to create a smooth workflow that automatically implements a given transformation of our choosing based on the type of variable - quantitative or qualitative. It is highly recommended to apply the transformations on the train and test feature sets seperately yet simultaneously, in parallel. Preprocessors will be the first step of *Pipelines* that take us from our engineered data to classification results in one click."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f9dc5388-acbc-4343-8b94-e23946af4b38",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "0c3ZNajGgylS"
      },
      "outputs": [],
      "source": [
        "#Classes for transforming quantitative and qualitative variables, respectively in the Preprocessing module\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "\n",
        "#Module for create new transformed columns, needed for newly created indicators\n",
        "from sklearn.compose import ColumnTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3b675861-2ffb-41e4-ad3e-7af12883b507",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UgVkp1K8gylS"
      },
      "source": [
        "######Step 1 - Select Features to Include"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5245a555-a12b-4365-a023-23a1e3b800f7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8GhwtBGGgylS"
      },
      "source": [
        "Recall the features in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0c1ed6f0-c9dc-411b-a4df-2589a1b1b5f0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fkBDx_2vgylS",
        "outputId": "1a99fa5e-ab1e-4fe4-80ec-e7afe5b064fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS', 'HS_MATH_GPA',\n",
              "       'HS_ENGL_GPA', 'COLLEGE', 'UNITS_ATTEMPTED_1', 'UNITS_ATTEMPTED_2',\n",
              "       'UNITS_COMPLETED_1', 'UNITS_COMPLETED_2', 'GPA_1', 'GPA_2',\n",
              "       'SEM_1_STATUS', 'DFW_RATE_1', 'DFW_RATE_2', 'GRADE_POINTS_1',\n",
              "       'GRADE_POINTS_2'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 229,
          "metadata": {}
        }
      ],
      "source": [
        "X_build_bal.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e97a3b95-0ce2-4c28-917a-1b10b09020b2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "FFPe89DVgylS",
        "outputId": "079eed82-fb64-461c-cf08-c75510836473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Female    7322\n",
              "Male      4782\n",
              "Name: GENDER, dtype: int64"
            ]
          },
          "execution_count": 230,
          "metadata": {}
        }
      ],
      "source": [
        "pd.value_counts(X_build_bal['GENDER'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "80c09f36-f3f9-424e-99cf-1160e10615e8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Hm0nq8hYgylS"
      },
      "source": [
        "Due to multicollinearity or adverse impacts of high dimensionality, some machine learning models recommend a reduction of our feature space. The code below allows us to select a subset of features if so desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93e05726-53d1-44d2-85e5-bd8fd894c736",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "OH6c1X4vgylS"
      },
      "outputs": [],
      "source": [
        "#First identify the variables to include in the analysis\n",
        "selected_columns = ['HS_ENGL_GPA', 'HS_MATH_GPA', 'GPA_1','UNITS_ATTEMPTED_1','DFW_RATE_1', 'UNITS_ATTEMPTED_2', 'GPA_2', 'DFW_RATE_2', 'RACE_ETHNICITY', 'GENDER', 'FIRST_GEN_STATUS']\n",
        "\n",
        "#To use all features select line below\n",
        "#selected_columns = X_build_bal.columns\n",
        "\n",
        "Build_c = X_build_bal[selected_columns]\n",
        "Train_c = X_train_bal[selected_columns]\n",
        "\n",
        "Val_c = X_val_c1[selected_columns]\n",
        "Test_c = X_test_c1[selected_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0ed72566-67f7-40af-be64-cfc44fcc8298",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1vuSnHbNgylS"
      },
      "source": [
        "######Step 2 - Automatically Identify Qualitative Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7b98ed96-7045-47ad-bd3a-b2170b04ef55",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "03v0xCjrgylT"
      },
      "outputs": [],
      "source": [
        "#Qualitative features are denoted by the 'object' type. List their names.\n",
        "categorical_columns=list(Build_c.select_dtypes('object').columns)\n",
        "\n",
        "#Identify the classes in each qualitative variable\n",
        "all_categories = [Build_c[col].unique() for col in categorical_columns]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d90b0d79-f96f-4e30-ad82-93a01def57b1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ylp5B2swgylT"
      },
      "source": [
        "######Step 3 - Automatically Identify Quantitative Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a778a91f-679a-454a-962d-3d990417cf00",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "QN4WbbSugylT"
      },
      "outputs": [],
      "source": [
        "#Quantitative features are denoted by the 'float64' type. List their names.\n",
        "numeric_columns=list(Build_c.select_dtypes('float64').columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "082ea28d-762e-4ad2-8c15-86fe668782c3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "NefTL9RfgylT"
      },
      "source": [
        "######Step 4 - Create Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c4033b8e-8cac-4c06-9282-63a825d5d37f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "arEeaKQZgylT"
      },
      "outputs": [],
      "source": [
        "#Create a function that inputs our feature set and transforms variables (standardizes numerical, and one hot encodes categorical) based on variable type\n",
        "preprocessor=ColumnTransformer([\n",
        "    ('num',StandardScaler(),numeric_columns),\n",
        "    ('cat',OneHotEncoder(categories = all_categories),categorical_columns),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d7fb35d7-681e-4cb2-9722-82c86eba1c94",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YSd7_IFJgylT"
      },
      "source": [
        "The *preprocessor* object we just created allows us to visualize how the preprocessing workflow is applied to our dataframe. Click on the triangles to reveal details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c701d26a-9338-45a4-8c4a-c40c52dc922e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fN11hwd6gylT",
        "outputId": "3d7c4672-0cd3-4e56-9d3f-b4aba72f6e21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                           array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                           array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                 [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                  &#x27;FIRST_GEN_STATUS&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                           array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                           array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                 [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                  &#x27;FIRST_GEN_STATUS&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;, &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;, &#x27;FIRST_GEN_STATUS&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                          array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                          array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)])</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "ColumnTransformer(transformers=[('num', StandardScaler(),\n",
              "                                 ['HS_ENGL_GPA', 'HS_MATH_GPA', 'GPA_1',\n",
              "                                  'UNITS_ATTEMPTED_1', 'DFW_RATE_1',\n",
              "                                  'UNITS_ATTEMPTED_2', 'GPA_2', 'DFW_RATE_2']),\n",
              "                                ('cat',\n",
              "                                 OneHotEncoder(categories=[array(['White', 'Hispanic/Latino', 'Asian', 'Visa Non-U.S.',\n",
              "       'Two or More Races', 'Other', 'Black or African American'],\n",
              "      dtype=object),\n",
              "                                                           array(['Female', 'Male'], dtype=object),\n",
              "                                                           array(['Continuing Generation', 'Unknown', 'First Generation'],\n",
              "      dtype=object)]),\n",
              "                                 ['RACE_ETHNICITY', 'GENDER',\n",
              "                                  'FIRST_GEN_STATUS'])])"
            ]
          },
          "execution_count": 235,
          "metadata": {}
        }
      ],
      "source": [
        "preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e2347ae9-c666-4b57-a45c-e6198372dbdc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "31nJTDHGgylT"
      },
      "source": [
        "######Step 5 - Apply the Preprocessor to our Training and Test Data to Transform Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6a59e588-58fb-4b4d-8c36-3d453b83ec6a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ko8A4iqzgylT"
      },
      "outputs": [],
      "source": [
        "# Transform the training dataframes. Note that the output is a numpy array\n",
        "X_build_array = preprocessor.fit_transform(Build_c)\n",
        "X_train_array = preprocessor.fit_transform(Train_c)\n",
        "X_val_array = preprocessor.fit_transform(Val_c)\n",
        "X_test_array = preprocessor.fit_transform(Test_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4993f721-4a44-4ff5-8138-06c0c3549651",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Ed0V4P0FgylT"
      },
      "source": [
        "###### Step 6 - Create Final Feature DataFrames with Column Names for Newly Created Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "41bbc15a-623f-49c6-bef4-2cd7c59d83c2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YmPedWoVgylT"
      },
      "source": [
        "Although scikit learn can work with NumPy arrays, it will be to our advantage to work with Pandas DataFrames in which variable names are clearly identified. The following code changes the arrays back to DataFrames, and also gives names to the new indicators resulting from the one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "42ccbaa2-b97e-4150-8337-e6fab519e473",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "C2kTCzeUgylT"
      },
      "outputs": [],
      "source": [
        "# Get column names for one hot encoded columns\n",
        "ohe_column_names = (preprocessor.named_transformers_['cat']\n",
        "                    .get_feature_names_out(categorical_columns)) #Note that depending on your version of sklearn, you may need to use either get_feature_names_out or get_feature_names\n",
        "\n",
        "# Combine original numeric column names and one hot encoded column names\n",
        "all_column_names = numeric_columns + ohe_column_names.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ba3df476-7c3c-41c4-8d2b-c8a71628e81b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "KWPV0E5cgylU"
      },
      "outputs": [],
      "source": [
        "# Convert the numpy array back to dataframe with column names to make model interpretation easier\n",
        "X_build_p = pd.DataFrame(X_build_array, columns=all_column_names, index = X_build_bal.index)\n",
        "X_train_p = pd.DataFrame(X_train_array, columns=all_column_names, index = X_train_bal.index)\n",
        "X_val_p = pd.DataFrame(X_val_array, columns=all_column_names, index = X_val_c1.index)\n",
        "X_test_p = pd.DataFrame(X_test_array, columns=all_column_names, index = X_test_c1.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fb0577a3-0a9b-43d4-adb5-50d198f771eb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YPOCVzsRgylU",
        "outputId": "3592b076-16a1-46a1-fab2-733f753fb8a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['RACE_ETHNICITY_White', 'RACE_ETHNICITY_Hispanic/Latino',\n",
              "       'RACE_ETHNICITY_Asian', 'RACE_ETHNICITY_Visa Non-U.S.',\n",
              "       'RACE_ETHNICITY_Two or More Races', 'RACE_ETHNICITY_Other',\n",
              "       'RACE_ETHNICITY_Black or African American', 'GENDER_Female',\n",
              "       'GENDER_Male', 'FIRST_GEN_STATUS_Continuing Generation',\n",
              "       'FIRST_GEN_STATUS_Unknown', 'FIRST_GEN_STATUS_First Generation'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 239,
          "metadata": {}
        }
      ],
      "source": [
        "ohe_column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "990a35d1-d5c0-47dc-a4c1-e4fb56f57231",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PvQ5otqFgylU"
      },
      "source": [
        "We now have more variables than before we preprocessed the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "096b3cdb-5baa-4b87-a5b7-55383a8e4a88",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8c5iVesNgylU",
        "outputId": "9efa01e5-e301-4e24-82cc-000c24612305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "print(len(all_column_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a8da93ff-fcab-4d1d-be51-6592ee5b1cab",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "NfkHoLkMgylU",
        "outputId": "b2d0325c-424f-43bd-a5c6-c0051f7afeac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['HS_ENGL_GPA',\n",
              " 'HS_MATH_GPA',\n",
              " 'GPA_1',\n",
              " 'UNITS_ATTEMPTED_1',\n",
              " 'DFW_RATE_1',\n",
              " 'UNITS_ATTEMPTED_2',\n",
              " 'GPA_2',\n",
              " 'DFW_RATE_2',\n",
              " 'RACE_ETHNICITY_White',\n",
              " 'RACE_ETHNICITY_Hispanic/Latino',\n",
              " 'RACE_ETHNICITY_Asian',\n",
              " 'RACE_ETHNICITY_Visa Non-U.S.',\n",
              " 'RACE_ETHNICITY_Two or More Races',\n",
              " 'RACE_ETHNICITY_Other',\n",
              " 'RACE_ETHNICITY_Black or African American',\n",
              " 'GENDER_Female',\n",
              " 'GENDER_Male',\n",
              " 'FIRST_GEN_STATUS_Continuing Generation',\n",
              " 'FIRST_GEN_STATUS_Unknown',\n",
              " 'FIRST_GEN_STATUS_First Generation']"
            ]
          },
          "execution_count": 241,
          "metadata": {}
        }
      ],
      "source": [
        "all_column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d6ae740b-85cf-41dc-948b-c0e12133224c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "LTrqZL-NgylU"
      },
      "source": [
        "Recall that the steps above were applied to the train and test sets in parallel. All preprocessing steps should be applied to the test set before model predictions are made. Remember not to view or analyze the test set in any way until after your final model is selected.\n",
        "\n",
        "Let's take a look at our preprocessed training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "477b21eb-b21c-4cd7-8cbd-17e06ac13946",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "r_mn0wINgylU",
        "outputId": "0e962874-30ab-40e3-fe33-6f4ee0526bb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>RACE_ETHNICITY_White</th>\n",
              "      <th>RACE_ETHNICITY_Hispanic/Latino</th>\n",
              "      <th>RACE_ETHNICITY_Asian</th>\n",
              "      <th>RACE_ETHNICITY_Visa Non-U.S.</th>\n",
              "      <th>RACE_ETHNICITY_Two or More Races</th>\n",
              "      <th>RACE_ETHNICITY_Other</th>\n",
              "      <th>RACE_ETHNICITY_Black or African American</th>\n",
              "      <th>GENDER_Female</th>\n",
              "      <th>GENDER_Male</th>\n",
              "      <th>FIRST_GEN_STATUS_Continuing Generation</th>\n",
              "      <th>FIRST_GEN_STATUS_Unknown</th>\n",
              "      <th>FIRST_GEN_STATUS_First Generation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9579</th>\n",
              "      <td>0.319915</td>\n",
              "      <td>0.476021</td>\n",
              "      <td>1.000298</td>\n",
              "      <td>0.460734</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>0.628562</td>\n",
              "      <td>0.621215</td>\n",
              "      <td>-0.853914</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5611</th>\n",
              "      <td>0.368935</td>\n",
              "      <td>0.149713</td>\n",
              "      <td>0.685207</td>\n",
              "      <td>0.460734</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>-0.647753</td>\n",
              "      <td>1.196482</td>\n",
              "      <td>-0.853914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9379</th>\n",
              "      <td>-0.881066</td>\n",
              "      <td>-1.329552</td>\n",
              "      <td>0.074718</td>\n",
              "      <td>1.688007</td>\n",
              "      <td>-0.272540</td>\n",
              "      <td>0.203123</td>\n",
              "      <td>0.804254</td>\n",
              "      <td>-0.308263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>0.442465</td>\n",
              "      <td>0.280236</td>\n",
              "      <td>1.157844</td>\n",
              "      <td>1.074371</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>1.054000</td>\n",
              "      <td>0.432455</td>\n",
              "      <td>-0.853914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15819</th>\n",
              "      <td>0.221876</td>\n",
              "      <td>1.650731</td>\n",
              "      <td>1.157844</td>\n",
              "      <td>-0.766539</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>0.203123</td>\n",
              "      <td>0.117856</td>\n",
              "      <td>-0.853914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15565</th>\n",
              "      <td>-1.469301</td>\n",
              "      <td>-1.046751</td>\n",
              "      <td>-1.861780</td>\n",
              "      <td>-1.380175</td>\n",
              "      <td>2.083284</td>\n",
              "      <td>1.054000</td>\n",
              "      <td>-1.440158</td>\n",
              "      <td>1.692457</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15149</th>\n",
              "      <td>1.177759</td>\n",
              "      <td>1.193899</td>\n",
              "      <td>-0.575157</td>\n",
              "      <td>0.460734</td>\n",
              "      <td>-0.205231</td>\n",
              "      <td>0.628562</td>\n",
              "      <td>-0.061915</td>\n",
              "      <td>-0.344640</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14490</th>\n",
              "      <td>-0.366359</td>\n",
              "      <td>-0.807458</td>\n",
              "      <td>0.895268</td>\n",
              "      <td>-0.766539</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>1.054000</td>\n",
              "      <td>0.657169</td>\n",
              "      <td>-0.376470</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15490</th>\n",
              "      <td>0.319915</td>\n",
              "      <td>-0.568166</td>\n",
              "      <td>0.370116</td>\n",
              "      <td>1.074371</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>-0.647753</td>\n",
              "      <td>0.297627</td>\n",
              "      <td>-0.853914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14439</th>\n",
              "      <td>0.908151</td>\n",
              "      <td>0.301990</td>\n",
              "      <td>1.157844</td>\n",
              "      <td>0.460734</td>\n",
              "      <td>-0.777360</td>\n",
              "      <td>0.203123</td>\n",
              "      <td>-0.190322</td>\n",
              "      <td>0.055504</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12104 rows × 20 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       HS_ENGL_GPA  HS_MATH_GPA     GPA_1  UNITS_ATTEMPTED_1  DFW_RATE_1  \\\n",
              "9579      0.319915     0.476021  1.000298           0.460734   -0.777360   \n",
              "5611      0.368935     0.149713  0.685207           0.460734   -0.777360   \n",
              "9379     -0.881066    -1.329552  0.074718           1.688007   -0.272540   \n",
              "5237      0.442465     0.280236  1.157844           1.074371   -0.777360   \n",
              "15819     0.221876     1.650731  1.157844          -0.766539   -0.777360   \n",
              "...            ...          ...       ...                ...         ...   \n",
              "15565    -1.469301    -1.046751 -1.861780          -1.380175    2.083284   \n",
              "15149     1.177759     1.193899 -0.575157           0.460734   -0.205231   \n",
              "14490    -0.366359    -0.807458  0.895268          -0.766539   -0.777360   \n",
              "15490     0.319915    -0.568166  0.370116           1.074371   -0.777360   \n",
              "14439     0.908151     0.301990  1.157844           0.460734   -0.777360   \n",
              "\n",
              "       UNITS_ATTEMPTED_2     GPA_2  DFW_RATE_2  RACE_ETHNICITY_White  \\\n",
              "9579            0.628562  0.621215   -0.853914                   1.0   \n",
              "5611           -0.647753  1.196482   -0.853914                   0.0   \n",
              "9379            0.203123  0.804254   -0.308263                   0.0   \n",
              "5237            1.054000  0.432455   -0.853914                   0.0   \n",
              "15819           0.203123  0.117856   -0.853914                   0.0   \n",
              "...                  ...       ...         ...                   ...   \n",
              "15565           1.054000 -1.440158    1.692457                   0.0   \n",
              "15149           0.628562 -0.061915   -0.344640                   1.0   \n",
              "14490           1.054000  0.657169   -0.376470                   1.0   \n",
              "15490          -0.647753  0.297627   -0.853914                   0.0   \n",
              "14439           0.203123 -0.190322    0.055504                   0.0   \n",
              "\n",
              "       RACE_ETHNICITY_Hispanic/Latino  RACE_ETHNICITY_Asian  \\\n",
              "9579                              0.0                   0.0   \n",
              "5611                              1.0                   0.0   \n",
              "9379                              0.0                   1.0   \n",
              "5237                              1.0                   0.0   \n",
              "15819                             0.0                   0.0   \n",
              "...                               ...                   ...   \n",
              "15565                             0.0                   0.0   \n",
              "15149                             0.0                   0.0   \n",
              "14490                             0.0                   0.0   \n",
              "15490                             1.0                   0.0   \n",
              "14439                             0.0                   0.0   \n",
              "\n",
              "       RACE_ETHNICITY_Visa Non-U.S.  RACE_ETHNICITY_Two or More Races  \\\n",
              "9579                            0.0                               0.0   \n",
              "5611                            0.0                               0.0   \n",
              "9379                            0.0                               0.0   \n",
              "5237                            0.0                               0.0   \n",
              "15819                           1.0                               0.0   \n",
              "...                             ...                               ...   \n",
              "15565                           0.0                               0.0   \n",
              "15149                           0.0                               0.0   \n",
              "14490                           0.0                               0.0   \n",
              "15490                           0.0                               0.0   \n",
              "14439                           1.0                               0.0   \n",
              "\n",
              "       RACE_ETHNICITY_Other  RACE_ETHNICITY_Black or African American  \\\n",
              "9579                    0.0                                       0.0   \n",
              "5611                    0.0                                       0.0   \n",
              "9379                    0.0                                       0.0   \n",
              "5237                    0.0                                       0.0   \n",
              "15819                   0.0                                       0.0   \n",
              "...                     ...                                       ...   \n",
              "15565                   0.0                                       1.0   \n",
              "15149                   0.0                                       0.0   \n",
              "14490                   0.0                                       0.0   \n",
              "15490                   0.0                                       0.0   \n",
              "14439                   0.0                                       0.0   \n",
              "\n",
              "       GENDER_Female  GENDER_Male  FIRST_GEN_STATUS_Continuing Generation  \\\n",
              "9579             1.0          0.0                                     1.0   \n",
              "5611             0.0          1.0                                     1.0   \n",
              "9379             0.0          1.0                                     1.0   \n",
              "5237             1.0          0.0                                     1.0   \n",
              "15819            1.0          0.0                                     0.0   \n",
              "...              ...          ...                                     ...   \n",
              "15565            1.0          0.0                                     1.0   \n",
              "15149            1.0          0.0                                     1.0   \n",
              "14490            0.0          1.0                                     1.0   \n",
              "15490            0.0          1.0                                     1.0   \n",
              "14439            1.0          0.0                                     1.0   \n",
              "\n",
              "       FIRST_GEN_STATUS_Unknown  FIRST_GEN_STATUS_First Generation  \n",
              "9579                        0.0                                0.0  \n",
              "5611                        0.0                                0.0  \n",
              "9379                        0.0                                0.0  \n",
              "5237                        0.0                                0.0  \n",
              "15819                       1.0                                0.0  \n",
              "...                         ...                                ...  \n",
              "15565                       0.0                                0.0  \n",
              "15149                       0.0                                0.0  \n",
              "14490                       0.0                                0.0  \n",
              "15490                       0.0                                0.0  \n",
              "14439                       0.0                                0.0  \n",
              "\n",
              "[12104 rows x 20 columns]"
            ]
          },
          "execution_count": 242,
          "metadata": {}
        }
      ],
      "source": [
        "X_build_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c9f3930b-4abd-4d70-b1cf-31edd4ce93a2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sCXP2ya6gylU"
      },
      "source": [
        "We can see that all quantitative variables appear to fall well within the range between -5 and 5. Additionally, new one-hot encoded variables were created corresponding to the categories of each variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "790cec80-75a9-4c03-a0e2-075e9eb375bc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7L0bpquUgylU"
      },
      "source": [
        "Some models are sensitive to multicollinearity: when the features in our data are highly correlated with each other. This is a reason why it may be advisable to drop some of the variables before we proceed with the analysis. We use the next code chunk as an on/off switch, where we decide whether we'll use all features or a subset. Here we'll choose to remove deterministic linear dependence among the features by removing the \"Other\" indicator for RACE_ETHNICITY and the \"Unknown\" indicator for FIRST_GEN_STATUS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "89c029e6-2be5-46c3-95ae-0e8baf903c6d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9kzKGe5xgylU"
      },
      "outputs": [],
      "source": [
        "#Keep all features\n",
        "#final_column_names = all_column_names\n",
        "\n",
        "#Select a subset of features\n",
        "final_column_names = ['HS_ENGL_GPA','HS_MATH_GPA','GPA_1','UNITS_ATTEMPTED_1','DFW_RATE_1','UNITS_ATTEMPTED_2','GPA_2','DFW_RATE_2','RACE_ETHNICITY_White','RACE_ETHNICITY_Hispanic/Latino','RACE_ETHNICITY_Asian','RACE_ETHNICITY_Visa Non-U.S.','RACE_ETHNICITY_Two or More Races','RACE_ETHNICITY_Black or African American','GENDER_Female','GENDER_Male','FIRST_GEN_STATUS_Continuing Generation','FIRST_GEN_STATUS_First Generation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8c8ab6ec-8595-4714-baf0-a127724d5c60",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "gkMckJxrgylU"
      },
      "source": [
        "This provides us the DataFrames we'll be using for our classification problem: identifying students likely to depart the universty in term 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c93408cc-a78f-4b50-aab7-701ae0846ebe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HXl4-yLqgylU"
      },
      "outputs": [],
      "source": [
        "X_build_p1 = X_build_p[final_column_names] #Features for y_build_bal\n",
        "X_train_p1 = X_train_p[final_column_names] #Features for y_train_bal\n",
        "X_val_p1 = X_val_p[final_column_names]     #Features for y_val_c\n",
        "X_test_p1 = X_test_p[final_column_names]   #Features for y_test_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4e0af73e-8d03-46d1-be90-5e9b68755dd5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZItoXqT1gylU"
      },
      "source": [
        "![Class Guide](https://github.com/ksuaray/IRML---Regression-and-Classification/blob/MLCert-Sketches/MLCert%20Sketches%202/4DataSets_MLReady2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aa953c05-6e25-48a2-ac7b-1798b7c31172",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "so22UWhzgylV"
      },
      "source": [
        "Finally, we export the data in preparation for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a47d81b6-2090-485c-8568-be1027a5ae16",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Rov7gLt0gylV",
        "outputId": "a42e0ac5-1178-40ca-847b-d6adb289a799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9579     0\n",
              "5611     0\n",
              "9379     0\n",
              "5237     0\n",
              "15819    1\n",
              "        ..\n",
              "15565    1\n",
              "15149    1\n",
              "14490    1\n",
              "15490    1\n",
              "14439    1\n",
              "Name: SEM_2_STATUS, Length: 12104, dtype: int64"
            ]
          },
          "execution_count": 245,
          "metadata": {}
        }
      ],
      "source": [
        "#X_train_p1.to_csv('/Workspace/ira-ml-cert/data/Train_Features.csv', index=False)\n",
        "#y_train_bal.to_csv('/Workspace/ira-ml-cert/data/Train_Label.csv', index=False)\n",
        "\n",
        "#X_build_p1.to_csv('/Workspace/ira-ml-cert/data/Build_Features.csv', index=False)\n",
        "#y_build_bal.to_csv('/Workspace/ira-ml-cert/data/Build_Label.csv', index=False)\n",
        "\n",
        "#X_val_p1.to_csv('/Workspace/ira-ml-cert/data/Val_Features.csv', index=False)\n",
        "#y_val_c.to_csv('/Workspace/ira-ml-cert/data/Val_Label.csv', index=False)\n",
        "\n",
        "#X_test_p1.to_csv('/Workspace/ira-ml-cert/data/Test_Features.csv', index=False)\n",
        "#y_test_c.to_csv('/Workspace/ira-ml-cert/data/Test_Label.csv', index=False)\n",
        "y_build_bal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "41097002-ad82-4028-aa34-7241376af050",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jU9EkKphgylV"
      },
      "source": [
        "#4.4. Your Second Scikit-Learn Classification Model: Predict Dropout at 3rd Semester using L1-L2 Penalty (Elastic Net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0890f44a-489b-4ec9-87e6-bedfc9187c33",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_hB7q_VqgylV"
      },
      "source": [
        "##Analyzing and Learning from the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "44635c8a-e1d3-4a86-81c6-6185ce37c217",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "qcbzu6TmgylV"
      },
      "source": [
        "###Model Specification - The Logistic Regression with Elastic Net Classifier Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e42ecd0b-c526-4868-a1a5-9593156f7fd9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "kwLRYhdngylV"
      },
      "source": [
        "######Logistic Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dd9084f0-4e2a-42ef-b029-f262214b9735",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "jc47eia8gylV"
      },
      "source": [
        "As discussed in Module 4, Section X: Your First Baseline Statistical Model - Predict Dropout at 3rd Semester with Logistic Regression in Statsmodels, logistic regression takes a special case of the *softmax* in the form\n",
        "\n",
        "$$\\pi(x)=\\frac{e^x}{1+e^x}$$\n",
        "\n",
        "to predict probabilities corresponding to points in our feature space. Recall that this function takes on a *sigmoidal* or *S* shape. Below we visualize a sigmoidal graph in a hypothetical model for the relationship between DFW units and probability of dropout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83a7a3dd-2e38-4697-bc73-62574f147d9f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "03Ofxnm_gylV",
        "outputId": "9187d20d-b065-40c0-8646-83c46b2bfa1a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"92666ead-fe0f-483b-97cb-fc029e56c799\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"92666ead-fe0f-483b-97cb-fc029e56c799\")) {                    Plotly.newPlot(                        \"92666ead-fe0f-483b-97cb-fc029e56c799\",                        [{\"hovertemplate\":\"x=%{x}<br>y=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.0,0.1,0.2,0.30000000000000004,0.4,0.5,0.6000000000000001,0.7000000000000001,0.8,0.9,1.0,1.1,1.2000000000000002,1.3,1.4000000000000001,1.5,1.6,1.7000000000000002,1.8,1.9000000000000001,2.0,2.1,2.2,2.3000000000000003,2.4000000000000004,2.5,2.6,2.7,2.8000000000000003,2.9000000000000004,3.0,3.1,3.2,3.3000000000000003,3.4000000000000004,3.5,3.6,3.7,3.8000000000000003,3.9000000000000004,4.0,4.1000000000000005,4.2,4.3,4.4,4.5,4.6000000000000005,4.7,4.800000000000001,4.9,5.0,5.1000000000000005,5.2,5.300000000000001,5.4,5.5,5.6000000000000005,5.7,5.800000000000001,5.9,6.0,6.1000000000000005,6.2,6.300000000000001,6.4,6.5,6.6000000000000005,6.7,6.800000000000001,6.9,7.0,7.1000000000000005,7.2,7.300000000000001,7.4,7.5,7.6000000000000005,7.7,7.800000000000001,7.9,8.0,8.1,8.200000000000001,8.3,8.4,8.5,8.6,8.700000000000001,8.8,8.9,9.0,9.1,9.200000000000001,9.3,9.4,9.5,9.600000000000001,9.700000000000001,9.8,9.9,10.0,10.100000000000001,10.200000000000001,10.3,10.4,10.5,10.600000000000001,10.700000000000001,10.8,10.9,11.0,11.100000000000001,11.200000000000001,11.3,11.4,11.5,11.600000000000001,11.700000000000001,11.8,11.9,12.0,12.100000000000001,12.200000000000001,12.3,12.4,12.5,12.600000000000001,12.700000000000001,12.8,12.9,13.0,13.100000000000001,13.200000000000001,13.3,13.4,13.5,13.600000000000001,13.700000000000001,13.8,13.9,14.0,14.100000000000001,14.200000000000001,14.3,14.4,14.5,14.600000000000001,14.700000000000001,14.8,14.9,15.0,15.100000000000001,15.200000000000001,15.3,15.4,15.5,15.600000000000001,15.700000000000001,15.8,15.9,16.0,16.1,16.2,16.3,16.400000000000002,16.5,16.6,16.7,16.8,16.900000000000002,17.0,17.1,17.2,17.3,17.400000000000002,17.5,17.6,17.7,17.8,17.900000000000002,18.0,18.1,18.2,18.3,18.400000000000002,18.5,18.6,18.7,18.8,18.900000000000002,19.0,19.1,19.200000000000003,19.3,19.400000000000002,19.5,19.6,19.700000000000003,19.8,19.900000000000002],\"xaxis\":\"x\",\"y\":[0.004070137715896127,0.004299276419903346,0.004541256245830871,0.0047967900155682945,0.005066629333466254,0.005351566605225267,0.0056524371512370695,0.005970121417520966,0.006305547287335863,0.006659692496458429,0.007033587154995159,0.007428316378435995,0.007845023030455625,0.008284910579719272,0.00874924607264706,0.009239363223728833,0.009756665624551348,0.01030263007219559,0.010878810017073925,0.011486839129596605,0.012128434984274234,0.012805402858966817,0.013519639645969639,0.014273137870468841,0.015067989810592822,0.01590639171181471,0.01679064808681382,0.017723176090062263,0.018706509954354605,0.019743305474224586,0.020836344518680414,0.021988539552919663,0.023202938145644073,0.024482727435265486,0.025831238524664058,0.027251950770216075,0.028748495926539955,0.030324662103807975,0.031984397489533266,0.033731813781467224,0.03557118927263616,0.0375069715236202,0.03954377955095296,0.04168640545402418,0.043939815396141335,0.046309149848500354,0.048799722998800005,0.05141702121918566,0.05416670048123608,0.05705458259892043,0.06008665017400762,0.06326904011246003,0.06660803557509064,0.07011005622141818,0.07378164660246529,0.07762946255648838,0.08166025546159467,0.08588085420122658,0.09029814470291987,0.09491904691793245,0.09975048911968512,0.10479937941282438,0.11007257436250302,0.1155768446755336,0.12131883789173686,0.12730503807537166,0.13354172253321248,0.14003491562777273,0.14679033980138242,0.1538133639792068,0.16110894957658523,0.16868159439781957,0.1765352747791167,0.18467338639691716,0.19309868423321644,0.20181322226037882,0.21081829347774714,0.22011437099927417,0.22970105095339816,0.239576998011269,0.2497398944048824,0.260186393330312,0.2709120776506936,0.28191142481665027,0.29317777890643243,0.3047033306524346,0.3164791062636843,0.3284949657745943,0.34073961154861465,0.35320060744201437,0.3658644089891993,0.3787164048089321,0.3917409692534857,0.40492152613386606,0.4182406231581638,0.4316800165217519,0.4452207648927854,0.45884333184977205,0.4725276956554064,0.4862534650976451,0.5,0.5137465349023551,0.5274723043445939,0.5411566681502281,0.5547792351072147,0.5683199834782481,0.5817593768418364,0.5950784738661341,0.6082590307465144,0.6212835951910679,0.6341355910108007,0.6467993925579859,0.6592603884513856,0.671505034225406,0.6835208937363156,0.6952966693475654,0.7068222210935677,0.7180885751833499,0.7290879223493065,0.739813606669688,0.7502601055951176,0.7604230019887311,0.7702989490466019,0.779885629000726,0.7891817065222528,0.7981867777396212,0.8069013157667837,0.815326613603083,0.8234647252208833,0.8313184056021805,0.8388910504234148,0.8461866360207934,0.8532096601986178,0.8599650843722273,0.8664582774667876,0.8726949619246284,0.8786811621082631,0.8844231553244665,0.889927425637497,0.8952006205871756,0.9002495108803149,0.9050809530820676,0.9097018552970803,0.9141191457987734,0.9183397445384054,0.9223705374435116,0.9262183533975348,0.9298899437785819,0.9333919644249093,0.93673095988754,0.9399133498259924,0.9429454174010796,0.9458332995187639,0.9485829787808143,0.9512002770012,0.9536908501514997,0.9560601846038587,0.9583135945459759,0.9604562204490471,0.9624930284763799,0.9644288107273639,0.9662681862185328,0.9680156025104667,0.9696753378961921,0.9712515040734601,0.972748049229784,0.9741687614753359,0.9755172725647345,0.9767970618543559,0.9780114604470803,0.9791636554813196,0.9802566945257755,0.9812934900456454,0.9822768239099378,0.9832093519131861,0.9840936082881853,0.9849320101894072,0.9857268621295312,0.9864803603540304,0.9871945971410332,0.9878715650157258,0.9885131608704034,0.9891211899829261,0.9896973699278044,0.9902433343754486,0.9907606367762711,0.991250753927353,0.9917150894202807,0.9921549769695444,0.992571683621564,0.9929664128450049,0.9933403075035416,0.9936944527126641,0.994029878582479,0.9943475628487629,0.9946484333947747,0.9949333706665338,0.9952032099844317,0.9954587437541691,0.9957007235800966],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"DFW_UNITS\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Probability of Dropout\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"Probability of 3rd Term Dropout as a Function of DFW Units - Logistic Relationship\"}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "x = np.arange(0,20,0.1) #Variable representing DFW Units\n",
        "scale = 0.55\n",
        "shift = 10\n",
        "\n",
        "PROB_DROPOUT = np.exp(scale*(x-shift))/(1+np.exp(scale*(x-shift)))\n",
        "fig = px.line(x=x,y=PROB_DROPOUT)\n",
        "fig.update_layout(title = 'Probability of 3rd Term Dropout as a Function of DFW Units - Logistic Relationship',xaxis_title='DFW_UNITS', yaxis_title='Probability of Dropout')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "64fef9de-e808-4920-bc50-88bc97e08b15",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "51NVbnxUgylV"
      },
      "source": [
        "Once probabilities are generated, we define a classifier by assigning a given example to the 0 label if the probability is below a given, user determined threshold \\\\(c\\\\), and 1 if the probability is above that threshold.\n",
        "\n",
        "In the case where we incorporate more than one feature to predict this probability, the model is called *Multiple Logistic Regression*, and the two step process remains the same. Let's visualize in th following diagram:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "75e02966-a609-4d82-989a-d55086740154",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "enX6oOGJgylV"
      },
      "source": [
        "**FIGURE: Multiple Logistic Regression: a two step process**\n",
        "\n",
        "<img src=\"files/ml-file-store/bronze/logistic_flow.png\" width=\"800\" height=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1267ddb8-b314-495e-83c0-dfc97d8eb09f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "F6awwOu0gylV"
      },
      "source": [
        "As seen in the figure, the regression task correnponds to using the features of each observation to predict the probability of belonging to the class of interest, i.e. the one associated with the \"1\" label. In our case this is the event that a student is not retained in semester 3. We start by combining the values of the features for each individual in a linear way: $$\\vec{x}\\vec{\\beta} = \\beta_0+\\beta_1\\ x_{1i}+\\beta_2\\ x_{2i}+\\cdots+\\beta_{p-1}\\ x_{(p-1)i}$$. These \\\\(\\beta\\\\)'s are learned from our data and are called coefficients. Then probabilities are calculated by\n",
        "\n",
        "$$\\hat{\\pi}(\\vec{x}) = \\frac{\\exp(\\beta_0+\\beta_1\\ x_{1i}+\\beta_2\\ x_{2i}+\\cdots+\\beta_{p-1}\\ x_{(p-1)i})}{1+\\exp(\\beta_0+\\beta_1\\ x_{1i}+\\beta_2\\ x_{2i}+\\cdots+\\beta_{p-1}\\ x_{(p-1)i})}.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "440f62ed-bb3c-4c25-89f0-fefa182dc3ba",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "aGwYAW-ugylV"
      },
      "source": [
        "######The Elastic Net Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e94bc0ac-2dc8-4654-b8d2-fcdaa3d09a69",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HRGnW1BXgylV"
      },
      "source": [
        "The \\\\(\\beta_i\\\\) values are parameters that characterize the true relationship between student characteristics and probability of leaving campus. They cannot be known, and must be estimated from the data. The objective function for logistic regression is derived using the *maximum likelihood* principle from classical statistics. The *log likelihood* \\\\(l(\\vec{\\beta})\\\\) is a measurement of the probability we would randomly obtain our observed data. We choose values for the \\\\(\\beta_i\\\\)'s that *maximize* (i.e. make as large as possible) the probability of their observed class. Scikit Learn uses computational algorithms to identify the values of the \\\\(\\beta_i\\\\)'s that maximize this probability. It will be beneficial to consider an equivalent formulation, and frame this task as *minimizing* the objective function \\\\(-l(\\vec{\\beta})\\\\)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1746dc63-be72-493a-bb5c-85ab82357a3a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9XRhBOidgylW"
      },
      "source": [
        "While this can decrease bias on the training set, it often leads to high variance and parameter estimates that are unusually large in magnitude. This can be especially pronounced when the feature space is high dimensional. Moreover there is a need for meticulous investigation of multicollinearity and at the same time balancing model parsimony with predictive ability in the selection stage. An approach that addresses these challenges comprehensively is called *regularization*. In this approach we augment an existing algorithm in a direct attempt to strike the balance between under and over fitting. The approach is to modify the objective function \\\\(-l(\\vec{\\beta})\\\\) by imposing a penalty for inflated parameter estimates as follows:\n",
        "\n",
        "*Estimate the \\\\(\\beta_i\\\\)'s by using the data to select estimates \\\\(b_0,b_1,\\ldots,b_{p-1}\\\\) that minimize the penalized likelihood \\\\(D_E\\\\)*\n",
        "\n",
        "$$\n",
        "D_E = -l(\\vec{\\beta}) + \\lambda_1\\sum_{i=1}^{p-1} |\\beta_i | + \\lambda_2\\sum_{i=1}^{p-1} \\beta^2_i ,\n",
        "$$\n",
        "\n",
        "This arrangement leads to the *Elastic Net*.\n",
        "The first hyperparameter we will be tuning is known as the l1 ratio, expressed as\n",
        "$$\\alpha = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}$$\n",
        "\n",
        "The elastic net is the weighted average of two fundamental regularization strategies, obtained as follows:\n",
        "\n",
        "\\\\(\\alpha = 1\\\\) - Lasso or l1 penalty\n",
        "\n",
        "\\\\(\\alpha = 0\\\\) - Ridge or l2 penalty\n",
        "\n",
        "Don't worry too much about the technicalities. We just need to keep in mind that \\\\(\\alpha\\\\) is a value that calibrates the degree to which the lasso or ridge portion of the penalty is emphasized, and vice versa.\n",
        "\n",
        "The other aspect of regularization that needs to be specified is the strength, or amount that we apply. This is characterized by the variable \\\\(C\\\\), the *inverse regularization strength*. \\\\(C\\\\) is a nonnegative *hyperparameter* that regulates the degree to which the algorithm reins in the values of \\\\(\\beta\\\\). Smaller values of \\\\(C\\\\) create a stronger penalty and lead to smaller values of the \\\\(\\beta_i\\\\)'s. This method tends to group correlated features together to make the choice whether to include or exclude them, effectively producing an automatic variable selection procedure. This characteristic is most beneficial for \"wide\" datasets with more features with observations (such as biological data where genes serve as features).\n",
        "\n",
        "The choice of \\\\(C\\\\), then, is yet another instance of bias-variance tradeoff. Selecting \\\\(C\\\\) small increases bias but reduces variance, whereas large values of \\\\(C\\\\) increase variance and reduce bias. So how do we fit the model if we don't know the best choice for \\\\(C\\\\) before we use the \\\\(D_E\\\\) criterion? The answer, of course is that we can't, and this occurs because \\\\(C\\\\), and \\\\(\\alpha\\\\) for that matter are hyperparameters for the Elastic net model. Hyperparameters are values that can be tuned to optimize machine learning algorithms, but cannot be estimated directly from the data.  The model validation step will enable us to identify an estimates for \\\\(C\\\\) and \\\\(\\alpha\\\\) that will work well under the circumstances for prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "47ee0b7e-2b9a-4c3a-a76d-d025a21eb4d9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "DERAL55dgylW"
      },
      "source": [
        "**Table 1:** Role of \\\\(C\\\\) in Elastic Net Classificaton.\n",
        "\n",
        "| \\\\(C\\\\) too Large - Overfitting | Optimal \\\\(C\\\\) | \\\\(C\\\\) too Large - Underfitting |\n",
        "|---|---|---|\n",
        "|Low Bias, High Variance | Low Bias, Low Variance | High Bias, Low Variance |\n",
        "| Large Train \\\\(Acc\\\\) < Small Test \\\\(Acc\\\\) | Large Train \\\\(Acc \\approxeq \\\\) Large Test \\\\(Acc\\\\) | Small Train \\\\(Acc\\\\), Small Test \\\\(Acc\\\\) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "99a54788-4e7d-4b9d-a7c0-3553de0c1747",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nwkDZtcxgylW"
      },
      "source": [
        "######Criteria for Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1776ded1-5cc0-487b-aac4-f4a70a5b6599",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "IjCIXsE2gylW"
      },
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "The *confusion matrix* cross-tabulates the classes predicted by our model versus the actual classes. The four entries correspond to\n",
        "- True Negatives: Observations from the 0 class that we classified as 0's\n",
        "- False Negatives: Observations from the 1 class that we classified as 0's\n",
        "- False Positives: Observations from the 0 class that we classified as 1's\n",
        "- True Positives: Observations from the 1 class that we classified as 1's\n",
        "\n",
        "A perfect classifier has 0 values in the top right (FP=0) and bottom left (FN=0).\n",
        "\n",
        "There are many other classification metrics that may be investigated in scikit learn (see [here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)). Most can be understood in the framework of the confusion matrix, we discuss some below.\n",
        "\n",
        "<img src=\"files/ml-file-store/bronze/conf_matrx.png\" width=\"800\" height=\"500\">\n",
        "\n",
        "**Accuracy and AUC**\n",
        "\n",
        "*Accuracy* represents the proportion of all classifications that were done correctly.\n",
        "\n",
        "$$\n",
        "Acc = \\frac{TP+TN}{TP+TN+FP+FN}\n",
        "$$\n",
        "\n",
        "*AUC* refers to *Area Under the Curve*, and will be discussed later.\n",
        "\n",
        "*Example: In our context, Acc represents the proportion of students we correctly classified as either being retained in term 3 or not.*\n",
        "\n",
        "**Precision**\n",
        "\n",
        "*Precision* represents the proportion of all observations clasified as 1's that were truly 1's.\n",
        "\n",
        "$$\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "$$\n",
        "\n",
        "*Example: In our context, Precision represents the proportion of students we classified as being retained in term 3 that actually were.*\n",
        "\n",
        "**Recall**\n",
        "\n",
        "*Recall* represents the proportion of all observations that are actually 1's that were classified as 1's.\n",
        "\n",
        "$$\n",
        "Recall = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "\n",
        "*Example: In our context, Recall represents the proportion of students that were actually retained in term 3 that we classify as being retained.*\n",
        "\n",
        "**Sensitivity and Specificity**\n",
        "\n",
        "These are terms used heavily in medical literature and directly applicable to machine learning classification.\n",
        "\n",
        "*Sensitivity* is equivalent to recall.\n",
        "$$\n",
        "Sensitivity = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "\n",
        "*Specificity* represents the proportion of all observations that are actually 0's that were classified as 0's.\n",
        "$$\n",
        "Specificity = \\frac{TN}{TN+FP}\n",
        "$$\n",
        "\n",
        "*Example: In our context, Specificity represents the proportion of students that were actually not retained in term 3 that we classify as being not retained.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83033bd1-92d0-4d49-af12-f5bd7c56057e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GxDRy88zyRMK"
      },
      "source": [
        "###Model Fitting - Learning parameters and learning from parameter estimates (training set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2c2b5b26-97a3-4d28-9e5a-f673dbcae6a1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nZXcSWPWgylW"
      },
      "source": [
        "First load the required modules for implementation of the algorithm and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e3a856c0-3501-4591-ade0-d62a6c3f4c42",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YJwnlMhdgylW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "65e818c7-a76a-4f71-9917-e372aa52f737",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5rrTOosJgylW"
      },
      "source": [
        "Our approach will be to create a pipeline that takes in a Pandas dataframe with all variables in their native format (including features we have created from existing ones), runs them through feature engineering, and outputs a LogisticRegression object that can be used to make predictions and evaluate model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "49e82e27-a663-4de9-829c-e08ce33f44e4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hEYX2G7LgylW"
      },
      "source": [
        "Let's create a pipeline that transforms the original features by standardizing quantitative and one hot encoding qualitative ones, and the applies the logistic regression model with elastic net penalty:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bc45c77f-6e74-4114-96b4-e910b68d2533",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-LhG4ZOAgylW",
        "outputId": "8494e464-e035-4bda-aeae-a3124f62d6d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                   &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                   &#x27;DFW_RATE_1&#x27;,\n",
              "                                                   &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;,\n",
              "                                                   &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                                            array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                                            array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                                  [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                                   &#x27;FIRST_GEN_STATUS&#x27;])])),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 LogisticRegression(l1_ratio=0.5, max_iter=1000,\n",
              "                                    penalty=&#x27;elasticnet&#x27;, random_state=33,\n",
              "                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                   &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                   &#x27;DFW_RATE_1&#x27;,\n",
              "                                                   &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;,\n",
              "                                                   &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                                            array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                                            array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                                  [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                                   &#x27;FIRST_GEN_STATUS&#x27;])])),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 LogisticRegression(l1_ratio=0.5, max_iter=1000,\n",
              "                                    penalty=&#x27;elasticnet&#x27;, random_state=33,\n",
              "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                           array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                           array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                 [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                  &#x27;FIRST_GEN_STATUS&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;, &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;, &#x27;FIRST_GEN_STATUS&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                          array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                          array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)])</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
              "                   random_state=33, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
              "                                                  ['HS_ENGL_GPA', 'HS_MATH_GPA',\n",
              "                                                   'GPA_1', 'UNITS_ATTEMPTED_1',\n",
              "                                                   'DFW_RATE_1',\n",
              "                                                   'UNITS_ATTEMPTED_2', 'GPA_2',\n",
              "                                                   'DFW_RATE_2']),\n",
              "                                                 ('cat',\n",
              "                                                  OneHotEncoder(categories=[array(['White', 'Hispanic/Latino', 'Asian', 'Visa Non-U.S.',\n",
              "       'Two or More Races', 'Other', 'Black or African American'],\n",
              "      dtype=object),\n",
              "                                                                            array(['Female', 'Male'], dtype=object),\n",
              "                                                                            array(['Continuing Generation', 'Unknown', 'First Generation'],\n",
              "      dtype=object)]),\n",
              "                                                  ['RACE_ETHNICITY', 'GENDER',\n",
              "                                                   'FIRST_GEN_STATUS'])])),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(l1_ratio=0.5, max_iter=1000,\n",
              "                                    penalty='elasticnet', random_state=33,\n",
              "                                    solver='saga'))])"
            ]
          },
          "execution_count": 248,
          "metadata": {}
        }
      ],
      "source": [
        "pipeline_lrc1 = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(penalty ='elasticnet',l1_ratio=0.5, solver='saga',C=1.0,random_state=33,max_iter=1000))\n",
        "])\n",
        "#Note the default l1_ratio value is 'None', so we must select some value here since the elasticnet penalty is chosen. So we select the balanced penalty at r=0.5\n",
        "\n",
        "pipeline_lrc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4bb96360-098f-4524-9a53-a6bf1bfc092f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3H0QPixBgylW"
      },
      "source": [
        "Now that the pipeline has been created, let's put it to work. The *Fit-Predict-Score* cycle is the hub of the machine learning cycle.\n",
        "\n",
        "**Fit.**\n",
        "Learn from the training data: estimate the parameters of the model using the training data. At this stage, if there are hyperparameters in the model we'll just use sklearn's default values. Analogous to studying material given to us by the professor.\n",
        "\n",
        "**Predict.**\n",
        "Based on the parameter estimates obtained in the fit stage, predict the labels for observations in the training set. This is the \\\\(\\hat{y}_{Train}\\\\) vector. Analogous to taking an exam that is the exact same material we were given to study, verbatim.\n",
        "\n",
        "**Score.**\n",
        "Our predicted labels were generated from parameters that minimized the objective function. But there are often other criteria that more readily describe how well our model fits the training data (and test data later). These criteria are referred to as *scores*. In scikit learn, the default score for classification problems is *accuracy*. We discuss others below as well. Analogous to the grade we earned on the exam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4ec71076-e05f-42c6-af2f-a3ca6f98eb7b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nw646p4YgylW"
      },
      "outputs": [],
      "source": [
        "# Build_c.to_csv('/Workspace/Users/tyler.nakamura@csulb.edu/data/Build_c.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "54f01d22-bed1-47ce-a76e-1da32ac7d624",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "M4BmxFbogylW"
      },
      "outputs": [],
      "source": [
        "# y_build_bal.to_csv('/Workspace/Users/tyler.nakamura@csulb.edu/data/y_build_bal.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2e169952-2bee-4891-b9b7-03e6a0007423",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "hM3UW5WVgylX",
        "outputId": "05137781-e3e4-4fcf-dde3-a0a10b4bc066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9579</th>\n",
              "      <td>3.88</td>\n",
              "      <td>3.72</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5611</th>\n",
              "      <td>3.90</td>\n",
              "      <td>3.57</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9379</th>\n",
              "      <td>3.39</td>\n",
              "      <td>2.89</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>14.0</td>\n",
              "      <td>3.454545</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5237</th>\n",
              "      <td>3.93</td>\n",
              "      <td>3.63</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15819</th>\n",
              "      <td>3.84</td>\n",
              "      <td>4.26</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15565</th>\n",
              "      <td>3.15</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15149</th>\n",
              "      <td>4.23</td>\n",
              "      <td>4.05</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.250000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14490</th>\n",
              "      <td>3.60</td>\n",
              "      <td>3.13</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15490</th>\n",
              "      <td>3.88</td>\n",
              "      <td>3.24</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14439</th>\n",
              "      <td>4.12</td>\n",
              "      <td>3.64</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2.071429</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>Visa Non-U.S.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12104 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       HS_ENGL_GPA  HS_MATH_GPA     GPA_1  UNITS_ATTEMPTED_1  DFW_RATE_1  \\\n",
              "9579          3.88         3.72  3.800000               15.0    0.000000   \n",
              "5611          3.90         3.57  3.400000               15.0    0.000000   \n",
              "9379          3.39         2.89  2.625000               17.0    0.176471   \n",
              "5237          3.93         3.63  4.000000               16.0    0.000000   \n",
              "15819         3.84         4.26  4.000000               13.0    0.000000   \n",
              "...            ...          ...       ...                ...         ...   \n",
              "15565         3.15         3.02  0.166667               12.0    1.000000   \n",
              "15149         4.23         4.05  1.800000               15.0    0.200000   \n",
              "14490         3.60         3.13  3.666667               13.0    0.000000   \n",
              "15490         3.88         3.24  3.000000               16.0    0.000000   \n",
              "14439         4.12         3.64  4.000000               15.0    0.000000   \n",
              "\n",
              "       UNITS_ATTEMPTED_2     GPA_2  DFW_RATE_2             RACE_ETHNICITY  \\\n",
              "9579                15.0  3.200000    0.000000                      White   \n",
              "5611                12.0  4.000000    0.000000            Hispanic/Latino   \n",
              "9379                14.0  3.454545    0.214286                      Asian   \n",
              "5237                16.0  2.937500    0.000000            Hispanic/Latino   \n",
              "15819               14.0  2.500000    0.000000              Visa Non-U.S.   \n",
              "...                  ...       ...         ...                        ...   \n",
              "15565               16.0  0.333333    1.000000  Black or African American   \n",
              "15149               15.0  2.250000    0.200000                      White   \n",
              "14490               16.0  3.250000    0.187500                      White   \n",
              "15490               12.0  2.750000    0.000000            Hispanic/Latino   \n",
              "14439               14.0  2.071429    0.357143              Visa Non-U.S.   \n",
              "\n",
              "       GENDER       FIRST_GEN_STATUS  \n",
              "9579   Female  Continuing Generation  \n",
              "5611     Male  Continuing Generation  \n",
              "9379     Male  Continuing Generation  \n",
              "5237   Female  Continuing Generation  \n",
              "15819  Female                Unknown  \n",
              "...       ...                    ...  \n",
              "15565  Female  Continuing Generation  \n",
              "15149  Female  Continuing Generation  \n",
              "14490    Male  Continuing Generation  \n",
              "15490    Male  Continuing Generation  \n",
              "14439  Female  Continuing Generation  \n",
              "\n",
              "[12104 rows x 11 columns]"
            ]
          },
          "execution_count": 251,
          "metadata": {}
        }
      ],
      "source": [
        "Build_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9ad3c972-697a-457e-96fb-7c72a8832d9a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Zrfr2gsCgylX",
        "outputId": "754b37b9-e3a2-4158-b644-1e6a0cca2ad6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9579     0\n",
              "5611     0\n",
              "9379     0\n",
              "5237     0\n",
              "15819    1\n",
              "        ..\n",
              "15565    1\n",
              "15149    1\n",
              "14490    1\n",
              "15490    1\n",
              "14439    1\n",
              "Name: SEM_2_STATUS, Length: 12104, dtype: int64"
            ]
          },
          "execution_count": 252,
          "metadata": {}
        }
      ],
      "source": [
        "y_build_bal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5f384d6a-fab0-46df-b1d5-c2cedc465a60",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "kPljcYc2gylX",
        "outputId": "b4dab6a4-798b-4a29-bb55-50ff4bf14e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7728849966953073\n"
          ]
        }
      ],
      "source": [
        "#Fit-Predict-Score: Training to training\n",
        "\n",
        "pipeline_lrc1.fit(Build_c, y_build_bal) #Fit\n",
        "predictions_lrc1 = pipeline_lrc1.predict(Build_c) #Predict\n",
        "score_lrc1 = pipeline_lrc1.score(Build_c,y_build_bal) #Score\n",
        "\n",
        "#Training accuracy\n",
        "print(score_lrc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0743d68b-8854-4413-b7bf-d69276ecef70",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "orOcivWogylX"
      },
      "source": [
        "This gives us overall performance of prediction of both students groups: the number of retained students that were classified that way added to the number who weren't retained, and classified as such. It will be helpful to dig deeper. Let's dissect this performance by creating the *confusion matrix*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0103c716-01da-40b4-851c-ede2c26e1a33",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "eKfszEpZgylX",
        "outputId": "c08a6b9c-73d1-4a36-f192-c2d6f20474fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7efd3957ec20>"
            ]
          },
          "execution_count": 254,
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "cm_lrc1 = confusion_matrix(y_build_bal, predictions_lrc1, labels=None, sample_weight=None, normalize=None)\n",
        "disp = ConfusionMatrixDisplay(cm_lrc1, display_labels=None)\n",
        "disp.plot()\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c69b4050-7d4c-41d4-9c9a-1e50894d2318",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "U32HTxhvgylX"
      },
      "source": [
        "We can thus calculate the accuracy at about 77%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4de6c2b7-a6b3-448a-baf7-d00dcedbaa7a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "WvyuArX9gylX",
        "outputId": "0a6dd5c1-7d3b-4ae3-e5f8-370d8d682e7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7728849966953073"
            ]
          },
          "execution_count": 255,
          "metadata": {}
        }
      ],
      "source": [
        "cm = cm_lrc1\n",
        "\n",
        "acc_lrc1 = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0])\n",
        "\n",
        "acc_lrc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a409c8b0-b20d-4095-9bb0-f10af9bb6fce",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "wjWL_YBKgylX"
      },
      "source": [
        "The confusion matrix reveals that our ML algorithm seems to have done a fair job of addressing one of the most important challenges in higher ed. If we slice it\n",
        "\n",
        "- vertically: we see that about 81% of students predicted to leave actually did\n",
        "- horizontally: 69% of students that actually left the university were predicted to leave."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1874a4e5-c753-4fce-89b0-c7df264de0ca",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "N3QLdBGMgylX"
      },
      "source": [
        "While this seems promising for an essentially fresh out the box model, we have to remember that the approximately 77% accurate predictions were made on the *training set*, i.e. the data used to build the model. In our running human learning analogy, all we've done is **memorize** a set of facts and definitions, and regurgitate them on a multiple choice exam that consisted of the precisely the same material we studied.  Unfortunately that is not indicative of whether we would be able to write an essay on the material we studied, or apply concepts learned to a scenario not discussed in class. To do that we'd need to examine our acuracy on an unseen test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0a05a2d6-85e0-4714-b966-28890bd17ff0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "T36l1w9LgylX"
      },
      "source": [
        "#VIDEO 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee5a670d-3d9a-4329-ac25-b71eff350b10",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Ng5txqergylX"
      },
      "source": [
        "###Model Validation and Selection - Hyperparameter tuning and model generalizability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ce90d32a-2f6d-48c8-8f60-93529d7f3873",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JudnBwazgylY"
      },
      "source": [
        "A model's performance on the training set is an initial indication of how well parameter choices do at minimizing the objective function. This is an important characteristic we build in to the modeling process. However, if our ultimate aim is deployment to new data, then it is mandatory that model choice be informed by applying the prediction and scoring steps of the *Fit-Predict-Score* cycle to unseen data.\n",
        "\n",
        "As discussed in *Explaining Machine Learning Cycle With Hyperparameter Tuning* in Module 4, *k-Fold Cross Validation* is a powerful process we can apply to our data and model to simulate performance on unseen data. Figure 3 gives a visual representation on our example training data. To perform 6-fold cross validation, we randomly partition the training data into six mutually exclusive samples of size 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "00eac977-9727-4a09-8bb6-a81df31a8932",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "aP8RoIHPgylY"
      },
      "source": [
        "**Figure 3:** An example of 6-fold cross validation on a the training set of a dataframe with 15 observations.\n",
        "![ih](files/ml-file-store/bronze/k_fold_pic.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f19bce49-2f53-4763-896f-626431d54fdf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "eZCpZ6bRgylY"
      },
      "source": [
        "These six mutually exclusive samples of size 2 refered to above will serve as test sets (referred to as *validation* sets in this context) for 6 independent batches of *Fit-Predict-Score* cycles. The corresponding training sets (referred to as *building* sets in this context) are the remaining 10 observations not selected for the validation set. Each batch is defined by a full run through the designated hyperparameter search space. To execute this process we first import the *cross_val_score* method from the *model_selection* module in scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1fd18162-b47f-43e1-b3a3-a4c1bd24d8eb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mLTJmgsNgylY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "884e0596-0117-45e0-ba03-31368f299ed8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Youwe4FfgylY"
      },
      "source": [
        "######Step 1: Identify Hyperparameter Search Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "20538080-f7e8-4072-9a21-75c7d19beea7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "pQpP2ko2gylY"
      },
      "source": [
        "The \\\\(C\\\\) hyperparameter controls the amount of regularization implemented. The default value in sklearn is \\\\(C=1.0\\\\). Based on this value, the l1_ratio, which we denote by \\\\(r\\\\) is selected. See below for a step-by-step approach on how to set the grid. Remember that this is both an art as well as a science, so there is no one right way to approach this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7369c0ac-b2b2-4d6c-8115-2439ffcb3364",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "75E_UwVUgylY"
      },
      "outputs": [],
      "source": [
        "hp_grid = {'classifier__C': np.arange(0.2, 0.4, 0.01),\n",
        "           'classifier__l1_ratio': np.arange(0,1,0.05)} # select a slice of the hyperparameter space informed by graphical observation. A dictionary is required for future operations, and we'll create a variable for the array by itself below\n",
        "\n",
        "C_values = hp_grid['classifier__C']\n",
        "r_values = hp_grid['classifier__l1_ratio']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6d63cf5d-766f-4239-8909-46a7c48b8594",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sxhz8ka9gylY"
      },
      "source": [
        "######Step 2: Specify Number of Folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fcbd718d-ba52-4082-92f2-679709a983d8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dyBWHioogylY"
      },
      "source": [
        "To what extent should we divvy up our data up for validation? Consider two extremes:\n",
        "\n",
        "\\\\(k=2\\\\)\n",
        "\n",
        "Set one validation set that is half of the training set.\n",
        " - Pro: Computationally efficient. Validation is more challenging, so a better simulation of unseen data.\n",
        " - Con: Only one Fit-Predict-Score cycle, minimal training experience.\n",
        "\n",
        "\\\\(k=n\\\\)\n",
        "\n",
        "Each observation becomes its own validation set, and we use the remaining data to fit the model. Also known as leave-one-out cross validation.\n",
        " - Pro: Maximizes number of Fit-Predict-Score cycles, providing a profile of each point\n",
        " - Con: Computationally expensive\n",
        "\n",
        "As we've become accustomed to, the ideal is in the balance. Typically we use \\\\(k\\\\) between \\\\(5\\\\) and \\\\(10\\\\)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b3d1806e-3050-4535-9066-e2a65fc9f11e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "qDXbCPpMgylY"
      },
      "outputs": [],
      "source": [
        "k = 5  # Selecting 5-fold cross-validation\n",
        "cv_data = StratifiedKFold(n_splits=k,shuffle=True,random_state=rms) #creating the 5 fold resampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "90b15723-c434-4877-9900-241aa840d867",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZoyBKNvygylY"
      },
      "source": [
        "######Step 3: *Fit-Predict-Score*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c57a8a79-e1b5-4c92-9bfe-af2e998257a8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "na69NV2HgylY"
      },
      "source": [
        "Implement the *Fit-Predict-Score* cycle to identify average score across the 5 folds for each point in the hyperparameter space. This provides the criterion we will use to select our estimates for \\\\(C\\\\) and \\\\(r\\\\) and fit our final model. The following figure and subsequent code illustrate and execute this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a80c1094-7b53-4fda-803e-7a5f29276763",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uuisWFQlgylY"
      },
      "source": [
        "**Figure 4:** An example of the Fit-Predict-Score cycle on one fold. This process is performed for each fold and averaged to produce one cross validation score per hyperparameter value.\n",
        "<img src=\"files/ml-file-store/bronze/FPS_elastic_kFold.png\" width=\"800\" height=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2944e28a-aa82-4d21-84a4-b55e3cf7ac26",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "31xvT2BkgylY"
      },
      "source": [
        "The **GridSearchCV** function in scikit learn will provide a powerful tool for a smooth implementation of the Fit-Predict-Score cycle in the framework of k-fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b195b059-3dc5-4428-9b82-c3ad054ef7ed",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GSYPHlgpgylY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2d457163-4068-4679-b1b8-9f0d391987a0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Vy95PoFlgylY"
      },
      "outputs": [],
      "source": [
        "#Specify the model\n",
        "model = pipeline_lrc1  #Our Elastic Net model with parameters selected to ensure convergence\n",
        "\n",
        "\n",
        "#Fit-Predict-Score: : Implementing the FPS cycle from build set to validation set\n",
        "\n",
        "classification_metric = 'accuracy'\n",
        "#Could also choose recall to maximize ability to predict students leaving\n",
        "\n",
        "# Create the grid search object\n",
        "grid = GridSearchCV(estimator = model,\n",
        "                    param_grid = hp_grid,\n",
        "                    cv=cv_data,\n",
        "                    scoring = classification_metric,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "# Use cross-validation to fit the model and get mean R^2 score over all folds\n",
        "grid.fit(Build_c,y_build_bal)\n",
        "\n",
        "metric_values = grid.cv_results_['mean_test_score']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "12db413d-b6d3-4d70-8618-c95d5c5a5ae0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "bqFrxdT-gylY"
      },
      "source": [
        "There are two hyperparameters (\\\\(C\\\\) and \\\\(r\\\\)) that characterize regularization in the elastic net, so our hyperparameter space is two dimensional. We can use a visualization known as a *heatmap* to search for optimal values for our classification metric. From that point, we'll look at the relationship between \\\\(C\\\\) and the classificatin metric for given values of the l1_ratio, \\\\(r\\\\).\n",
        "In this context, an interactive graph will be immensely useful. So we again utilize the **plotly** library, this time working with the *graph_objects* module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2b0433eb-f736-458e-9301-a2033c914506",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "J9FJhpr1gylZ"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ca04e1aa-97e8-4e51-b3af-e99f4bc48643",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-iUwwwGAgylZ"
      },
      "outputs": [],
      "source": [
        "#First reshape the long list of classification metric values into a rectangular array in which each row corresponds to a unique value of r\n",
        "mvd = metric_values.reshape(len(r_values),len(C_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "241dc490-5c46-426c-832b-e5066c38c4be",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JM7VtdBEgylZ",
        "outputId": "46b74817-416f-42da-f07f-17ba69ba1dd6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"c07fae6a-b90a-47ff-a2e4-04319bb36586\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c07fae6a-b90a-47ff-a2e4-04319bb36586\")) {                    Plotly.newPlot(                        \"c07fae6a-b90a-47ff-a2e4-04319bb36586\",                        [{\"hoverongaps\":false,\"x\":[0.2,0.21000000000000002,0.22000000000000003,0.23000000000000004,0.24000000000000005,0.25000000000000006,0.26000000000000006,0.2700000000000001,0.2800000000000001,0.2900000000000001,0.3000000000000001,0.3100000000000001,0.3200000000000001,0.3300000000000001,0.34000000000000014,0.35000000000000014,0.36000000000000015,0.37000000000000016,0.38000000000000017,0.3900000000000002],\"y\":[0.0,0.05,0.1,0.15000000000000002,0.2,0.25,0.30000000000000004,0.35000000000000003,0.4,0.45,0.5,0.55,0.6000000000000001,0.65,0.7000000000000001,0.75,0.8,0.8500000000000001,0.9,0.9500000000000001],\"z\":[[0.7742885427441021,0.7743711532356345,0.7742885427441021,0.7742885427441021,0.7741232876244705,0.7742885086075353,0.7741232876244705,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.7739580666414055,0.774040677132938,0.7737928456583407,0.7737102010302417,0.7737102010302417,0.7735449800471768,0.7733796907909785,0.7730491122785818,0.7731317227701141,0.7730491122785816],[0.7742059322525696,0.7743711532356345,0.7742059322525696,0.7741232876244705,0.7741232876244705,0.7742885086075353,0.7741232876244705,0.774040677132938,0.7742058981160029,0.7741232876244705,0.7738754561498731,0.7739580666414055,0.774040677132938,0.7736275905387092,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7733796907909785,0.7730491122785818,0.7730491122785816],[0.7742059322525696,0.7742885427441021,0.7742885427441021,0.7741232876244705,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7742058981160029,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.774040677132938,0.7740406429963713,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7730491122785818,0.7730491122785818],[0.7741233217610373,0.7742885427441021,0.7742885427441021,0.7742058981160029,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7738754561498731,0.7738754561498731,0.7739580325048389,0.7736275905387092,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7730491122785818],[0.7740407112695048,0.7742885427441021,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.774040677132938,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.774040677132938,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.7738754220133064,0.7738754220133064,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7732144015347802],[0.7740407112695048,0.7742059322525697,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.7739580666414057,0.7740406771329381,0.7742058981160029,0.7742058981160029,0.774040677132938,0.774040677132938,0.7741232876244705,0.7738754561498731,0.7737928115217739,0.7738754220133064,0.7738754220133064,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7742059322525697,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7741232876244705,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.7742058981160029,0.7737928115217739,0.7737928115217739,0.7737928115217739,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7741232876244706,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7737928115217739,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7736275905387092,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7740406771329381,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.774040677132938,0.774040677132938,0.774040677132938,0.7742058981160029,0.7737928115217739,0.7737928115217739,0.7737928115217739,0.7736275905387092,0.7736275905387092,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244705,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7741232534879037,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7735449800471768,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7742059322525696,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7739580325048389,0.7737928115217739,0.7737928115217739,0.7736275905387092,0.7736275905387092,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742059322525696,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.7739580325048389,0.7740406429963713,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7741233217610373,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.774040677132938,0.7739580325048389,0.7739580325048389,0.7737102010302415,0.7737928115217739,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7740407112695048,0.7740406771329381,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7739580325048389,0.7739580325048389,0.7740406429963713,0.7737102010302415,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7740407112695048,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7739580325048389,0.7739580325048389,0.7740406429963713,0.7736275905387092,0.7737102010302417],[0.7740407112695048,0.7740407112695048,0.7739580666414057,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7739580325048389,0.7740406429963713,0.7737102010302415,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7739580666414057,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7741232876244706,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7739580325048389,0.7740406429963713,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7738754561498732,0.7738754561498732,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7737928456583407,0.7737928456583407,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7739580325048389],[0.7740407112695048,0.7740407112695048,0.7737102351668084,0.7737102351668084,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7740406771329381,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#The heatmap is a visual representation of the rectangular array we created in the preceeding cell\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "                   z=mvd,\n",
        "                   x=C_values,\n",
        "                   y=r_values,\n",
        "                   hoverongaps = False))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3a557e13-c8d0-4af6-952a-7db214837903",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "auUS-vQBgylZ",
        "outputId": "74059124-67f0-4e90-da8f-5ec73da6afaf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"d189d311-5323-450c-a74e-8e0f96898d8e\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d189d311-5323-450c-a74e-8e0f96898d8e\")) {                    Plotly.newPlot(                        \"d189d311-5323-450c-a74e-8e0f96898d8e\",                        [{\"contours\":{\"x\":{\"color\":\"white\",\"end\":0.3900000000000002,\"show\":true,\"size\":0.8,\"start\":0.2},\"y\":{\"color\":\"white\",\"end\":0.9500000000000001,\"show\":true,\"size\":0.8,\"start\":0.0},\"z\":{\"end\":1,\"show\":true,\"size\":0.5,\"start\":0.6}},\"x\":[0.2,0.21000000000000002,0.22000000000000003,0.23000000000000004,0.24000000000000005,0.25000000000000006,0.26000000000000006,0.2700000000000001,0.2800000000000001,0.2900000000000001,0.3000000000000001,0.3100000000000001,0.3200000000000001,0.3300000000000001,0.34000000000000014,0.35000000000000014,0.36000000000000015,0.37000000000000016,0.38000000000000017,0.3900000000000002],\"y\":[0.0,0.05,0.1,0.15000000000000002,0.2,0.25,0.30000000000000004,0.35000000000000003,0.4,0.45,0.5,0.55,0.6000000000000001,0.65,0.7000000000000001,0.75,0.8,0.8500000000000001,0.9,0.9500000000000001],\"z\":[[0.7742885427441021,0.7743711532356345,0.7742885427441021,0.7742885427441021,0.7741232876244705,0.7742885086075353,0.7741232876244705,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.7739580666414055,0.774040677132938,0.7737928456583407,0.7737102010302417,0.7737102010302417,0.7735449800471768,0.7733796907909785,0.7730491122785818,0.7731317227701141,0.7730491122785816],[0.7742059322525696,0.7743711532356345,0.7742059322525696,0.7741232876244705,0.7741232876244705,0.7742885086075353,0.7741232876244705,0.774040677132938,0.7742058981160029,0.7741232876244705,0.7738754561498731,0.7739580666414055,0.774040677132938,0.7736275905387092,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7733796907909785,0.7730491122785818,0.7730491122785816],[0.7742059322525696,0.7742885427441021,0.7742885427441021,0.7741232876244705,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7742058981160029,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.774040677132938,0.7740406429963713,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7730491122785818,0.7730491122785818],[0.7741233217610373,0.7742885427441021,0.7742885427441021,0.7742058981160029,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7738754561498731,0.7738754561498731,0.7739580325048389,0.7736275905387092,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7730491122785818],[0.7740407112695048,0.7742885427441021,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.774040677132938,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.774040677132938,0.7741232876244705,0.7742058981160029,0.7738754561498731,0.7738754220133064,0.7738754220133064,0.7737102010302417,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7732144015347802],[0.7740407112695048,0.7742059322525697,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.7739580666414057,0.7740406771329381,0.7742058981160029,0.7742058981160029,0.774040677132938,0.774040677132938,0.7741232876244705,0.7738754561498731,0.7737928115217739,0.7738754220133064,0.7738754220133064,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7742059322525697,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7741232876244705,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.7742058981160029,0.7737928115217739,0.7737928115217739,0.7737928115217739,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7741232876244706,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7737928115217739,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7736275905387092,0.7735449800471768,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7740406771329381,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.774040677132938,0.774040677132938,0.774040677132938,0.7742058981160029,0.7737928115217739,0.7737928115217739,0.7737928115217739,0.7736275905387092,0.7736275905387092,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742885427441021,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244705,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7741232534879037,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7735449800471768,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7742059322525696,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.774040677132938,0.7739580325048389,0.7737928115217739,0.7737928115217739,0.7736275905387092,0.7736275905387092,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7742059322525696,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7742058981160029,0.774040677132938,0.774040677132938,0.774040677132938,0.7739580325048389,0.7740406429963713,0.7737928115217739,0.7738754220133064,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7741233217610373,0.7741232876244706,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.774040677132938,0.7739580325048389,0.7739580325048389,0.7737102010302415,0.7737928115217739,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7740407112695048,0.7740406771329381,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7739580325048389,0.7739580325048389,0.7740406429963713,0.7737102010302415,0.7736275905387092,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7740407112695048,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7739580666414057,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7739580325048389,0.7739580325048389,0.7740406429963713,0.7736275905387092,0.7737102010302417],[0.7740407112695048,0.7740407112695048,0.7739580666414057,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7739580325048389,0.7740406429963713,0.7737102010302415,0.7736275905387092],[0.7740407112695048,0.7740407112695048,0.7739580666414057,0.7738754561498732,0.7739580666414057,0.7739580666414057,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7741232876244706,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7739580325048389,0.7740406429963713,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7738754561498732,0.7738754561498732,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7741232876244705,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7735449800471768],[0.7740407112695048,0.7740407112695048,0.7737928456583407,0.7737928456583407,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7742058981160029,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7739580325048389],[0.7740407112695048,0.7740407112695048,0.7737102351668084,0.7737102351668084,0.7738754561498732,0.7739580666414057,0.7740406771329381,0.7740406771329381,0.7741232876244706,0.7741232876244706,0.7740406771329381,0.7740406771329381,0.7740406771329381,0.7741232876244705,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713,0.7740406429963713]],\"type\":\"surface\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"xaxis\":{\"nticks\":20},\"zaxis\":{\"nticks\":4},\"camera\":{\"eye\":{\"x\":0,\"y\":-1,\"z\":0.5}},\"aspectratio\":{\"x\":1,\"y\":1,\"z\":0.2}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = go.Figure(go.Surface(\n",
        "    contours = {\n",
        "        \"x\": {\"show\": True, \"start\": min(C_values), \"end\": max(C_values), \"size\": 0.8, \"color\":\"white\"},\n",
        "        \"y\": {\"show\": True, \"start\": min(r_values), \"end\": max(r_values), \"size\": 0.8, \"color\":\"white\"},\n",
        "        \"z\": {\"show\": True, \"start\": 0.6, \"end\": 1, \"size\": 0.5}\n",
        "    },\n",
        "    x = C_values,\n",
        "    y = r_values,\n",
        "    z = mvd\n",
        "    ))\n",
        "fig.update_layout(\n",
        "        scene = {\n",
        "            \"xaxis\": {\"nticks\": 20},\n",
        "            \"zaxis\": {\"nticks\": 4},\n",
        "            'camera_eye': {\"x\": 0, \"y\": -1, \"z\": 0.5},\n",
        "            \"aspectratio\": {\"x\": 1, \"y\": 1, \"z\": 0.2}\n",
        "        })\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "76567194-8990-4610-b6be-cc5e0c767fb7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "axG03UmEgylZ"
      },
      "source": [
        "For our initial implementation, we selected a broad swathe of values for the \\\\(C\\\\) and \\\\(r\\\\) hyperparameters. We will graph the relationship between \\\\(C\\\\) and our metric, and use the insight gained from it to identify a more selective subset for \\\\(C\\\\) to investigate more critically. As far as the values of \\\\(C\\\\) and \\\\(r\\\\) to select in the first place, one approach is the folowing:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6b8a8c03-ea98-4e8d-bce2-18aa4090aeb4",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "UnZgTkW1gylZ"
      },
      "source": [
        "**Tips for investigating the hyperparameter space**\n",
        "\n",
        "1. Consider the \"out-of-the-box\" fitted model. If it is in some sense \"good\" (e.g. performance exceeds baseline model) then explore hyperparameter values that are in close proximity to the scikit learn defaults. If the model is not \"good\", select a wider range of values away from the defaults.\n",
        "\n",
        "2. Plot a heatmap visualizing the hyperparameter values on the x and y axes as suggested by step 1. Where applicable, let the x-axis represent the parameter most directly responsible for regularization. Your metric values should be plotted on the z-axis.\n",
        "\n",
        "3. Identify a horizontal slice of the heatmap with the largest metric, and visualize the two dimensional relationship between the hyperparameter represented on the x-axis and the metric.\n",
        "\n",
        "4. Keep in mind the hyperparameter space may be much wider than the search grid you have explored. Understanding the underlying model will inform to what extent you need to search the entire space once trends have been determined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9774e556-188f-4b5d-9cf7-d7e0e33d304b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "juh29x_OgylZ",
        "outputId": "d9caf96c-f46f-4a93-d21e-11b6592a10fc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
              "       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#To make the horizontal slice, view the heatmap in light of the values we used for r. Set the value or r we select in the r_val variable at the top of the next cell\n",
        "display(r_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ae181001-f5a7-41b9-ac79-87164fbdcb3f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mNfuCfGxgylZ",
        "outputId": "976f6ef6-3917-4b9c-c5a0-d5e118a7a216"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"b93f9a7a-b81a-4bae-b6a1-4baf0e975086\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b93f9a7a-b81a-4bae-b6a1-4baf0e975086\")) {                    Plotly.newPlot(                        \"b93f9a7a-b81a-4bae-b6a1-4baf0e975086\",                        [{\"x\":[0.2,0.21000000000000002,0.22000000000000003,0.23000000000000004,0.24000000000000005,0.25000000000000006,0.26000000000000006,0.2700000000000001,0.2800000000000001,0.2900000000000001,0.3000000000000001,0.3100000000000001,0.3200000000000001,0.3300000000000001,0.34000000000000014,0.35000000000000014,0.36000000000000015,0.37000000000000016,0.38000000000000017,0.3900000000000002],\"y\":[0.7740407112695048,0.7742059322525697,0.7742885427441021,0.7742058981160029,0.7741232876244706,0.7739580666414057,0.7740406771329381,0.7742058981160029,0.7742058981160029,0.774040677132938,0.774040677132938,0.7741232876244705,0.7738754561498731,0.7737928115217739,0.7738754220133064,0.7738754220133064,0.7735449800471768,0.7735449800471768,0.7735449800471768,0.7735449800471768],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"title\":{\"text\":\"C\"},\"tickangle\":-45},\"title\":{\"text\":\"Average Cross-Validation Score vs. regularization strength C for Elastic Net Classification\"},\"yaxis\":{\"title\":{\"text\":\"Average CV Score\"}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "r_val = 0.25\n",
        "\n",
        "r_ind = np.where(r_values==r_val)[0][0]\n",
        "\n",
        "#Plot and connect the dots with line segments\n",
        "fig = go.Figure(data=go.Scatter(x=C_values, y=mvd[r_ind,]))\n",
        "\n",
        "# Set plot title and axis labels\n",
        "fig.update_layout(title='Average Cross-Validation Score vs. regularization strength C for Elastic Net Classification',\n",
        "                  xaxis_title='C',\n",
        "                  yaxis_title='Average CV Score',\n",
        "                  xaxis=dict(tickangle=-45))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "31b11b29-1b18-48c0-acda-995bee6388a9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RMMcxcxkgylZ"
      },
      "source": [
        "We can see that the graph reaches a peak somewhere between 0.30 and 0.35. Let's zoom our grid in and look for the optimal values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "96f96346-40fb-48aa-9cd0-12d674e1d452",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fVpTQt_pgylZ",
        "outputId": "2c8fd472-d03c-42c3-88d9-76a6b3a0f9d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                        ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
              "                                                                         StandardScaler(),\n",
              "                                                                         [&#x27;HS_ENGL_GPA&#x27;,\n",
              "                                                                          &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                                          &#x27;GPA_1&#x27;,\n",
              "                                                                          &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                                          &#x27;DFW_RATE_1&#x27;,\n",
              "                                                                          &#x27;UNITS_ATTEMPTED_2&#x27;,\n",
              "                                                                          &#x27;GPA_2&#x27;,\n",
              "                                                                          &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                                        (&#x27;cat&#x27;,\n",
              "                                                                         OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black o...\n",
              "                                        LogisticRegression(l1_ratio=0.5,\n",
              "                                                           max_iter=1000,\n",
              "                                                           penalty=&#x27;elasticnet&#x27;,\n",
              "                                                           random_state=33,\n",
              "                                                           solver=&#x27;saga&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;classifier__C&#x27;: array([0.2  , 0.205, 0.21 , 0.215, 0.22 , 0.225, 0.23 , 0.235, 0.24 ,\n",
              "       0.245, 0.25 , 0.255, 0.26 , 0.265, 0.27 , 0.275, 0.28 , 0.285,\n",
              "       0.29 , 0.295]),\n",
              "                         &#x27;classifier__l1_ratio&#x27;: array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55])},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                                        ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
              "                                                                         StandardScaler(),\n",
              "                                                                         [&#x27;HS_ENGL_GPA&#x27;,\n",
              "                                                                          &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                                          &#x27;GPA_1&#x27;,\n",
              "                                                                          &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                                          &#x27;DFW_RATE_1&#x27;,\n",
              "                                                                          &#x27;UNITS_ATTEMPTED_2&#x27;,\n",
              "                                                                          &#x27;GPA_2&#x27;,\n",
              "                                                                          &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                                        (&#x27;cat&#x27;,\n",
              "                                                                         OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black o...\n",
              "                                        LogisticRegression(l1_ratio=0.5,\n",
              "                                                           max_iter=1000,\n",
              "                                                           penalty=&#x27;elasticnet&#x27;,\n",
              "                                                           random_state=33,\n",
              "                                                           solver=&#x27;saga&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;classifier__C&#x27;: array([0.2  , 0.205, 0.21 , 0.215, 0.22 , 0.225, 0.23 , 0.235, 0.24 ,\n",
              "       0.245, 0.25 , 0.255, 0.26 , 0.265, 0.27 , 0.275, 0.28 , 0.285,\n",
              "       0.29 , 0.295]),\n",
              "                         &#x27;classifier__l1_ratio&#x27;: array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55])},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                   &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                   &#x27;DFW_RATE_1&#x27;,\n",
              "                                                   &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;,\n",
              "                                                   &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                                            array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                                            array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                                  [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                                   &#x27;FIRST_GEN_STATUS&#x27;])])),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 LogisticRegression(l1_ratio=0.5, max_iter=1000,\n",
              "                                    penalty=&#x27;elasticnet&#x27;, random_state=33,\n",
              "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                           array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                           array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                 [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                  &#x27;FIRST_GEN_STATUS&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;, &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;, &#x27;FIRST_GEN_STATUS&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                          array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                          array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)])</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, penalty=&#x27;elasticnet&#x27;,\n",
              "                   random_state=33, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('preprocessor',\n",
              "                                        ColumnTransformer(transformers=[('num',\n",
              "                                                                         StandardScaler(),\n",
              "                                                                         ['HS_ENGL_GPA',\n",
              "                                                                          'HS_MATH_GPA',\n",
              "                                                                          'GPA_1',\n",
              "                                                                          'UNITS_ATTEMPTED_1',\n",
              "                                                                          'DFW_RATE_1',\n",
              "                                                                          'UNITS_ATTEMPTED_2',\n",
              "                                                                          'GPA_2',\n",
              "                                                                          'DFW_RATE_2']),\n",
              "                                                                        ('cat',\n",
              "                                                                         OneHotEncoder(categories=[array(['White', 'Hispanic/Latino', 'Asian', 'Visa Non-U.S.',\n",
              "       'Two or More Races', 'Other', 'Black o...\n",
              "                                        LogisticRegression(l1_ratio=0.5,\n",
              "                                                           max_iter=1000,\n",
              "                                                           penalty='elasticnet',\n",
              "                                                           random_state=33,\n",
              "                                                           solver='saga'))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={'classifier__C': array([0.2  , 0.205, 0.21 , 0.215, 0.22 , 0.225, 0.23 , 0.235, 0.24 ,\n",
              "       0.245, 0.25 , 0.255, 0.26 , 0.265, 0.27 , 0.275, 0.28 , 0.285,\n",
              "       0.29 , 0.295]),\n",
              "                         'classifier__l1_ratio': array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55])},\n",
              "             scoring='accuracy')"
            ]
          },
          "execution_count": 267,
          "metadata": {}
        }
      ],
      "source": [
        "hp_grid_1 = {'classifier__C': np.arange(0.20, 0.30, 0.005),\n",
        "             'classifier__l1_ratio': np.arange(0.1,0.6,0.05)\n",
        "             } # A more selective slice of the hyperparameter space informed by graphical observation\n",
        "\n",
        "metric = classification_metric\n",
        "\n",
        "cv_folds = 5\n",
        "\n",
        "# Create the grid search object\n",
        "grid_1 = GridSearchCV(estimator = pipeline_lrc1,\n",
        "                    param_grid = hp_grid_1,\n",
        "                    cv=cv_folds,\n",
        "                    scoring = metric,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "# Use cross-validation to fit the model and get mean R^2 score over all folds\n",
        "grid_1.fit(Build_c,y_build_bal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bc8583cb-47b1-41a5-9888-54d552155494",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rlkHQWJZgylZ"
      },
      "source": [
        "We can identify the optimal score, parameter and model as attributes of the *grid_1* object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "03b043be-b9b4-4e7f-a936-3c966dd4f89d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uKf4-iJ2gylZ",
        "outputId": "c30f4cac-f4ba-416f-919e-9e5f31f8e5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best metric:  0.7728011783942842\nBest alpha:  {'classifier__C': 0.21500000000000002, 'classifier__l1_ratio': 0.5000000000000001}\nBest estimator:  Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['HS_ENGL_GPA', 'HS_MATH_GPA',\n                                                   'GPA_1', 'UNITS_ATTEMPTED_1',\n                                                   'DFW_RATE_1',\n                                                   'UNITS_ATTEMPTED_2', 'GPA_2',\n                                                   'DFW_RATE_2']),\n                                                 ('cat',\n                                                  OneHotEncoder(categories=[array(['White', 'Hispanic/Latino', 'Asian', 'Visa Non-U.S.',\n       'Two or More Races', 'Other', 'Black or African American'],\n      dtype=object),\n                                                                            array(['Female', 'Male'], dtype=object),\n                                                                            array(['Continuing Generation', 'Unknown', 'First Generation'],\n      dtype=object)]),\n                                                  ['RACE_ETHNICITY', 'GENDER',\n                                                   'FIRST_GEN_STATUS'])])),\n                ('classifier',\n                 LogisticRegression(C=0.21500000000000002,\n                                    l1_ratio=0.5000000000000001, max_iter=1000,\n                                    penalty='elasticnet', random_state=33,\n                                    solver='saga'))])\n"
          ]
        }
      ],
      "source": [
        "# Get the results\n",
        "print('Best metric: ',grid_1.best_score_)\n",
        "print('Best alpha: ',grid_1.best_params_)\n",
        "print('Best estimator: ',grid_1.best_estimator_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8e657019-f737-4cd7-adb1-4cf782d3e437",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "l6RLSJ-LgylZ"
      },
      "source": [
        "Now that the optimal values for \\\\(C\\\\) and \\\\(r\\\\) have been chosen via cross validation, we fit the entire training set with the *best_lrc1* model, and use that to calculate performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "065fe5cf-84e8-499a-bcdb-5579dee2d73b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JwerSSvbgylZ",
        "outputId": "29c9adc1-fd06-42aa-d0a6-e4d145873755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7734633179114342"
            ]
          },
          "execution_count": 269,
          "metadata": {}
        }
      ],
      "source": [
        "# Train and evaluate the model with the best hyperparameters on the entire training set\n",
        "best_lrc1 = LogisticRegression(penalty='elasticnet', solver='saga',C=grid_1.best_params_['classifier__C'],l1_ratio=grid_1.best_params_['classifier__l1_ratio'],random_state=rms,max_iter=1000)\n",
        "\n",
        "\n",
        "pipeline_lrc1_best = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_lrc1)\n",
        "])\n",
        "\n",
        "pipeline_lrc1_best.fit(Build_c, y_build_bal)\n",
        "predictions_lrc1_best = pipeline_lrc1_best.predict(Build_c)\n",
        "score_lrc1_best = pipeline_lrc1_best.score(Build_c,y_build_bal)\n",
        "score_lrc1_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "49a234a5-0dda-49fb-a968-3e9c32371d7b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7_nlk75ugyla"
      },
      "source": [
        "Note that this represents a slight improvement over the \"out of the box\" Lasso estimator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "292e38dc-44b4-49d0-b586-edf0bc5b65f6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5YZp463pgyla",
        "outputId": "33439755-28aa-4e6b-d363-8849f841bc87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0005783212161268869"
            ]
          },
          "execution_count": 270,
          "metadata": {}
        }
      ],
      "source": [
        "score_lrc1_best-score_lrc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee9d423e-a7b5-40c9-a58c-a22901dc3899",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ovvpDpmUgyla"
      },
      "source": [
        "######Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5bb88afd-69f7-408b-9d2c-04258b4e7e18",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "v1rksMG6gyla"
      },
      "source": [
        "We're ready to see how our built model performs on the validation set. We fit the model on the bulld set above, let's predict and score here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "53c259ed-6ec9-4153-93eb-452256b4645c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "67mdlnqogylb",
        "outputId": "ea2ebc22-87f4-4e2a-c5a4-02b1057413b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8215384615384616\n"
          ]
        }
      ],
      "source": [
        "predictions_lrc1_best_val = pipeline_lrc1_best.predict(X_val_c1)\n",
        "score_lrc1_best_val = pipeline_lrc1_best.score(X_val_c1,y_val_c)\n",
        "print(score_lrc1_best_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "635dcdf6-77b3-4635-bec8-fe241af62402",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "7pmKDrSMgylb"
      },
      "source": [
        "Not bad at all; the higher accuracy indicates our model generalizes very well. Now let's incorporate a search for the optimal threshold in our validation process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c8abee6f-06d3-462b-90da-fc64254dd119",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "J18qJruIgylb",
        "outputId": "519859d6-3f69-4e27-b346-05bf0989afbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.11794871794871795  Specificity= 0.011494252873563218  Precision= 0.10880829015544041  Sensitivity/Recall= 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get predicted probabilities\n",
        "y_proba = pipeline_lrc1_best.predict_proba(X_val_c1)[:, 1]  # Probabilities for the positive class\n",
        "threshold = 0.13        #This value is identified through trial and error - repeatedly using this code block to identify the best validation accuracy\n",
        "yhat_th = (y_proba>=threshold).astype(int)\n",
        "\n",
        "cm_lrc1_best = confusion_matrix(y_val_c, yhat_th, labels=None, sample_weight=None, normalize=None)\n",
        "disp = ConfusionMatrixDisplay(cm_lrc1_best, display_labels=None)\n",
        "disp.plot()\n",
        "cm = cm_lrc1_best\n",
        "\n",
        "acc_lrc1 = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0])\n",
        "\n",
        "specif_lrc1 = (cm[0,0])/(cm[0,0]+cm[0,1])\n",
        "\n",
        "prec_lrc1 = (cm[1,1])/(cm[1,1]+cm[0,1])\n",
        "\n",
        "sens_lrc1 = (cm[1,1])/(cm[1,1]+cm[1,0])\n",
        "\n",
        "print(\"Accuracy=\",acc_lrc1,\" Specificity=\",specif_lrc1,\" Precision=\",prec_lrc1,\" Sensitivity/Recall=\",sens_lrc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a72e5fdb-c1c9-44cf-9e93-6016e4d6d709",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "mC-dYiU5gylc"
      },
      "source": [
        "######Variable Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "917a6fcd-52e3-4fb2-94f3-68dee2c1b086",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5yfBoGvBgylc"
      },
      "source": [
        "Recall that regularized regression does not make the strong assumptions on the response variable that ordinary least squares does, and because of that, statistical inference concerning individual predictors is not available. That being said, elastic net is not a complete black box; we can identify and rank the variables that played the biggest role in identifying the optimal model. *Variable importance* is usually visualized graphically as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d49bfd7c-2d4a-4f6e-ae2e-3da940158f8d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "X0MWNni8gylc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "be4846a5-bce5-42cf-8cce-62eb8cf8d1a9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PH8xkO1Ggylc",
        "outputId": "82280c0b-3485-4ebe-ac7e-f773300baafc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"3418d175-79f3-4e61-9a34-2d36f3ae18b3\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3418d175-79f3-4e61-9a34-2d36f3ae18b3\")) {                    Plotly.newPlot(                        \"3418d175-79f3-4e61-9a34-2d36f3ae18b3\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Change in Accuracy=%{x}<br>Feature=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[-0.0018783256825672945,-0.001155808423983884,-0.0007224389765299044,-0.0005778259299386024,-0.0005778259299384914,-0.00036108901817599737,-0.00028896515300491643,-0.00021668472372693426,-7.228042927787115e-05,-7.222824124231142e-05,0.0,0.0,7.222824124242244e-05,0.00014443038846689848,0.00021678909979816474,0.0003610368301403266,0.0003611412062116681,0.004984087867951681],\"xaxis\":\"x\",\"y\":[\"DFW_RATE_1\",\"GPA_1\",\"FIRST_GEN_STATUS_First Generation\",\"RACE_ETHNICITY_Two or More Races\",\"RACE_ETHNICITY_Hispanic/Latino\",\"GPA_2\",\"HS_ENGL_GPA\",\"HS_MATH_GPA\",\"RACE_ETHNICITY_Black or African American\",\"RACE_ETHNICITY_White\",\"GENDER_Female\",\"GENDER_Male\",\"RACE_ETHNICITY_Visa Non-U.S.\",\"UNITS_ATTEMPTED_1\",\"FIRST_GEN_STATUS_Continuing Generation\",\"UNITS_ATTEMPTED_2\",\"RACE_ETHNICITY_Asian\",\"DFW_RATE_2\"],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change in Accuracy\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Feature\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Feature Importance for Elastic Net\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "X = X_train_p1\n",
        "\n",
        "\n",
        "y = y_train_bal\n",
        "\n",
        "model = LogisticRegression(penalty ='elasticnet',l1_ratio=0.1, solver='saga',C=0.445,random_state=rms,max_iter=1000)\n",
        "\n",
        "# Calculate baseline accuracy\n",
        "baseline_accuracy = cross_val_score(model, X, y, cv=5, scoring=classification_metric).mean()\n",
        "\n",
        "# Calculate feature importance scores\n",
        "importance_scores = []\n",
        "\n",
        "for feature_index in range(X.shape[1]):\n",
        "    X_subset = X.drop(X.columns[feature_index], axis=1)\n",
        "    accuracy_after_removal = cross_val_score(model, X_subset, y, cv=5, scoring=classification_metric).mean()\n",
        "    importance_scores.append(baseline_accuracy - accuracy_after_removal)\n",
        "\n",
        "#Original Variables\n",
        "importance_df = pd.DataFrame({'Feature': X_build_p1.columns, 'Change in Accuracy': importance_scores})\n",
        "\n",
        "importance_df_sorted = importance_df.sort_values('Change in Accuracy')\n",
        "\n",
        "# Plot the importance scores using Plotly Express\n",
        "fig = px.bar(importance_df_sorted, y='Feature', x='Change in Accuracy',\n",
        "             labels={'Change in Accuracy': 'Change in Accuracy'},\n",
        "             orientation='h',  # Set orientation to horizontal\n",
        "             title='Feature Importance for Elastic Net')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "93f3f6e7-1db8-4745-8cf9-bc849476bb24",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "1VpdODJ2gylc"
      },
      "source": [
        "It is interesting to note that if we consider the \\\\(z\\\\) statistic as a measure of variable importance, our multiple logistic model 2 ranked the top 4 features in the same order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6fc7d644-96e7-405c-bb20-2fa1a9c33dd6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-cK2n3E1gylc"
      },
      "source": [
        "#VIDEO 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7b295211-263c-45e0-93e6-c5248691c488",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_YiYXAcWy9FG"
      },
      "source": [
        "###Model Testing - Evaluating generalizability and looking ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f4b2b656-5f7a-4ea2-8178-145f23aeff23",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nQvZvaF2gyld"
      },
      "source": [
        "A model is only useful to the extent that it can be deployed to make accurate predictions on unseen data from similar target populations. This is the reason for reserving a portion of our data as a test set, completely untouched by the building, validation or specification steps. We use accuracy as the primary metric to evaluate performance of our model, but due to the critical nature of identifying the positive class (students who leave campus), monitoring recall and sensitivity will be of utmost importance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8c3b02af-7ccf-48b5-b4fb-5e159c785575",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "2oyq2s0Xgyld"
      },
      "source": [
        "######Fit Selected Model on Entire Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9c37a0fc-c43f-4074-9f11-a19e736601b5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8IyqFJ6Ngyld",
        "outputId": "1b0869ca-809f-4855-d213-058f16cc2a6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                   &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                   &#x27;DFW_RATE_1&#x27;,\n",
              "                                                   &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;,\n",
              "                                                   &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                                            array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                                            array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                                  [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                                   &#x27;FIRST_GEN_STATUS&#x27;])])),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 LogisticRegression(C=0.21500000000000002,\n",
              "                                    l1_ratio=0.5000000000000001, max_iter=1000,\n",
              "                                    penalty=&#x27;elasticnet&#x27;, random_state=34,\n",
              "                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;,\n",
              "                                                   &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;,\n",
              "                                                   &#x27;DFW_RATE_1&#x27;,\n",
              "                                                   &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;,\n",
              "                                                   &#x27;DFW_RATE_2&#x27;]),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                                            array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                                            array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                                  [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                                   &#x27;FIRST_GEN_STATUS&#x27;])])),\n",
              "                (&#x27;classifier&#x27;,\n",
              "                 LogisticRegression(C=0.21500000000000002,\n",
              "                                    l1_ratio=0.5000000000000001, max_iter=1000,\n",
              "                                    penalty=&#x27;elasticnet&#x27;, random_state=34,\n",
              "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 [&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;,\n",
              "                                  &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                                                           array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                                                           array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)]),\n",
              "                                 [&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;,\n",
              "                                  &#x27;FIRST_GEN_STATUS&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;HS_ENGL_GPA&#x27;, &#x27;HS_MATH_GPA&#x27;, &#x27;GPA_1&#x27;, &#x27;UNITS_ATTEMPTED_1&#x27;, &#x27;DFW_RATE_1&#x27;, &#x27;UNITS_ATTEMPTED_2&#x27;, &#x27;GPA_2&#x27;, &#x27;DFW_RATE_2&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;RACE_ETHNICITY&#x27;, &#x27;GENDER&#x27;, &#x27;FIRST_GEN_STATUS&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(categories=[array([&#x27;White&#x27;, &#x27;Hispanic/Latino&#x27;, &#x27;Asian&#x27;, &#x27;Visa Non-U.S.&#x27;,\n",
              "       &#x27;Two or More Races&#x27;, &#x27;Other&#x27;, &#x27;Black or African American&#x27;],\n",
              "      dtype=object),\n",
              "                          array([&#x27;Female&#x27;, &#x27;Male&#x27;], dtype=object),\n",
              "                          array([&#x27;Continuing Generation&#x27;, &#x27;Unknown&#x27;, &#x27;First Generation&#x27;],\n",
              "      dtype=object)])</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.21500000000000002, l1_ratio=0.5000000000000001,\n",
              "                   max_iter=1000, penalty=&#x27;elasticnet&#x27;, random_state=34,\n",
              "                   solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
              "                                                  ['HS_ENGL_GPA', 'HS_MATH_GPA',\n",
              "                                                   'GPA_1', 'UNITS_ATTEMPTED_1',\n",
              "                                                   'DFW_RATE_1',\n",
              "                                                   'UNITS_ATTEMPTED_2', 'GPA_2',\n",
              "                                                   'DFW_RATE_2']),\n",
              "                                                 ('cat',\n",
              "                                                  OneHotEncoder(categories=[array(['White', 'Hispanic/Latino', 'Asian', 'Visa Non-U.S.',\n",
              "       'Two or More Races', 'Other', 'Black or African American'],\n",
              "      dtype=object),\n",
              "                                                                            array(['Female', 'Male'], dtype=object),\n",
              "                                                                            array(['Continuing Generation', 'Unknown', 'First Generation'],\n",
              "      dtype=object)]),\n",
              "                                                  ['RACE_ETHNICITY', 'GENDER',\n",
              "                                                   'FIRST_GEN_STATUS'])])),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=0.21500000000000002,\n",
              "                                    l1_ratio=0.5000000000000001, max_iter=1000,\n",
              "                                    penalty='elasticnet', random_state=34,\n",
              "                                    solver='saga'))])"
            ]
          },
          "execution_count": 275,
          "metadata": {}
        }
      ],
      "source": [
        "pipeline_lrc1_train = pipeline_lrc1_best.fit(X_train_c1,y_train_c)\n",
        "pipeline_lrc1_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3997d74b-475c-4e0b-92a7-736316771114",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "fVhICjR8gyld",
        "outputId": "9b1e851e-0f1e-4ec8-f171-1d32c6339102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.8460749102103643  Specificity= 0.8713956170703575  Precision= 0.38227146814404434  Sensitivity/Recall= 0.641860465116279\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "probs = pipeline_lrc1_train.predict_proba(X_test_c1)[:, 1]  # Probabilities for the positive class\n",
        "threshold = 0.13\n",
        "yhat_th = (probs>=threshold).astype(int)\n",
        "\n",
        "cm_lrc1 = confusion_matrix(y_test_c, yhat_th, labels=None, sample_weight=None, normalize=None)\n",
        "disp = ConfusionMatrixDisplay(cm_lrc1, display_labels=None)\n",
        "disp.plot()\n",
        "cm = cm_lrc1\n",
        "\n",
        "acc_lrc1 = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[1,1]+cm[0,1]+cm[1,0])\n",
        "\n",
        "specif_lrc1 = (cm[0,0])/(cm[0,0]+cm[0,1])\n",
        "\n",
        "prec_lrc1 = (cm[1,1])/(cm[1,1]+cm[0,1])\n",
        "\n",
        "sens_lrc1 = (cm[1,1])/(cm[1,1]+cm[1,0])\n",
        "\n",
        "print(\"Accuracy=\",acc_lrc1,\" Specificity=\",specif_lrc1,\" Precision=\",prec_lrc1,\" Sensitivity/Recall=\",sens_lrc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "61378021-f686-42c1-93fd-019a30b31b1d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cSAUS5L_gyld"
      },
      "source": [
        "It is interesting to note the distribution of predicted probabilities of departure:``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9312697a-1ff8-480f-b1c1-502a31de650e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "RIibV07Vgyle",
        "outputId": "7054860b-eafa-417b-cfd9-0484693ad21f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"0a40e3da-936b-4412-9b31-c09598d00005\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0a40e3da-936b-4412-9b31-c09598d00005\")) {                    Plotly.newPlot(                        \"0a40e3da-936b-4412-9b31-c09598d00005\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"x=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.026928178752686212,0.0746648981043685,0.06223065306601496,0.02768501200014812,0.03022838397195125,0.2769412541254853,0.030540375029556036,0.028162243863633127,0.037606509834214846,0.021367679053933032,0.3291616771250519,0.0344681259804426,0.03204953926898918,0.03099641574702126,0.04541557592609954,0.06815505557752215,0.07867818746764885,0.03265791297793498,0.03106180173353854,0.02442514014791345,0.3600299852896786,0.13518539177477645,0.02277176011971547,0.06581722417400862,0.029334078465548592,0.027073893601332156,0.022534265148071875,0.023171988762334565,0.5861954537822163,0.02892339094754965,0.06698189214777261,0.8669233316782922,0.07843093880493285,0.1403971592525167,0.09849472803194653,0.09680318179857987,0.023229617488670393,0.02952016067068891,0.0367293728009898,0.027959791131662238,0.02406733366485555,0.030603992463167175,0.08209901005504426,0.0760149897504618,0.038904380051372464,0.034310770922975235,0.024069553320773728,0.024049273686466827,0.025748702002702087,0.03327228210721207,0.46989364612479,0.06573976986148786,0.059066290391946046,0.04054687117141811,0.03126552351485413,0.020909014495875382,0.03579959392099514,0.03722764001774668,0.02479607178804577,0.7749603879739024,0.03964788093740653,0.02716442884441612,0.028882227535125805,0.0325560375680274,0.0667649975873596,0.022273176874663212,0.023267882423131946,0.021502834853534717,0.02282709795239422,0.05677763197872,0.04071750488952617,0.03931193491908646,0.03429084736397218,0.06908245024838686,0.024698109263648432,0.24368817976157692,0.054480089259128006,0.03466073064560179,0.18818067989092394,0.026849655444151833,0.024327801202134357,0.03180104873131577,0.03248264991546822,0.025865481452824697,0.05265391221777258,0.09554347181351798,0.02126184311272822,0.024748806425280975,0.8753158416552657,0.8714369965712739,0.026751334449916946,0.07344220722221934,0.026991369067611067,0.026360522520695658,0.023358373155950702,0.709845060341367,0.023387463805224565,0.0564496537302082,0.028516832518393036,0.12444275309699603,0.02958666426066199,0.07870577841636574,0.07110152784560098,0.022979515390111692,0.2195139310794889,0.029735367029139554,0.10095520703001484,0.02299217404327194,0.034658741130661176,0.02837065949338353,0.14170198057060582,0.29101728947975536,0.026241911428947867,0.023016935921407997,0.5827259411229324,0.12300718285300913,0.055283422111564506,0.02579743322184752,0.031791221211605915,0.06898224542408543,0.030507305504788477,0.028930063692337814,0.02987204297683292,0.023755382646943164,0.022738665530151227,0.026574034408662454,0.15620343135195885,0.7176403401247542,0.05035603924426267,0.02639389563279057,0.028335140464797,0.028660737930739345,0.027955019821797964,0.76488683125382,0.03415594533522237,0.08877016693681809,0.25327055239761853,0.02474725004214879,0.03108904631820109,0.2888090049771832,0.05331652938394827,0.041596398211216856,0.04143469708435899,0.12829795806369515,0.06742999392785604,0.11719394213480729,0.5219204506053386,0.06877950948851347,0.030916102709577894,0.3827702025175629,0.02353655572787053,0.03650054957449822,0.07996607223626,0.05854735841957071,0.027706609462035972,0.0321276090331726,0.7905786267376443,0.05364067909202347,0.5750947313550767,0.3910428078310069,0.022267995311057494,0.05372115968115044,0.3499526591946978,0.02058657916274429,0.027756270971241156,0.029836772167952597,0.07966772913190459,0.0732401932915261,0.42938092369146386,0.024074015311605128,0.029587084188717756,0.06751911393365563,0.028023683410258595,0.025871241449102345,0.03384255140482247,0.021853438034003747,0.2045692326430064,0.026470217957645294,0.03258374023181146,0.027306789399356404,0.08339585711628592,0.20070470228104695,0.035936365111363,0.1359940240692433,0.024834363383206215,0.15815201986944555,0.034143502808794496,0.04832290697604199,0.026590538740511715,0.028285457194993674,0.2699678021378171,0.03864654347699509,0.0561178116246906,0.024709922224195057,0.03430618043127362,0.024477927921787854,0.31273819932415037,0.03950696899594866,0.02754321676196929,0.024485313741457758,0.030806486337343817,0.6048619202303697,0.027493798074197624,0.02736947822866328,0.5199619502202412,0.021352276052395258,0.02737524346306384,0.022681145014382724,0.027766599105885553,0.15736026168883335,0.033583146857006356,0.02939594399321687,0.4582422444429014,0.034205449376783305,0.06120582036280108,0.02672475083535264,0.19393572485212365,0.02486917773282666,0.06829588302638151,0.02762054826165518,0.025130298019109758,0.06037611385237781,0.03681980757580684,0.023010006096432986,0.030656694580686806,0.0801695702351318,0.026609882115221962,0.04105774660949575,0.07046581704517542,0.032001745631812115,0.03182146486654054,0.0263143810096889,0.029251480128243475,0.02451317485893416,0.031858451548172254,0.029470630619073155,0.03137709579881317,0.03453069202842083,0.029594803203492618,0.024848292470242426,0.023783668811118926,0.08722992203219808,0.02200738957595655,0.02747960555732859,0.031813883617560115,0.031290837236432285,0.03463921184466373,0.2806833919194073,0.023911087202112213,0.029800087574448266,0.035315146871338694,0.027404052035372547,0.024394933104542105,0.03218934546367679,0.10036009899166473,0.02770054733304433,0.029942038279683402,0.11402489793754236,0.028884044659681786,0.04753065109557407,0.5395232449065076,0.33544870801152793,0.7883954676545292,0.03243000533094793,0.04881775232293779,0.02723785774497992,0.0341937854489996,0.613603121266749,0.026674613006869012,0.41704392799662493,0.026433697308661564,0.03888218903526868,0.05938700434340885,0.1468391823254556,0.03467891817260902,0.0321892416561362,0.27497033280792327,0.13172934631185976,0.024545702619501408,0.05773369438241037,0.07437830205713285,0.02429888832950135,0.029517646038068234,0.2692008067581566,0.0971746677906802,0.02440106785268345,0.03416030045333872,0.03084013992005939,0.199963833927257,0.03346403998010503,0.2843489967971383,0.06672844685961768,0.027700311388804594,0.36774640078024684,0.07139367091224483,0.19305277290380593,0.028766809127496715,0.031175542771800447,0.038013310556599254,0.045675875455900024,0.036155096854366596,0.03776900125758446,0.06207262355615355,0.027567898990754656,0.185876960112762,0.025919656521022263,0.02731594343687729,0.023116960100388315,0.1784436209814369,0.025162218267135248,0.06539750054026658,0.12919117815955355,0.6053860828852886,0.03789026188186301,0.08715564035391803,0.0830332373555003,0.06117393653097278,0.023759781663462248,0.047795369016598664,0.09784283175790667,0.036841113555857355,0.025449368196002124,0.0214232703040246,0.026592278855404706,0.12047291798891233,0.09265092763807693,0.04337131386776512,0.03173989701340115,0.022318971191140877,0.03512566452743241,0.24029557698180362,0.03363137683233819,0.032934897601173546,0.05585849705523537,0.023298314245630218,0.023562949441473212,0.03584080261736609,0.03871399409178456,0.03849040619680689,0.03921369609533477,0.2857399040310171,0.6392777062427457,0.19561209349490385,0.024408239531475192,0.030931354467409734,0.13109940955459798,0.0364138891296247,0.16288457570819856,0.08241919675289675,0.02866456352781977,0.02885831163053276,0.02709293092466155,0.4515299989544578,0.02792623952119588,0.023640314990417948,0.028905473979025553,0.4022699800831919,0.027956592084577604,0.03432729847648421,0.04077522500526433,0.1441354211943603,0.051966959919967845,0.027418550117287475,0.028729095367081486,0.07527896686606146,0.060214933885760005,0.026634214549001427,0.027445809499848723,0.03684034547461321,0.020958889648476413,0.5046846001868631,0.23392889799147773,0.4470887935537862,0.09464545250772267,0.263875024407272,0.02436041731230621,0.05044420530369252,0.02822938554154609,0.044260942968794806,0.04202908124529787,0.3432442294504353,0.0500806191774976,0.056653328320108026,0.06945877155841204,0.0238162028693964,0.024780096242902692,0.022336489941558058,0.02493092103075567,0.03561609879285005,0.10311737813184643,0.06333985315653584,0.027201807343523808,0.21105037421482165,0.023830883241353754,0.04102282344990916,0.042812676407136825,0.031496195196790984,0.0625268348375607,0.03471993240146712,0.02737650517239226,0.02366394386039648,0.02345689622622476,0.05727567535994352,0.0365298482923431,0.06408066094188516,0.02516325171864408,0.027499126664930266,0.06874791725656423,0.02625154979284869,0.021720904643077906,0.025046150182627024,0.24863561490025082,0.02410791929384769,0.07097047245623268,0.648641103522884,0.03809009094180509,0.8769632894675011,0.027089075293043597,0.024154903824703897,0.39082043329207555,0.026662516572943062,0.02300024163636964,0.02730714408333113,0.15110237361215537,0.08887892792510052,0.028386089188547885,0.030546279220678475,0.03589455564084057,0.029189557826776527,0.031405243358986965,0.029695185591992218,0.03929441044478073,0.024952063661655945,0.027592191701797216,0.024934756553716162,0.029317378479293187,0.02223775984048391,0.4668951289757433,0.02634554921841569,0.023260472476163984,0.0284945443453147,0.16999919998972546,0.03649445621041017,0.3629878739710536,0.02617701811822,0.03547403442328252,0.18806426781457616,0.02675345634678909,0.027448321580312584,0.05012891271722157,0.02811380011481283,0.027282808953012692,0.11634924351848701,0.09573061353540745,0.03784042948360243,0.08010422213024941,0.31234015844571555,0.8244093305203788,0.03144101795139975,0.5661316344305521,0.03532361121665975,0.02949601448740157,0.04981953668939712,0.11355204198488472,0.0292427925078122,0.06118221894340277,0.024295039841039475,0.023531539930225442,0.07163632886622416,0.09062234883387024,0.025944582651795847,0.18300070827790318,0.024340391586695983,0.12143040368753567,0.1654767313336673,0.10294248434048535,0.10195167480847431,0.42857072978709865,0.025742776752978997,0.020658061134086973,0.024862660057075346,0.04085452040497812,0.17491510846260747,0.04793470541006475,0.0413508344043086,0.10308319335513551,0.02742995495142419,0.025434150356581187,0.04400201738369138,0.0267625686173649,0.04971587610904931,0.34685537416259765,0.10377717657797458,0.06983805695699094,0.03118502654904368,0.20733118951331655,0.029704828256844198,0.025151847878443916,0.040184837892627015,0.05026313615968927,0.0418926211918527,0.029276255266402134,0.18553396448278664,0.042853148730400935,0.12264794356198307,0.031929595721520786,0.1981712556421435,0.03641077658854104,0.16399665693510596,0.030557265490354344,0.023895622705158395,0.02523481167279829,0.02622168248859573,0.0413477402137605,0.023143640339064114,0.023682456181901748,0.024245437215463933,0.0502266182741648,0.6654442123201674,0.025724401749343984,0.043425171996402306,0.0933134178141303,0.3343790566155115,0.15128365060626403,0.07120105325029233,0.025101485898712854,0.6987512721855639,0.06094207961561284,0.032102432559726314,0.027550965583892875,0.03930577347584271,0.047871089063708856,0.02349798756290523,0.028657233243515063,0.2569294556224148,0.7763616266278593,0.035641491818860054,0.022761638661150857,0.027252116185210885,0.1338509710328774,0.06249792558919435,0.02787937631715113,0.05747997447348763,0.023495047024592635,0.12933879044934873,0.24617195415511262,0.026083787691215315,0.027497109346000154,0.03604584127571295,0.027548581452612193,0.039288771302166264,0.06049218985686024,0.187065076216271,0.022889890669597047,0.021122061888124384,0.028036543577626437,0.028371084789600584,0.0498029672786415,0.642408313523627,0.022624825275096936,0.5908291883712916,0.02125778720010419,0.02143440585016272,0.030042957207339824,0.02439808498954441,0.6917118574049428,0.2321594088025341,0.04171641714939941,0.5420798527919596,0.02214530336517211,0.08647219167569237,0.03134357872748735,0.040220828299385614,0.02570248272454458,0.02689145203497826,0.05664182049924985,0.09456769947176026,0.024828705504330455,0.026797905232949188,0.026542132779640244,0.024497766544504637,0.06270835361893846,0.047486637540541256,0.039185622051194906,0.02990641564766384,0.0807412847950779,0.06743697982387968,0.024893032990554616,0.0256556098986548,0.07451026948120915,0.030775534265021574,0.04938878540469158,0.03944193160010617,0.05530996965556043,0.11556808467908551,0.07005406179295458,0.0404356709028388,0.07144522047976605,0.04384820038232949,0.05458306888455808,0.1116901439534327,0.023757914095607464,0.023917113188692846,0.027051945686414808,0.12029622851057924,0.1409283389273201,0.027402404856308876,0.0290803759154782,0.11423455792334994,0.03303939678976819,0.03323667289039017,0.1285364184608481,0.021826857638812307,0.7476150389253219,0.026954063064370048,0.02886179675715563,0.3192511593247641,0.020266081647559345,0.03546937182210581,0.0655246558859917,0.03005759981288016,0.025304984251780287,0.3403583783208688,0.02481537697425176,0.031762805891653235,0.07348615405288367,0.7196794571913943,0.06684146516983444,0.027433857147534033,0.06281139095330183,0.023119804683147373,0.04296900019659828,0.6247904215247734,0.03802957214449795,0.1365913517117354,0.0356319012040132,0.027905813879055642,0.023350374642425193,0.04206761790025595,0.14586628862308862,0.03068118787290349,0.11600894514257819,0.05256747209618368,0.024758006578909284,0.048851609587914284,0.024187430400634653,0.08913531829180522,0.0315585213672267,0.18550976034467445,0.027793627290658745,0.2304518338850215,0.03230591600505386,0.2335650070596421,0.8783257244505382,0.5353784186016596,0.10775917024932487,0.02645703540176816,0.048641799393511015,0.07574227310428264,0.03090670087947678,0.05580619295286896,0.027602532245279133,0.02730032691186879,0.02372246425571491,0.0623683897664377,0.026940952220752944,0.046463289995920004,0.02511996235847065,0.04679541016915824,0.04212019418566965,0.03026007840297869,0.025959979422670598,0.024195919903558148,0.2629380287340692,0.025594137434020347,0.04286345460295197,0.09183087901406982,0.2401631011847846,0.3569267295998232,0.03393305654051485,0.5687859907541519,0.10363332490058023,0.023556299621996523,0.023026610888401685,0.3470682678280585,0.03897350123758823,0.024431295268569367,0.028903237798919117,0.04741993369610119,0.08928973645147964,0.4685505033615268,0.03677146689762442,0.028253529105921073,0.04772755695379321,0.5422958667484614,0.03267290318418734,0.1346012596244624,0.11435378875223196,0.03294155428626596,0.03174190401452393,0.2122689345162356,0.8880734625423601,0.026625411793498,0.04580490376603858,0.02693804956822663,0.03147220021776529,0.5482358503292208,0.5701844673917419,0.02410667705135641,0.03954034400742321,0.02801001962109992,0.023437858228856575,0.08431383504666748,0.02820385988694513,0.025663926753463913,0.0220014374109061,0.2735556920553872,0.18053151227062939,0.025001817305745896,0.29857133148899767,0.03483502600482765,0.024804144902116656,0.3063427457564212,0.024399266655472474,0.03531891883836642,0.08902678381595118,0.024712901923658825,0.028827141450625832,0.2706426569949633,0.026328551007760763,0.021706492926989854,0.030829109516966317,0.030723742532510156,0.05165233328931641,0.02778940439030259,0.13949121097396774,0.029987952592800654,0.03715884951549948,0.029631590474190245,0.021492003971342318,0.026162710389121015,0.5982592467386095,0.048574462065091244,0.4330905046880579,0.7494679666814624,0.06432871652179968,0.4987288597896304,0.02870215429568749,0.03697537153198676,0.024442853545022056,0.02543488581002903,0.36720352008956847,0.16875019707946742,0.0698625552212856,0.031781387920634944,0.18210145043454248,0.027485685242530037,0.023929824840809008,0.02476412290858158,0.026974601258248156,0.498342760549516,0.1630866128176969,0.025598908869709773,0.033998184492,0.0521846953990707,0.027704481386615614,0.08803278801689188,0.08565916182867653,0.030095309181834937,0.026554686932653667,0.023577007067343204,0.05865549188503051,0.14996041292428752,0.029623844127921573,0.2943179728527552,0.02347161258033981,0.023098748335251204,0.04634908634095228,0.09205347629463914,0.030363278220815113,0.0381165772515623,0.024048010998933896,0.0295540131845043,0.023109736145796634,0.042170381827331856,0.02383512861890069,0.8317665994697327,0.03269922368142979,0.02695582185887388,0.14598211460978908,0.03353283743969408,0.02616901156624883,0.04738856109669521,0.060036847425118854,0.10553393443228834,0.10509289547736905,0.11328434401900025,0.05435552432198911,0.029522891322868915,0.09471996764970189,0.02921880875280223,0.028953119823033106,0.02405038913752391,0.19929726632548825,0.2504259480603542,0.17697692750277508,0.8045578540471954,0.02496132997632739,0.1924286416681533,0.026029550038071274,0.024699432045724497,0.028299697634687353,0.31747219813089184,0.028561202150260888,0.03350200798326502,0.17388536292889564,0.028023179707106275,0.026789852366764576,0.1499858197497278,0.026056332785168196,0.3376825894483319,0.09421564045945668,0.02571728063246346,0.0343869557383822,0.056797758799367035,0.033648018040996514,0.030786298963850545,0.051553075130606515,0.09683468886582844,0.061658978844170334,0.022510586833749936,0.44503563831636095,0.048315050301334664,0.02641880337447407,0.12269121949762668,0.02365407688262832,0.23319122804216763,0.024229307503637252,0.02642404974461031,0.6560992707543561,0.028148097096456662,0.04438413393850363,0.24889393273592064,0.02519755328266001,0.18882548686832515,0.11485655931086497,0.027342399195762094,0.02544951281704792,0.028346494673613113,0.030090868296969654,0.3480021771260156,0.026041384180085243,0.3297268142221029,0.029684549813088684,0.024040494810633337,0.024816127532588238,0.02393065674517188,0.029523234412081798,0.06230602235136333,0.06500256962770715,0.039939930966253055,0.029381606145977106,0.03638248060032343,0.0660056228778115,0.047154586433659926,0.8646201903874935,0.023244919562609483,0.026815792552612135,0.7662093714169375,0.19913538139056708,0.03333854010748796,0.023936897703286886,0.827262604132324,0.024000504991159806,0.863527531487217,0.06535808649224421,0.048949176658773894,0.02144286424439302,0.041038744366734874,0.029797827527940433,0.03548178746471708,0.7591773282859899,0.030944798323250998,0.22585298375727011,0.06876803437369763,0.039218699983915244,0.4270442494284971,0.04044781755508572,0.037604653493582885,0.1928846625952347,0.24959589197038057,0.024076575747732518,0.024110570883039226,0.021051783790624848,0.02403609502301298,0.027726850018150798,0.02292564707645303,0.03236102646285094,0.02901109278763233,0.062060500290472206,0.7973708328824095,0.029476861042444843,0.024426400310604498,0.03563612073090489,0.23750906631299082,0.17073719324323064,0.11895389178150502,0.037347385773826916,0.05508925766781873,0.028036309196286656,0.21680116599152666,0.03540351699521274,0.052711848725340335,0.3600019965122066,0.07246895521933033,0.12447496947005669,0.030369526328180362,0.046244837542262336,0.04952373961962293,0.022928362886387187,0.16677251789185663,0.03748896382108104,0.027817786636917464,0.027045778749876806,0.04728431586393545,0.03590249422073293,0.021220711657046754,0.025296031218602362,0.08449343199271993,0.026453671911713397,0.025653064812928335,0.02344230568517374,0.1973564048313039,0.022327122198880972,0.8358775095938633,0.46460258492756984,0.6231589979694806,0.024028663931179698,0.12737117303001005,0.2992427377894686,0.07184054277656357,0.3240299178273347,0.0310391576202906,0.23971366607282993,0.037035160880580606,0.02973997600254798,0.030306009702175275,0.19834253643216068,0.021844286177409227,0.025445994364573607,0.025067286840133797,0.02613010267542666,0.04816325324836732,0.03512767566528858,0.04934206521494052,0.14608116680254313,0.023372190772237445,0.025744957850008455,0.028781986664353425,0.027625089698028963,0.21282261993742568,0.038506699601689835,0.7925397838876767,0.04155757039034482,0.047930923783184344,0.021624034243918178,0.0691836834815047,0.0284847315896099,0.48432002751148806,0.05984586053828889,0.025351211308617037,0.027985380481974933,0.024979889997202364,0.53774684779148,0.053439458170339314,0.45873993049223843,0.030320079452419123,0.02785598107773543,0.6282649844067636,0.02544656948914247,0.03601779152533504,0.031806795777207665,0.06167274974243974,0.04570520443687756,0.20915816571177798,0.033154470627591213,0.10289940995259123,0.03873231167567525,0.03420669604806068,0.1980305192471697,0.03879746244830357,0.0288216273391447,0.04026307661916444,0.02184808006784851,0.09213738504284653,0.02469345514722468,0.029406134097623875,0.03467458186205681,0.0246905428517861,0.0752580950646658,0.032373483785567335,0.02756513958703604,0.034997510652801336,0.08035982240911696,0.030863886844001664,0.45729546679144234,0.030037590803634162,0.08510388404352086,0.2844715269778414,0.02617371146562368,0.02932372484078367,0.07672471415669914,0.025675714091608817,0.037448534406987935,0.2940220459465219,0.02644079629546499,0.25212184700366963,0.03629434923587372,0.030401976518529484,0.02730472075163481,0.027567622600385616,0.030875715225602555,0.02761957903909607,0.027447632739396603,0.026247511664013935,0.05149862119359965,0.11536114699549098,0.04874100168844424,0.11680559655527104,0.07863266599847049,0.16041774297677183,0.0637657373594116,0.03406855047000007,0.03365658851479287,0.026201698794266187,0.520518326590973,0.06097035759956591,0.04793883149162591,0.05658384555268189,0.022851082387665384,0.021895432678538544,0.026791487308696835,0.030101533189339287,0.0394202124074496,0.03721141373553704,0.15937277574917688,0.025073073157885113,0.03034884932663961,0.02961716512906991,0.024374942185544122,0.29959661065561155,0.3320972559347134,0.021975454384858213,0.027198276477148765,0.27532854187581945,0.061187020454141275,0.08181437706189966,0.04573090620202011,0.06389343264285456,0.05076374812642192,0.022331409789034314,0.03879915067989007,0.02968654913010724,0.06784158728500647,0.0309868092290854,0.03275067835442644,0.0271587621920992,0.06034068481440838,0.33345563689884444,0.0416183537186305,0.03333282078017096,0.072265306701445,0.06551787924426267,0.022482222095600794,0.07102628255141995,0.027124494616115914,0.049324907161809234,0.0549333832298056,0.7038859147443793,0.05272697991075832,0.02629081885956495,0.05707847258679403,0.032368853476101814,0.028071337020876358,0.024535037405727808,0.029913474841969927,0.03747617320909105,0.02188540315482499,0.060201744269726375,0.04161972488027816,0.02843969962020784,0.05858137169051977,0.875701984496712,0.0354263135164409,0.6099356454868056,0.024923697177297297,0.02911903032742112,0.038726471217623744,0.023813090010424885,0.023171988762334565,0.021997047709739443,0.026416401825859907,0.033395079680795164,0.039608300263242406,0.02374401495276368,0.46849932576290293,0.023822820685770997,0.03240686579692887,0.03359982422974732,0.04079243738998836,0.023573130476530733,0.031255043016556586,0.02915643047320922,0.026994361362681345,0.036370285393718,0.05358528291852143,0.9103842370320006,0.029573443002348578,0.27992604287985745,0.023184826776230433,0.5657318159833459,0.024798531057605846,0.4791287778219963,0.07200042780335819,0.057449301840618285,0.5110727742147966,0.08002397277545502,0.028940318912058838,0.08838742180669085,0.05252299856601917,0.8352939021721929,0.023358456845613298,0.027272384388169833,0.03684967704424719,0.031917005401787976,0.027440802652149928,0.02438288011846082,0.03068728410358242,0.024260428519041193,0.030633003279269825,0.02334627178846252,0.026539043810689122,0.02529174238131489,0.041578164976717694,0.027977089511906392,0.04602340032382028,0.28092917456681443,0.02618602937417485,0.024749510561848927,0.026295389457332653,0.6811897554795165,0.03684080783273648,0.06789187903248625,0.03647716642825345,0.034646784752349134,0.03492831224618384,0.026476781525656575,0.049293981431357534,0.025483951596895944,0.09065475340034936,0.03408123597286127,0.02723660063797104,0.03247830075118217,0.032569501900973054,0.023254327471260754,0.03831803359012383,0.029175870697047454,0.032004724061690824,0.044982685993858545,0.21866413671054724,0.05389389564022908,0.029778882064896286,0.0990673572493754,0.7845801238822434,0.03201342147265324,0.501217035341646,0.06992503081482415,0.025845964449793445,0.02612571860049865,0.2043340362292062,0.36244953236309846,0.023810188758610944,0.8899267945502571,0.029631903180761407,0.5338208641199862,0.0358888597869821,0.03191669164522464,0.03867282331448365,0.3070775904908201,0.02696254046945625,0.04435520002817231,0.2162751423833054,0.08638245462519198,0.14492189542223294,0.024590167320624173,0.4652921680566843,0.028424118501171113,0.06054974339649446,0.0453601568365761,0.17304133963524684,0.04444305348301492,0.031129032097626578,0.022669609237486177,0.025491647344855112,0.07300588862093327,0.028034226090047718,0.07414002530218392,0.04264362668714738,0.10198232232545826,0.02377486445945382,0.029254600573221132,0.0819023220010168,0.03447590191070934,0.024213501122576597,0.1666292805455263,0.02969272787511039,0.06536160304002513,0.030761592719795606,0.08274469250355614,0.4977486052418621,0.021289093227368244,0.02157323543816621,0.06209553203681113,0.024648461671788986,0.2745082358803668,0.26847946022251096,0.6242397567182478,0.03049507949754785,0.035759751540505286,0.573787546465677,0.06217972634924156,0.029943145716400124,0.023645378560083,0.026615243557067018,0.05780774504509787,0.041797805562594825,0.08385136694232566,0.035046972463814885,0.03460237917777713,0.028880085438707723,0.05454418482192887,0.4411014992488613,0.055493949778322935,0.4032356259933322,0.044731204475308824,0.02844265958419144,0.02347251615008475,0.02692595840451874,0.03369437210089062,0.024327593581827223,0.05174051145023793,0.22487272192036997,0.23628495412365583,0.03549244257623495,0.026252763491817092,0.247572832834856,0.020580458487921813,0.4977210826830877,0.04351398206672933,0.11487707099120595,0.021661392307822516,0.03113933608759805,0.02364062299179741,0.035597980127126866,0.02542070381412886,0.03455443522684782,0.03204348151989815,0.0443457930663499,0.1570792025815634,0.08678830206159081,0.022983596836566533,0.04975921331773505,0.03208223576314762,0.07207260901391187,0.023101825558513454,0.36453404544831197,0.06683589315911763,0.03366659249652136,0.028205801743501357,0.6399189874379821,0.02804489376560327,0.027862975083451088,0.24429046768649537,0.03383227801003995,0.02880957494777513,0.07190593309739171,0.024874318633620787,0.026078271802967852,0.028336122610674586,0.023583121856422477,0.03191396181967367,0.04980702144572628,0.7957147208958367,0.025720191475133568,0.12309097217611449,0.026898455446697545,0.031386117270114006,0.023364962214489984,0.15814436002787624,0.02323845661021372,0.03498547588960065,0.11309779303443593,0.02989141266389589,0.05727454542791348,0.05383258707916795,0.08546869282768743,0.02707867142193529,0.026080147115096784,0.0362965405773135,0.05856027412139542,0.021909921868113108,0.03188694732250409,0.03404989726316406,0.05841405241890956,0.2713570404279431,0.03512691434170049,0.028399513303494273,0.05798417890270568,0.024248359116969095,0.08347853675124263,0.049407627046161695,0.031733641585924885,0.024895208278422423,0.02297279753291503,0.12461775594551651,0.034517964239083605,0.03082048833573678,0.03398712885839408,0.026625120367566237,0.09913542017473605,0.20415281974189128,0.035143868696513134,0.10047932523743956,0.027377847187607794,0.6654725108166971,0.03633711778617383,0.036858305804268986,0.029572397794091043,0.023157602667691988,0.03872953538240297,0.05531863538446155,0.025162166420513027,0.14459568318281593,0.20214480627506207,0.026316509603574432,0.057187700727408086,0.03241463846777321,0.023634830832927656,0.029263517790715312,0.029668236165105506,0.3033759508034695,0.06808569240393479,0.028345013813615205,0.04467227341982591,0.028443215589539314,0.03322919446298932,0.03681364758648884,0.033220461686871657,0.8257574199552414,0.05382314457752907,0.04279665946639223,0.025460759322331477,0.028622653114640836,0.02683193061721226,0.030328654448790328,0.3654371745835513,0.03125473496230709,0.03666216363680315,0.8907320654548502,0.0347675385406855,0.04939540523051964,0.03266817767206031,0.022580819200831236,0.14111914081014984,0.058353677205969755,0.02888486960253543,0.07081749364642967,0.062350818189967856,0.048691828662515,0.05333199226521826,0.027662365771357583,0.12045982193613575,0.05523181729316448,0.023144738846495656,0.09718500492939543,0.3147278527234998,0.031074846666598387,0.08209320683149374,0.02644614506009266,0.0917513720486214,0.03202396130005767,0.02748821200509905,0.026858795403824236,0.03366491521933708,0.45768938092925593,0.10830425145143094,0.23063405778672433,0.04975586510882057,0.026647805980130328,0.02857599044106561,0.02942854851294309,0.02390247489587139,0.026202567725512584,0.034890339373669664,0.04161233094037952,0.06758932010272024,0.026405098692343247,0.07150055769490377,0.02374991033339927,0.02350677489108428,0.05046699372982588,0.03563953456624441,0.4451267045325261,0.17552110377342536,0.03210031351053128,0.02365756556055502,0.039291103286386304,0.04507442836532065,0.026577957415391065,0.024497913257653974,0.17396342190157188,0.033667672909434926,0.03401028948614133,0.028018938308841296,0.02638472018062558,0.025272726470558688,0.8687663454277701,0.029019018177000346,0.029196440485831202,0.17811776969285845,0.024145418410300917,0.03096125502037263,0.04786294336317229,0.05237359221880549,0.03213808434607274,0.09394468103460785,0.15210326799386617,0.031063435173184228,0.08722553969445669,0.023000723668484806,0.022834892439700556,0.02593797996219998,0.020964708578948305,0.025672960924507073,0.04216908899797105,0.5927824949090387,0.1202739149768195,0.03273181377249271,0.024019973859799,0.024566003069074933,0.025653374711126095,0.030040468995256833,0.028545056358450095,0.033021597207177476,0.036399543258362844,0.0304600122517232,0.023646906342830808,0.08577427136407492,0.030905816910791348,0.21594974839393966,0.03023340393797412,0.026997376243547345,0.02907024974934894,0.029129382882251847,0.03011690667231746,0.036542103354946856,0.11704268700820464,0.061210194962516284,0.026051115461187137,0.024841942231539935,0.025967107184619788,0.026819964507750434,0.12477708537671567,0.23153895327424762,0.025323701067477274,0.024929005110072563,0.13219541075859667,0.031058161771657777,0.6397396020923641,0.08364372945112365,0.421792232729131,0.5794227056153454,0.0344232152663897,0.042202604821871284,0.028953086710472226,0.05776190971863398,0.022385733079696678,0.03381415235333751,0.022095818900846734,0.025640438998435548,0.027892798830617783,0.04350482173765792,0.02488954475104149,0.12773420878430428,0.024104028159197367,0.04545882000806917,0.047104763815917064,0.042420295788134846,0.025780210363248592,0.057780431444367036,0.02634572914049901,0.033284637577150715,0.22474599465658232,0.03154980575594595,0.11898346779422041,0.027654592847328716,0.024822218835300582,0.05485212142663911,0.027156337597553603,0.030918100983064446,0.05011909665663818,0.02213846651803552,0.039052408644187325,0.5139274574156903,0.2582138553856528,0.027597988390832502,0.02723010329128156,0.032335898620930975,0.020120200650054477,0.04782471252628401,0.04599379952008579,0.036237908715922744,0.02307137699017869,0.05125488367142286,0.026628049295769934,0.02956169771418334,0.029656305677947997,0.02587075062866858,0.08124601730853867,0.020608260634220856,0.0769142681896706,0.03473295812247433,0.028307370238670936,0.059626141568880194,0.06098058966452493,0.538837956848845,0.0475960055938683,0.02429021460266691,0.026272924959465135,0.06129784525447697,0.024272724986194816,0.10336557113839354,0.029377865290849573,0.026644441611934213,0.06448656800758887,0.029624716973854263,0.1070257720108993,0.07789189950192203,0.02565798598158279,0.03311112889892467,0.033780587776946376,0.029306272680646353,0.05311457446076486,0.023204513592301735,0.029914941637468523,0.6060581076990319,0.032626971834045074,0.45550949124683116,0.030798910310562005,0.5777947109382601,0.03393135722680813,0.8800627393204032,0.5856584531998271,0.02557836435873135,0.031838997547574424,0.4060139979810368,0.03331707725474519,0.06108162024384491,0.0919271081293722,0.024228977038743604,0.6016567784690854,0.03428498252255754,0.026899253999771958,0.05404404082926086,0.1007642141702281,0.059310036757879915,0.02641971732060915,0.029090376928164643,0.02330872050848875,0.35446494024586506,0.08476522030272142,0.03154664061953321,0.024625921606556765,0.05591562191214532,0.04134705516795187,0.028938931583820066,0.5525573083509361,0.046919464770640344,0.031560898742837476,0.11865558227415951,0.025604022807741558,0.030239640668092295,0.024120861025111802,0.14880077924336862,0.4826914278654534,0.025226634649031285,0.028753765773646425,0.05844603234245112,0.03255174387091005,0.8794212545533153,0.03320781405024585,0.026766697921638757,0.04528626131056703,0.029247694857390937,0.11192372184150182,0.029549569240695184,0.027751178339660707,0.029284899152852217,0.04846644531233251,0.02668115361601191,0.05550312795642633,0.030016166060129777,0.024302153723600697,0.026825787704605315,0.13702489914216498,0.6835385470408636,0.22785771679871925,0.2615694950266972,0.03823440152535248,0.030749994818916108,0.0216083481397881,0.2983059680735007,0.024446997229063787,0.03148284181655451,0.037518099414099795,0.028465042713913632,0.11314562271155502,0.04192057384314819,0.025447876307830398,0.03201818933124544,0.06209504553016494,0.03614289263496884,0.12494469179305391,0.028074450445996515,0.024441732262700938,0.025802248602844975,0.03126387819525529,0.022803402406921258,0.19811683382337653,0.034294960567174,0.2067255730037334,0.03413210631760474,0.02327678123839612,0.024469153509525804,0.07316500091208405,0.04927668566751503,0.028403406506438692,0.029610769253442765,0.026535782018923097,0.032547228372606804,0.8783612374689432,0.06929835321966626,0.618733872562251,0.05585041464493319,0.06378640657122131,0.044750299937805285,0.0930950778921133,0.023995496732444586,0.6955493992880735,0.039234681304191193,0.024606364931874423,0.47069496028621644,0.03115492382279159,0.034046835296651444,0.8353000501624519,0.5557928227628764,0.033136885402069584,0.04669797864534193,0.30343608903815367,0.3281065767191355,0.023419622758033717,0.02460241297581806,0.023692670094811297,0.2518428656475411,0.021400597815048696,0.0730362824527556,0.7605487799214956,0.03854943106147399,0.034442710442887714,0.4137075281261557,0.0238810118239019,0.023860626992153643,0.037283180602280454,0.09870657143662863,0.4851406581054083,0.6506045033851031,0.02629103448471115,0.04320737636726697,0.04789969326789271,0.0664782900540203,0.03124628832581795,0.8614288773364704,0.0279268531119616,0.04770592780348024,0.4322371084609151,0.08844795807507089,0.8893430882127372,0.034268065444614575,0.2499265014833237,0.025455771743458967,0.029921715316327167,0.025485167456207842,0.029357154406695604,0.43401640968407634,0.026509960510426797,0.051213379082552954,0.027412966711723163,0.02649239344411369,0.03544125046823444,0.026112047333152442,0.029191496367534125,0.06768052081173255,0.0282648623978805,0.025071993521938365,0.02841236590720544,0.02250278086524409,0.023298220338391275,0.13842974859809667,0.030963769568548405,0.09713678595391856,0.08895335164194627,0.024351977479720725,0.1916618631057069,0.028717893940511368,0.028959708414109945,0.0346755619206227,0.02941305726922896,0.02358398643369595,0.36471504396050936,0.022682435416850895,0.2965238332485758,0.032946711913183116,0.026269726465399153,0.13671335911427823,0.02766732742355516,0.034039788884078014,0.4422864857390734,0.03006762236967039,0.02948143709833228,0.023532490242860804,0.02401755760258894,0.07991287376735128,0.027271762507738242,0.02117595589737046,0.03007416699865735,0.039507960937696714,0.06682739532693185,0.0338800432935328,0.04178657752080827,0.035599961605907636,0.02801101226419229,0.02917123100065113,0.12471891371898372,0.2728666072360221,0.7033219893543243,0.02154814292880383,0.05811224600927323,0.1578920920568555,0.02816322702726255,0.02968229029031036,0.12004486140098537,0.04376073804376746,0.020980820426297467,0.02738962772208174,0.04677321580049074,0.32719313185166604,0.07543488722302702,0.03807896832210407,0.3129981029920889,0.4524179729993411,0.06449157134596094,0.030270710876051123,0.8251552883203795,0.5148939810169437,0.3696314599449679,0.10172815340989397,0.02112623951410891,0.025336043247483883,0.022667686956027642,0.02852784955384485,0.025759449739974464,0.02686266704489636,0.03543387315239366,0.1914059396750171,0.02801643227157155,0.030242195633630113,0.02670748343458914,0.0228629161597298,0.06265075009881266,0.029267652793227336,0.027773457710946782,0.028293536293045717,0.026811869753519714,0.027596547574248533,0.07535044149485183,0.7611268772746432,0.026191606292145175,0.46497082950881435,0.058906153779436365,0.04815996589849567,0.034219014882326794,0.030380865676250698,0.02994841145541886,0.03988106496735753,0.029914964329762525,0.32060484557823515,0.02763997449206026,0.0323296867013645,0.024883291522968917,0.022965735708180862,0.05244915816733431,0.07584552143269041,0.20771913754033863,0.031643256211459414,0.03335232802617554,0.5612163418426385,0.05898545877380108,0.02435216417688374,0.03795786677789924,0.035574210941335234,0.03163761574763195,0.030730369861200462,0.05281732383389689,0.20047821993550805,0.03370820532274475,0.02543495237779043,0.02510230775031151,0.03214013658961647,0.042878173652642515,0.02456728504823037,0.04389323085050796,0.03876144974980696,0.8628273219489905,0.029747752832944546,0.020587205536132924,0.032318911877295194,0.5423106581415451,0.29446092376329375,0.02262771709914876,0.7995430774811135,0.03607079347022346,0.052792445583121135,0.03041056649796404,0.028564474254159956,0.05786904414803766,0.025379017859899097,0.03583026834988098,0.07815557728034521,0.029142655821948974,0.315590502652336,0.046801920502114895,0.0261711738356039,0.6164500678109747,0.04675742033290651,0.23495436340326747,0.030053788920693148,0.04308331591904738,0.030232792075979977,0.021756007143504206,0.029903780133844288,0.023697363833742473,0.05549982994934333,0.030575812327411835,0.03064642037721906,0.2552757499514908,0.02327157071047955,0.03155973003167183,0.12501775878208912,0.06782957829854386,0.0583568125926066,0.026307554215984963,0.03916580576614716,0.18173379569671666,0.06131669409960414,0.03476991262340338,0.10124267533365741,0.022478398949354075,0.08458777421457922,0.08484042663963219,0.11320566311714375,0.6920149079641036,0.03387106272518402,0.08271398782266184,0.039890234901574034,0.023878760890180255,0.027901345504379293,0.5455495201529621,0.04508434678798222,0.034669086256063804,0.0392612022574165,0.031001499391381027,0.026640317205632737,0.03709008325711288,0.030132249050054467,0.044681798390710875,0.11909945758880496,0.02599575015900152,0.03474935717679533,0.03267055955721105,0.3359241671576085,0.03438343416623382,0.026465806205424023,0.6332577781950626,0.030690283385848355,0.0283733457624431,0.09390991650881214,0.06093042744260395,0.022534578525888943,0.03361816540022931,0.10147929630156691,0.04743773176405499,0.025840266122102974,0.028296904041922942,0.02888363754077635,0.6809798093227312,0.02590154485233615,0.07198256863994668,0.019783083987715982,0.043621732292929496,0.043310903131759164,0.03199840115354995,0.25093991650679875,0.07574966002627921,0.024100644195380667,0.03639048301391915,0.5782577525607477,0.022615127803225263,0.022224773638181906,0.034453562363430314,0.025637395837924726,0.028696116849569745,0.032189160815864815,0.1012600947314642,0.02964851606914975,0.021632501429240582,0.10759293402873595,0.03172797769731792,0.024294307470399713,0.05598334113812091,0.027928415039933548,0.027228431785838756,0.03422523009566733,0.028423879240892798,0.034872490733806515,0.02424888700228307,0.05572996282557564,0.17760426455270545,0.06868832319962724,0.5610862447980547],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "px.histogram(x=probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3cf059a2-dcbc-4e33-a19b-5b0ce3432729",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YMBr_PUFgyle"
      },
      "source": [
        "To evaluate performance of our final model and compare it with others, let's plot our final ROC and P-R curves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bf46852b-fcea-49f6-9cf1-b83a3f9824bd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "pw7imzJvgyle"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fd4f02d3-1ae1-4dd7-a10a-66d3e133d4b5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-tiHNoaugyle",
        "outputId": "a97b933e-16d3-4931-b723-57785c1c1202"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"2d715364-8c6a-465d-92a8-b803e925e8a2\" class=\"plotly-graph-div\" style=\"height:500px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2d715364-8c6a-465d-92a8-b803e925e8a2\")) {                    Plotly.newPlot(                        \"2d715364-8c6a-465d-92a8-b803e925e8a2\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.0005767012687427913,0.0005767012687427913,0.0011534025374855825,0.0011534025374855825,0.0017301038062283738,0.0017301038062283738,0.002306805074971165,0.002306805074971165,0.0028835063437139563,0.0028835063437139563,0.0034602076124567475,0.0034602076124567475,0.004036908881199538,0.004036908881199538,0.00461361014994233,0.00461361014994233,0.0057670126874279125,0.0057670126874279125,0.006343713956170703,0.006343713956170703,0.006920415224913495,0.006920415224913495,0.007497116493656286,0.007497116493656286,0.008073817762399077,0.008073817762399077,0.00865051903114187,0.00865051903114187,0.00980392156862745,0.00980392156862745,0.010957324106113034,0.010957324106113034,0.011534025374855825,0.011534025374855825,0.012110726643598616,0.012110726643598616,0.012687427912341407,0.012687427912341407,0.01384083044982699,0.01384083044982699,0.01787773933102653,0.01787773933102653,0.01845444059976932,0.01845444059976932,0.020184544405997693,0.020184544405997693,0.021337946943483274,0.021337946943483274,0.02422145328719723,0.02422145328719723,0.02768166089965398,0.02768166089965398,0.02883506343713956,0.02883506343713956,0.029411764705882353,0.029411764705882353,0.031141868512110725,0.031141868512110725,0.031718569780853516,0.031718569780853516,0.03690888119953864,0.03690888119953864,0.03748558246828143,0.03748558246828143,0.03806228373702422,0.03806228373702422,0.04209919261822376,0.04209919261822376,0.04555940023068051,0.04555940023068051,0.04844290657439446,0.04844290657439446,0.049019607843137254,0.049019607843137254,0.049596309111880045,0.049596309111880045,0.050173010380622836,0.050173010380622836,0.052479815455594,0.052479815455594,0.05363321799307959,0.05363321799307959,0.05536332179930796,0.05536332179930796,0.05651672433679354,0.05651672433679354,0.05709342560553633,0.05709342560553633,0.06228373702422145,0.06228373702422145,0.06978085351787774,0.06978085351787774,0.07151095732410612,0.07151095732410612,0.07208765859284891,0.07208765859284891,0.07497116493656286,0.07497116493656286,0.07958477508650519,0.07958477508650519,0.0847750865051903,0.0847750865051903,0.08650519031141868,0.08650519031141868,0.08823529411764706,0.08823529411764706,0.10265282583621683,0.10265282583621683,0.11245674740484429,0.11245674740484429,0.11418685121107267,0.11418685121107267,0.12283737024221453,0.12283737024221453,0.12687427912341406,0.12687427912341406,0.13264129181084197,0.13264129181084197,0.13552479815455595,0.13552479815455595,0.14994232987312572,0.14994232987312572,0.15051903114186851,0.15051903114186851,0.1522491349480969,0.1522491349480969,0.15455594002306805,0.15455594002306805,0.15859284890426759,0.15859284890426759,0.1632064590542099,0.1632064590542099,0.16666666666666666,0.16666666666666666,0.16897347174163782,0.16897347174163782,0.20645905420991925,0.20645905420991925,0.22722029988465975,0.22722029988465975,0.22779700115340254,0.22779700115340254,0.23760092272202998,0.23760092272202998,0.23817762399077277,0.23817762399077277,0.2497116493656286,0.2497116493656286,0.25836216839677045,0.25836216839677045,0.26528258362168394,0.26528258362168394,0.2681660899653979,0.2681660899653979,0.27277970011534025,0.27277970011534025,0.3119953863898501,0.3119953863898501,0.3137254901960784,0.3137254901960784,0.31891580161476357,0.31891580161476357,0.3217993079584775,0.3217993079584775,0.3275663206459054,0.3275663206459054,0.328719723183391,0.328719723183391,0.35986159169550175,0.35986159169550175,0.36389850057670126,0.36389850057670126,0.39850057670126876,0.39850057670126876,0.40253748558246827,0.40253748558246827,0.4031141868512111,0.4031141868512111,0.4129181084198385,0.4129181084198385,0.4671280276816609,0.4671280276816609,0.47520184544405997,0.47520184544405997,0.4826989619377163,0.4826989619377163,0.4942329873125721,0.4942329873125721,0.5063437139561707,0.5063437139561707,0.5115340253748558,0.5115340253748558,0.5173010380622838,0.5173010380622838,0.5253748558246828,0.5253748558246828,0.5259515570934256,0.5259515570934256,0.5265282583621684,0.5265282583621684,0.5288350634371396,0.5288350634371396,0.5392156862745098,0.5392156862745098,0.5415224913494809,0.5415224913494809,0.5490196078431373,0.5490196078431373,0.5824682814302191,0.5824682814302191,0.5997693194925029,0.5997693194925029,0.6014994232987313,0.6014994232987313,0.6170703575547867,0.6170703575547867,0.6239907727797002,0.6239907727797002,0.6326412918108419,0.6326412918108419,0.6459054209919262,0.6459054209919262,0.6660899653979239,0.6660899653979239,0.6839677047289504,0.6839677047289504,0.6972318339100346,0.6972318339100346,0.7416378316032295,0.7416378316032295,0.7479815455594002,0.7479815455594002,0.754325259515571,0.754325259515571,0.7837370242214533,0.7837370242214533,0.8148788927335641,0.8148788927335641,0.8235294117647058,0.8235294117647058,0.8269896193771626,0.8269896193771626,0.8512110726643599,0.8512110726643599,0.8546712802768166,0.8546712802768166,0.8662053056516724,0.8662053056516724,0.8690888119953863,0.8690888119953863,0.8910034602076125,0.8910034602076125,0.9273356401384083,0.9273356401384083,0.936562860438293,0.936562860438293,0.9423298731257209,0.9434832756632064,0.9544405997693195,0.9544405997693195,0.9602076124567474,0.9602076124567474,0.9711649365628604,0.9711649365628604,0.9919261822376009,0.9919261822376009,0.9948096885813149,0.9948096885813149,1.0,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.004651162790697674,0.06046511627906977,0.06046511627906977,0.09767441860465116,0.09767441860465116,0.11162790697674418,0.11162790697674418,0.12093023255813953,0.12093023255813953,0.15348837209302327,0.15348837209302327,0.15813953488372093,0.15813953488372093,0.18604651162790697,0.18604651162790697,0.21395348837209302,0.21395348837209302,0.26976744186046514,0.26976744186046514,0.27906976744186046,0.27906976744186046,0.28837209302325584,0.28837209302325584,0.29767441860465116,0.29767441860465116,0.3116279069767442,0.3116279069767442,0.32558139534883723,0.32558139534883723,0.3302325581395349,0.3302325581395349,0.3488372093023256,0.3488372093023256,0.3581395348837209,0.3581395348837209,0.386046511627907,0.386046511627907,0.39069767441860465,0.39069767441860465,0.3953488372093023,0.3953488372093023,0.4046511627906977,0.4046511627906977,0.4232558139534884,0.4232558139534884,0.42790697674418604,0.42790697674418604,0.4418604651162791,0.4418604651162791,0.4511627906976744,0.4511627906976744,0.4558139534883721,0.4558139534883721,0.4604651162790698,0.4604651162790698,0.46511627906976744,0.46511627906976744,0.4744186046511628,0.4744186046511628,0.48372093023255813,0.48372093023255813,0.4883720930232558,0.4883720930232558,0.4930232558139535,0.4930232558139535,0.49767441860465117,0.49767441860465117,0.5023255813953489,0.5023255813953489,0.5069767441860465,0.5069767441860465,0.5116279069767442,0.5116279069767442,0.5255813953488372,0.5255813953488372,0.5302325581395348,0.5302325581395348,0.5348837209302325,0.5348837209302325,0.5395348837209303,0.5395348837209303,0.5441860465116279,0.5441860465116279,0.5534883720930233,0.5534883720930233,0.5581395348837209,0.5581395348837209,0.5627906976744186,0.5627906976744186,0.5674418604651162,0.5674418604651162,0.5720930232558139,0.5720930232558139,0.5767441860465117,0.5767441860465117,0.5813953488372093,0.5813953488372093,0.586046511627907,0.586046511627907,0.5906976744186047,0.5906976744186047,0.5953488372093023,0.5953488372093023,0.6046511627906976,0.6046511627906976,0.6093023255813953,0.6093023255813953,0.6139534883720931,0.6139534883720931,0.6186046511627907,0.6186046511627907,0.6232558139534884,0.6232558139534884,0.6325581395348837,0.6325581395348837,0.6372093023255814,0.6372093023255814,0.641860465116279,0.641860465116279,0.6465116279069767,0.6465116279069767,0.6511627906976745,0.6511627906976745,0.6558139534883721,0.6558139534883721,0.6604651162790698,0.6604651162790698,0.6651162790697674,0.6651162790697674,0.6697674418604651,0.6697674418604651,0.6744186046511628,0.6744186046511628,0.6790697674418604,0.6790697674418604,0.6837209302325581,0.6837209302325581,0.6883720930232559,0.6883720930232559,0.6930232558139535,0.6930232558139535,0.6976744186046512,0.6976744186046512,0.7023255813953488,0.7023255813953488,0.7069767441860465,0.7069767441860465,0.7116279069767442,0.7116279069767442,0.7162790697674418,0.7162790697674418,0.7209302325581395,0.7209302325581395,0.7255813953488373,0.7255813953488373,0.7302325581395349,0.7302325581395349,0.7348837209302326,0.7348837209302326,0.7395348837209302,0.7395348837209302,0.7441860465116279,0.7441860465116279,0.7488372093023256,0.7488372093023256,0.7534883720930232,0.7534883720930232,0.7581395348837209,0.7581395348837209,0.7627906976744186,0.7627906976744186,0.7674418604651163,0.7674418604651163,0.772093023255814,0.772093023255814,0.7767441860465116,0.7767441860465116,0.7813953488372093,0.7813953488372093,0.786046511627907,0.786046511627907,0.7906976744186046,0.7906976744186046,0.7953488372093023,0.7953488372093023,0.8,0.8,0.8046511627906977,0.8046511627906977,0.8093023255813954,0.8093023255813954,0.813953488372093,0.813953488372093,0.8186046511627907,0.8186046511627907,0.8232558139534883,0.8232558139534883,0.827906976744186,0.827906976744186,0.8325581395348837,0.8325581395348837,0.8372093023255814,0.8372093023255814,0.8418604651162791,0.8418604651162791,0.8465116279069768,0.8465116279069768,0.8511627906976744,0.8511627906976744,0.8558139534883721,0.8558139534883721,0.8604651162790697,0.8604651162790697,0.8651162790697674,0.8651162790697674,0.8744186046511628,0.8744186046511628,0.8790697674418605,0.8790697674418605,0.8837209302325582,0.8837209302325582,0.8883720930232558,0.8883720930232558,0.8930232558139535,0.8930232558139535,0.8976744186046511,0.8976744186046511,0.9023255813953488,0.9023255813953488,0.9069767441860465,0.9069767441860465,0.9116279069767442,0.9116279069767442,0.9162790697674419,0.9162790697674419,0.9209302325581395,0.9209302325581395,0.9255813953488372,0.9255813953488372,0.9302325581395349,0.9302325581395349,0.9348837209302325,0.9348837209302325,0.9395348837209302,0.9395348837209302,0.9441860465116279,0.9441860465116279,0.9488372093023256,0.9488372093023256,0.9534883720930233,0.9534883720930233,0.958139534883721,0.958139534883721,0.9627906976744186,0.9627906976744186,0.9674418604651163,0.9674418604651163,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9767441860465116,0.9767441860465116,0.9813953488372092,0.9813953488372092,0.986046511627907,0.986046511627907,0.9906976744186047,0.9906976744186047,0.9953488372093023,0.9953488372093023,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.7996)\"},\"height\":500,\"width\":700,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}]},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test_c, probs)\n",
        "\n",
        "fig = px.area(\n",
        "    x=fpr, y=tpr,\n",
        "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
        "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
        "    width=700, height=500\n",
        ")\n",
        "fig.add_shape(\n",
        "    type='line', line=dict(dash='dash'),\n",
        "    x0=0, x1=1, y0=0, y1=1\n",
        ")\n",
        "\n",
        "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "fig.update_xaxes(constrain='domain')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2ed97ff8-957a-4661-b21c-93ecd456e865",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-NJdM_grgyle",
        "outputId": "84005325-ca70-486e-ed82-834cce9da76b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"c18b1927-6c5b-4331-a22a-c689d628a4f2\" class=\"plotly-graph-div\" style=\"height:500px; width:700px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c18b1927-6c5b-4331-a22a-c689d628a4f2\")) {                    Plotly.newPlot(                        \"c18b1927-6c5b-4331-a22a-c689d628a4f2\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}<br>Precision=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9953488372093023,0.9906976744186047,0.9906976744186047,0.9906976744186047,0.9906976744186047,0.9906976744186047,0.9906976744186047,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.986046511627907,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9813953488372092,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9767441860465116,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9720930232558139,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9674418604651163,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.9627906976744186,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.958139534883721,0.9534883720930233,0.9534883720930233,0.9534883720930233,0.9534883720930233,0.9534883720930233,0.9534883720930233,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9488372093023256,0.9441860465116279,0.9441860465116279,0.9441860465116279,0.9441860465116279,0.9441860465116279,0.9441860465116279,0.9441860465116279,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9395348837209302,0.9348837209302325,0.9348837209302325,0.9348837209302325,0.9348837209302325,0.9348837209302325,0.9348837209302325,0.9348837209302325,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9302325581395349,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9255813953488372,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9209302325581395,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9162790697674419,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9116279069767442,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9069767441860465,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.9023255813953488,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8976744186046511,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8930232558139535,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8883720930232558,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8837209302325582,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8790697674418605,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8744186046511628,0.8697674418604651,0.8651162790697674,0.8651162790697674,0.8651162790697674,0.8651162790697674,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8604651162790697,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8558139534883721,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8511627906976744,0.8465116279069768,0.8465116279069768,0.8465116279069768,0.8465116279069768,0.8465116279069768,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8418604651162791,0.8372093023255814,0.8372093023255814,0.8372093023255814,0.8372093023255814,0.8372093023255814,0.8325581395348837,0.8325581395348837,0.827906976744186,0.827906976744186,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8232558139534883,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.8186046511627907,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.813953488372093,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8093023255813954,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8046511627906977,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.8,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7953488372093023,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.7906976744186046,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.786046511627907,0.7813953488372093,0.7813953488372093,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.7767441860465116,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.772093023255814,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7674418604651163,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7627906976744186,0.7581395348837209,0.7581395348837209,0.7581395348837209,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7534883720930232,0.7488372093023256,0.7488372093023256,0.7488372093023256,0.7488372093023256,0.7488372093023256,0.7488372093023256,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7441860465116279,0.7395348837209302,0.7395348837209302,0.7395348837209302,0.7395348837209302,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7348837209302326,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7302325581395349,0.7255813953488373,0.7255813953488373,0.7255813953488373,0.7255813953488373,0.7255813953488373,0.7255813953488373,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7209302325581395,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7162790697674418,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7116279069767442,0.7069767441860465,0.7069767441860465,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.7023255813953488,0.6976744186046512,0.6976744186046512,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6930232558139535,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6883720930232559,0.6837209302325581,0.6837209302325581,0.6837209302325581,0.6837209302325581,0.6837209302325581,0.6790697674418604,0.6790697674418604,0.6790697674418604,0.6790697674418604,0.6790697674418604,0.6790697674418604,0.6790697674418604,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6744186046511628,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6697674418604651,0.6651162790697674,0.6651162790697674,0.6651162790697674,0.6651162790697674,0.6651162790697674,0.6604651162790698,0.6604651162790698,0.6604651162790698,0.6604651162790698,0.6558139534883721,0.6558139534883721,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6511627906976745,0.6465116279069767,0.6465116279069767,0.6465116279069767,0.6465116279069767,0.6465116279069767,0.6465116279069767,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.641860465116279,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6372093023255814,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.6325581395348837,0.627906976744186,0.6232558139534884,0.6232558139534884,0.6232558139534884,0.6232558139534884,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6186046511627907,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6139534883720931,0.6093023255813953,0.6093023255813953,0.6093023255813953,0.6093023255813953,0.6046511627906976,0.6046511627906976,0.6046511627906976,0.6046511627906976,0.6,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5953488372093023,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.5906976744186047,0.586046511627907,0.586046511627907,0.586046511627907,0.586046511627907,0.586046511627907,0.586046511627907,0.5813953488372093,0.5813953488372093,0.5767441860465117,0.5767441860465117,0.5767441860465117,0.5767441860465117,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5720930232558139,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5674418604651162,0.5627906976744186,0.5627906976744186,0.5581395348837209,0.5581395348837209,0.5581395348837209,0.5534883720930233,0.5534883720930233,0.5534883720930233,0.5534883720930233,0.5488372093023256,0.5441860465116279,0.5441860465116279,0.5441860465116279,0.5395348837209303,0.5395348837209303,0.5395348837209303,0.5395348837209303,0.5395348837209303,0.5348837209302325,0.5348837209302325,0.5302325581395348,0.5302325581395348,0.5255813953488372,0.5255813953488372,0.5209302325581395,0.5162790697674419,0.5116279069767442,0.5116279069767442,0.5116279069767442,0.5116279069767442,0.5116279069767442,0.5116279069767442,0.5069767441860465,0.5069767441860465,0.5069767441860465,0.5069767441860465,0.5069767441860465,0.5069767441860465,0.5069767441860465,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.5023255813953489,0.49767441860465117,0.49767441860465117,0.4930232558139535,0.4930232558139535,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.4883720930232558,0.48372093023255813,0.48372093023255813,0.4790697674418605,0.4744186046511628,0.4744186046511628,0.4744186046511628,0.4744186046511628,0.4697674418604651,0.46511627906976744,0.46511627906976744,0.4604651162790698,0.4604651162790698,0.4604651162790698,0.4558139534883721,0.4558139534883721,0.4558139534883721,0.4558139534883721,0.4558139534883721,0.4558139534883721,0.4558139534883721,0.4511627906976744,0.4511627906976744,0.4511627906976744,0.4511627906976744,0.4511627906976744,0.4511627906976744,0.44651162790697674,0.4418604651162791,0.4418604651162791,0.4418604651162791,0.4372093023255814,0.4325581395348837,0.42790697674418604,0.42790697674418604,0.42790697674418604,0.42790697674418604,0.4232558139534884,0.4232558139534884,0.4186046511627907,0.413953488372093,0.40930232558139534,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4046511627906977,0.4,0.3953488372093023,0.3953488372093023,0.3953488372093023,0.39069767441860465,0.39069767441860465,0.386046511627907,0.386046511627907,0.3813953488372093,0.3767441860465116,0.37209302325581395,0.3674418604651163,0.3627906976744186,0.3581395348837209,0.3581395348837209,0.35348837209302325,0.3488372093023256,0.3488372093023256,0.3488372093023256,0.34418604651162793,0.3395348837209302,0.33488372093023255,0.3302325581395349,0.3302325581395349,0.3302325581395349,0.32558139534883723,0.32558139534883723,0.3209302325581395,0.31627906976744186,0.3116279069767442,0.3116279069767442,0.30697674418604654,0.3023255813953488,0.29767441860465116,0.29767441860465116,0.2930232558139535,0.28837209302325584,0.28837209302325584,0.2837209302325581,0.27906976744186046,0.27906976744186046,0.2744186046511628,0.26976744186046514,0.26976744186046514,0.26976744186046514,0.2651162790697674,0.26046511627906976,0.2558139534883721,0.25116279069767444,0.24651162790697675,0.24186046511627907,0.2372093023255814,0.23255813953488372,0.22790697674418606,0.22325581395348837,0.2186046511627907,0.21395348837209302,0.21395348837209302,0.20930232558139536,0.20465116279069767,0.2,0.19534883720930232,0.19069767441860466,0.18604651162790697,0.18604651162790697,0.1813953488372093,0.17674418604651163,0.17209302325581396,0.16744186046511628,0.16279069767441862,0.15813953488372093,0.15813953488372093,0.15348837209302327,0.15348837209302327,0.14883720930232558,0.14418604651162792,0.13953488372093023,0.13488372093023257,0.13023255813953488,0.12558139534883722,0.12093023255813953,0.12093023255813953,0.11627906976744186,0.11162790697674418,0.11162790697674418,0.10697674418604651,0.10232558139534884,0.09767441860465116,0.09767441860465116,0.09302325581395349,0.08837209302325581,0.08372093023255814,0.07906976744186046,0.07441860465116279,0.06976744186046512,0.06511627906976744,0.06046511627906977,0.06046511627906977,0.05581395348837209,0.05116279069767442,0.046511627906976744,0.04186046511627907,0.037209302325581395,0.03255813953488372,0.027906976744186046,0.023255813953488372,0.018604651162790697,0.013953488372093023,0.009302325581395349,0.004651162790697674,0.0],\"xaxis\":\"x\",\"y\":[0.11031298101590559,0.10985626283367557,0.10991268618387262,0.10996916752312436,0.11002570694087403,0.11008230452674897,0.11013896037056099,0.1101956745623069,0.11025244719216898,0.11030927835051546,0.11036616812790098,0.10990712074303406,0.109963861641714,0.11002066115702479,0.11007751937984496,0.11013443640124095,0.11019141231246767,0.10973084886128365,0.10978767477990678,0.10984455958549223,0.10990150336962157,0.10995850622406639,0.11001556824078879,0.11007268951194185,0.11012987012987013,0.1101871101871102,0.11024440977639105,0.11030176899063475,0.11035918792295679,0.11041666666666666,0.11047420531526837,0.1105318039624609,0.11058946270213876,0.11064718162839249,0.11070496083550914,0.11076280041797283,0.11082070047046524,0.1108786610878661,0.1109366823652538,0.11099476439790576,0.11105290728129912,0.1111111111111111,0.11116937598321971,0.11122770199370409,0.11128608923884514,0.11134453781512606,0.11140304781923278,0.11146161934805468,0.1115202524986849,0.11157894736842106,0.11163770405476567,0.11169652265542676,0.1117554032683184,0.11181434599156118,0.11134564643799472,0.11140443505807814,0.11146328578975172,0.11152219873150106,0.1115811739820201,0.11164021164021164,0.11169931180518793,0.11175847457627118,0.11181770005299417,0.11187698833510074,0.11193633952254642,0.11199575371549894,0.11205523101433883,0.11211477151965994,0.11217437533227007,0.11223404255319148,0.11229377328366152,0.11235356762513313,0.11241342567927544,0.11247334754797442,0.112,0.11205976520811099,0.11211959423384944,0.11217948717948718,0.1122394441475147,0.11229946524064172,0.11235955056179775,0.11241970021413276,0.11247991430101767,0.11254019292604502,0.1126005361930295,0.11212446351931331,0.11218464841653247,0.11224489795918367,0.1123052122514777,0.11236559139784946,0.11242603550295859,0.11248654467168999,0.11254711900915455,0.11260775862068965,0.11266846361185984,0.11272923408845739,0.11279007015650297,0.11285097192224622,0.1129119394921664,0.11297297297297297,0.11303407247160627,0.1130952380952381,0.11315646995127233,0.11321776814734562,0.11327913279132791,0.1134020618556701,0.11346362649294245,0.11352525801195003,0.11358695652173913,0.11364872213159326,0.11371055495103373,0.11377245508982035,0.11383442265795207,0.11389645776566758,0.11395856052344602,0.11402073104200763,0.11353711790393013,0.113599126160568,0.11366120218579236,0.11372334609075997,0.1137855579868709,0.11384783798576902,0.11391018619934283,0.11397260273972602,0.11403508771929824,0.11409764125068568,0.1141602634467618,0.11422295442064799,0.11428571428571428,0.11434854315557999,0.11441144114411442,0.11447440836543754,0.1145374449339207,0.1140495867768595,0.11411245865490628,0.1141753998896856,0.11423841059602649,0.11430149088901159,0.1143646408839779,0.11442786069651742,0.11449115044247787,0.11455451023796348,0.11461794019933555,0.1146814404432133,0.1147450110864745,0.11480865224625623,0.1148723640399556,0.11493614658523042,0.115,0.1150639244024458,0.11512791991101223,0.11519198664440734,0.11525612472160357,0.11532033426183844,0.11538461538461539,0.1154489682097044,0.11551339285714286,0.11557788944723618,0.11564245810055866,0.11570709893795417,0.11577181208053691,0.11583659764969222,0.11590145576707726,0.11596638655462185,0.11603139013452915,0.1160964666292765,0.11616161616161616,0.11622683885457608,0.11629213483146067,0.1163575042158516,0.11642294713160854,0.11648846370287,0.11655405405405406,0.11661971830985915,0.11668545659526494,0.116751269035533,0.11681715575620767,0.11688311688311688,0.11694915254237288,0.11701526286037309,0.1170814479638009,0.11714770797962648,0.11721404303510759,0.11728045325779037,0.11734693877551021,0.11741349971639252,0.11748013620885357,0.11754684838160136,0.11761363636363636,0.11768050028425242,0.11774744027303755,0.11781445645987479,0.11788154897494305,0.11794871794871795,0.11801596351197263,0.11808328579577866,0.11815068493150685,0.11764705882352941,0.11771428571428572,0.11778158947970269,0.11784897025171624,0.1179164281625644,0.11798396334478808,0.1180515759312321,0.11811926605504587,0.11818703384968446,0.1182548794489093,0.1183228029867892,0.11839080459770115,0.11845888441633122,0.11852704257767549,0.11859527921704087,0.11866359447004608,0.11873198847262248,0.11880046136101499,0.11886901327178304,0.11893764434180139,0.11900635470826113,0.11907514450867052,0.11914401388085599,0.11921296296296297,0.11928199189345687,0.11935110081112399,0.11942028985507247,0.11948955916473318,0.1195589088798607,0.11962833914053426,0.11969785008715862,0.11976744186046512,0.1198371146015125,0.119906868451688,0.11997670355270822,0.12004662004662005,0.12011661807580175,0.12018669778296383,0.12025685931115003,0.11974299065420561,0.11981297486849796,0.11988304093567251,0.11995318899941486,0.12002341920374707,0.1200937316930287,0.11957796014067995,0.11964809384164223,0.11971830985915492,0.11978860833822666,0.11985898942420682,0.11992945326278659,0.12,0.12007062978222484,0.12014134275618374,0.12021213906894519,0.12028301886792453,0.12035398230088495,0.1204250295159386,0.12049616066154754,0.12056737588652482,0.12063867534003549,0.12071005917159763,0.12078152753108348,0.12085308056872038,0.12092471843509188,0.12099644128113879,0.12047477744807121,0.12054631828978622,0.12061794414735591,0.1206896551724138,0.1207614515169542,0.12083333333333333,0.1209053007742704,0.12038140643623362,0.12045319022063208,0.12052505966587113,0.12059701492537313,0.12066905615292713,0.12074118350268978,0.12081339712918661,0.12088569718731298,0.12095808383233533,0.12103055721989216,0.1211031175059952,0.12117576484703059,0.1212484993997599,0.12132132132132133,0.12139423076923077,0.12146722790138305,0.12154031287605295,0.12161348585189645,0.1216867469879518,0.12176009644364075,0.1218335343787696,0.12190706095353047,0.12198067632850242,0.12205438066465257,0.12212817412333736,0.12220205686630368,0.12227602905569007,0.12235009085402786,0.12242424242424242,0.12249848392965433,0.12257281553398058,0.12264723740133576,0.1227217496962333,0.12279635258358662,0.12287104622871046,0.12294583079732198,0.12302070645554203,0.12309567336989641,0.12317073170731707,0.12324588163514338,0.12332112332112333,0.12339645693341478,0.1234718826405868,0.12293577981651377,0.1230110159118727,0.12308634415186773,0.12316176470588236,0.12323727774371551,0.12331288343558282,0.12338858195211787,0.12285012285012285,0.12292562999385372,0.12300123001230012,0.12307692307692308,0.12315270935960591,0.12322858903265557,0.12330456226880394,0.12338062924120913,0.12345679012345678,0.12353304508956146,0.12360939431396786,0.12368583797155226,0.12376237623762376,0.1238390092879257,0.12391573729863693,0.12399256044637322,0.12344913151364764,0.12352576039726877,0.12360248447204969,0.12367930391547545,0.12375621890547264,0.1238332296204107,0.12391033623910336,0.12398753894080997,0.12406483790523691,0.12414223331253899,0.12421972534332085,0.12429731417863835,0.124375,0.12445278298936835,0.12453066332916145,0.12460864120225423,0.12468671679197994,0.12476489028213165,0.12484316185696362,0.12492153170119272,0.125,0.12507856693903205,0.12515723270440252,0.1252359974826935,0.1253148614609572,0.12539382482671707,0.12547288776796975,0.12555205047318613,0.12563131313131312,0.12571067593177512,0.12579013906447534,0.12586970271979758,0.1259493670886076,0.1260291323622546,0.12610899873257286,0.12618896639188332,0.12626903553299493,0.12634920634920635,0.12642947903430748,0.12650985378258106,0.12659033078880408,0.12667091024824953,0.1267515923566879,0.1268323773103888,0.12691326530612246,0.12699425654116145,0.12707535121328226,0.12715654952076677,0.1272378516624041,0.127319257837492,0.12740076824583868,0.12748238308776424,0.12756410256410255,0.1276459268762027,0.1277278562259307,0.12716763005780346,0.12724935732647816,0.12733118971061094,0.1274131274131274,0.12749517063747584,0.12757731958762886,0.1276595744680851,0.12774193548387097,0.12782440284054228,0.12790697674418605,0.1279896574014221,0.12807244501940493,0.12815533980582525,0.12823834196891193,0.12832145171743356,0.12840466926070038,0.12848799480856588,0.12857142857142856,0.1286549707602339,0.12873862158647595,0.12882238126219908,0.12890625,0.1289902280130293,0.1290743155149935,0.12915851272015655,0.12924281984334204,0.12932723709993468,0.12941176470588237,0.12949640287769784,0.12958115183246074,0.12966601178781925,0.12975098296199214,0.1298360655737705,0.12992125984251968,0.13000656598818122,0.13009198423127463,0.1301775147928994,0.13026315789473683,0.130348913759052,0.13043478260869565,0.13052076466710613,0.13060686015831136,0.1306930693069307,0.130779392338177,0.13086582947785855,0.13095238095238096,0.13103904698874919,0.13112582781456952,0.1312127236580517,0.1312997347480106,0.13138686131386862,0.13147410358565736,0.13089700996677742,0.1309840425531915,0.13107119095143047,0.1311584553928096,0.1312458361092605,0.13133333333333333,0.1314209472981988,0.1315086782376502,0.13159652638610556,0.13168449197860962,0.13177257525083613,0.1318607764390897,0.13127930341594105,0.13136729222520108,0.13145539906103287,0.13154362416107382,0.13163196776359973,0.13172043010752688,0.13180901143241425,0.13189771197846567,0.13198653198653199,0.1320754716981132,0.13216453135536074,0.13225371120107962,0.13166779203241052,0.13175675675675674,0.13184584178498987,0.13193504736129905,0.13202437373053486,0.13211382113821138,0.13220338983050847,0.1322930800542741,0.13238289205702647,0.1324728260869565,0.13256288239293,0.1326530612244898,0.13274336283185842,0.13283378746594005,0.1329243353783231,0.1330150068212824,0.13310580204778158,0.13319672131147542,0.13328776486671223,0.13337893296853626,0.13347022587268995,0.13356164383561644,0.13365318711446195,0.1337448559670782,0.1338366506520247,0.13392857142857142,0.13402061855670103,0.13411279229711143,0.13420509291121818,0.13429752066115702,0.13439007580978635,0.13448275862068965,0.13457556935817805,0.13466850828729282,0.13476157567380787,0.13485477178423236,0.13494809688581316,0.13504155124653738,0.13513513513513514,0.13522884882108183,0.13532269257460097,0.13541666666666666,0.13551077136900624,0.1356050069541029,0.13569937369519833,0.13579387186629527,0.13588850174216027,0.13598326359832635,0.1360781577110956,0.1361731843575419,0.13626834381551362,0.13636363636363635,0.1364590622813156,0.13655462184873948,0.13665031534688157,0.13674614305750352,0.1368421052631579,0.136938202247191,0.13703443429374562,0.1371308016877637,0.13722730471498945,0.13732394366197184,0.13742071881606766,0.1375176304654443,0.13761467889908258,0.13771186440677965,0.13780918727915195,0.1379066478076379,0.13800424628450106,0.13810198300283286,0.13819985825655565,0.13829787234042554,0.1383960255500355,0.13849431818181818,0.13859275053304904,0.1386913229018492,0.1387900355871886,0.1388888888888889,0.1382751247327156,0.1383737517831669,0.13847251962883655,0.13857142857142857,0.13867047891350964,0.13876967095851217,0.1388690050107373,0.13896848137535817,0.13906810035842293,0.13916786226685796,0.13926776740847094,0.13936781609195403,0.13946800862688713,0.13956834532374102,0.1396688264938805,0.13976945244956773,0.13987022350396539,0.13997113997113997,0.14007220216606497,0.14017341040462428,0.14027476500361533,0.14037626628075253,0.14047791455467054,0.14057971014492754,0.13995649021029732,0.14005805515239478,0.140159767610748,0.14026162790697674,0.14036363636363636,0.14046579330422126,0.14056809905316825,0.14067055393586006,0.14077315827862874,0.14087591240875913,0.14097881665449233,0.14108187134502925,0.14118507681053402,0.14128843338213762,0.1413919413919414,0.14149560117302054,0.14159941305942772,0.14170337738619676,0.14180749448934607,0.14191176470588235,0.14201618837380428,0.14212076583210603,0.14222549742078114,0.14233038348082597,0.14243542435424356,0.14254062038404727,0.1426459719142646,0.14275147928994084,0.14285714285714285,0.14296296296296296,0.1430689399555226,0.14317507418397626,0.14253897550111358,0.1426448736998514,0.14275092936802974,0.14285714285714285,0.14296351451973194,0.14307004470938897,0.14317673378076062,0.14328358208955225,0.14339058999253174,0.14349775784753363,0.14360508601346297,0.1437125748502994,0.14382022471910114,0.14392803598200898,0.14403600900225055,0.14414414414414414,0.14425244177310292,0.1443609022556391,0.14446952595936793,0.14457831325301204,0.14468726450640543,0.14479638009049775,0.1449056603773585,0.14501510574018128,0.14512471655328799,0.14523449319213314,0.1453444360333081,0.14545454545454545,0.14556482183472327,0.1456752655538695,0.14578587699316628,0.1458966565349544,0.14600760456273765,0.1461187214611872,0.14623000761614624,0.14634146341463414,0.14569031273836766,0.14580152671755725,0.14591291061879297,0.14602446483180428,0.14613618974751338,0.14624808575803983,0.14636015325670498,0.1464723926380368,0.14658480429777437,0.14669738863287252,0.14681014604150652,0.14692307692307693,0.147036181678214,0.14714946070878274,0.14726291441788744,0.14737654320987653,0.14749034749034748,0.14760432766615147,0.1477184841453983,0.14783281733746131,0.14794732765298219,0.14806201550387596,0.14817688130333592,0.1482919254658385,0.14763014763014762,0.14774494556765164,0.14785992217898833,0.14797507788161993,0.1480904130943102,0.1482059282371295,0.1483216237314598,0.1484375,0.1485535574667709,0.1486697965571205,0.14878621769772904,0.14890282131661442,0.14901960784313725,0.14913657770800628,0.14925373134328357,0.14937106918238993,0.14870180959874116,0.14881889763779527,0.14893617021276595,0.14905362776025236,0.14917127071823205,0.14928909952606634,0.14940711462450593,0.1495253164556962,0.1496437054631829,0.14976228209191758,0.14988104678826328,0.15,0.15011914217633043,0.1494435612082671,0.14956245027844073,0.14968152866242038,0.149800796812749,0.14992025518341306,0.15003990422984836,0.1501597444089457,0.15027977617905675,0.1504,0.15052041633306645,0.15064102564102563,0.1507618283881315,0.1508828250401284,0.15100401606425704,0.15112540192926044,0.1512469831053902,0.15136876006441224,0.1514907332796132,0.15161290322580645,0.15173527037933818,0.1518578352180937,0.15198059822150364,0.15210355987055016,0.1522267206477733,0.15235008103727715,0.15247364152473641,0.1525974025974026,0.15272136474411047,0.15203252032520326,0.15134255492270138,0.15146579804560262,0.15158924205378974,0.15171288743882544,0.1510204081632653,0.1511437908496732,0.15126737530662307,0.1513911620294599,0.15151515151515152,0.15163934426229508,0.15176374077112387,0.15188834154351397,0.15201314708299096,0.15213815789473684,0.1522633744855967,0.15238879736408567,0.15251442704039572,0.15264026402640263,0.152766308835673,0.15289256198347106,0.15301902398676592,0.15314569536423842,0.15327257663628832,0.15339966832504145,0.15352697095435686,0.1536544850498339,0.15378221113881962,0.15391014975041598,0.1540383014154871,0.15416666666666667,0.1542952460383653,0.15442404006677796,0.1545530492898914,0.15468227424749165,0.15481171548117154,0.1541038525963149,0.15423302598491198,0.15436241610738255,0.15449202350965574,0.1546218487394958,0.15475189234650968,0.15488215488215487,0.15501263689974726,0.1551433389544688,0.15527426160337554,0.1554054054054054,0.15553677092138632,0.155668358714044,0.15580016934801016,0.15593220338983052,0.15606446140797287,0.15619694397283532,0.15632965165675447,0.1564625850340136,0.15659574468085105,0.1567291311754685,0.1568627450980392,0.15699658703071673,0.15713065755764305,0.15726495726495726,0.15739948674080412,0.15753424657534246,0.15766923736075408,0.15780445969125215,0.15793991416309014,0.15807560137457044,0.1582115219260533,0.15834767641996558,0.15848406546080965,0.15862068965517243,0.15875754961173424,0.15889464594127806,0.15903197925669835,0.15916955017301038,0.15930735930735931,0.15944540727902945,0.1595836947094536,0.1597222222222222,0.15986099044309296,0.16,0.16013925152306355,0.1602787456445993,0.16041848299912817,0.16055846422338568,0.16069868995633188,0.16083916083916083,0.1609798775153106,0.16112084063047286,0.16126205083260298,0.16140350877192983,0.16154521510096576,0.1616871704745167,0.16182937554969218,0.1619718309859155,0.16123348017621145,0.16137566137566137,0.1615180935569285,0.16166077738515902,0.16180371352785147,0.16194690265486725,0.16209034543844109,0.1622340425531915,0.16237799467613132,0.1625222024866785,0.16266666666666665,0.16281138790035588,0.16295636687444345,0.16310160427807488,0.1623550401427297,0.1625,0.16264521894548703,0.16279069767441862,0.1629364368845121,0.16218637992831542,0.16233183856502242,0.16247755834829444,0.16262353998203055,0.16276978417266186,0.1629162916291629,0.16306306306306306,0.16321009918845808,0.16335740072202165,0.16350496838301717,0.16365280289330922,0.16380090497737557,0.16394927536231885,0.16409791477787852,0.16424682395644283,0.16439600363306087,0.16454545454545455,0.16469517743403095,0.16484517304189436,0.1640838650865998,0.16423357664233576,0.1643835616438356,0.16453382084095064,0.16468435498627632,0.16391941391941392,0.16406966086159486,0.163302752293578,0.16345270890725436,0.16268382352941177,0.16283348666053357,0.16298342541436464,0.1631336405529954,0.16328413284132842,0.1634349030470914,0.16358595194085027,0.1637372802960222,0.1638888888888889,0.16404077849860982,0.16419294990723562,0.16434540389972144,0.16449814126394052,0.16465116279069766,0.164804469273743,0.16402609506057783,0.16417910447761194,0.16433239962651727,0.16448598130841122,0.1646398503274088,0.1647940074906367,0.16494845360824742,0.1651031894934334,0.1652582159624413,0.16541353383458646,0.16556914393226718,0.1647834274952919,0.1649387370405278,0.1650943396226415,0.1652502360717658,0.16540642722117202,0.16556291390728478,0.16571969696969696,0.16587677725118483,0.16603415559772297,0.16619183285849953,0.16539923954372623,0.16555661274976213,0.1657142857142857,0.16587225929456625,0.16603053435114504,0.166189111747851,0.16634799235181644,0.16650717703349283,0.16666666666666666,0.16682646212847554,0.16698656429942418,0.16714697406340057,0.1673076923076923,0.16746871992300288,0.1676300578034682,0.1677917068466731,0.16795366795366795,0.1681159420289855,0.16827852998065765,0.16844143272023232,0.1686046511627907,0.1687681862269641,0.1679611650485437,0.16812439261418854,0.16828793774319067,0.16845180136319376,0.16861598440545808,0.16878048780487806,0.1689453125,0.16911045943304007,0.16927592954990214,0.1694417238001959,0.1696078431372549,0.16977428851815504,0.16994106090373282,0.17010816125860373,0.1702755905511811,0.17044334975369457,0.17061143984220908,0.17077986179664364,0.17094861660079053,0.17111770524233433,0.1712871287128713,0.17046580773042616,0.17063492063492064,0.1708043694141013,0.1709741550695825,0.17114427860696518,0.17131474103585656,0.17148554336989033,0.17165668662674652,0.17182817182817184,0.172,0.17217217217217218,0.17234468937875752,0.17251755265797392,0.17269076305220885,0.17185929648241205,0.1720321931589537,0.17220543806646527,0.17237903225806453,0.17255297679112008,0.17272727272727273,0.17290192113245703,0.17307692307692307,0.17325227963525835,0.17342799188640973,0.1736040609137056,0.17378048780487804,0.17395727365208546,0.17413441955193482,0.1743119266055046,0.17346938775510204,0.17364657814096016,0.1738241308793456,0.17400204708290687,0.17418032786885246,0.17435897435897435,0.17453798767967146,0.1747173689619733,0.1748971193415638,0.17507723995880536,0.17525773195876287,0.17543859649122806,0.1756198347107438,0.17580144777662876,0.17598343685300208,0.17616580310880828,0.17634854771784234,0.17653167185877466,0.17671517671517672,0.1768990634755463,0.17708333333333334,0.1772679874869656,0.17745302713987474,0.17763845350052246,0.17782426778242677,0.17801047120418848,0.17819706498951782,0.1783840503672613,0.17857142857142858,0.17875920084121977,0.17894736842105263,0.1791359325605901,0.17932489451476794,0.1795142555438226,0.17970401691331925,0.17989417989417988,0.18008474576271186,0.18027571580063625,0.18046709129511676,0.1806588735387885,0.18085106382978725,0.1810436634717785,0.1812366737739872,0.1814300960512273,0.18162393162393162,0.18181818181818182,0.18201284796573874,0.18220793140407288,0.18240343347639484,0.18259935553168635,0.1827956989247312,0.18299246501614638,0.18318965517241378,0.18338727076591155,0.183585313174946,0.1837837837837838,0.18398268398268397,0.18418201516793067,0.1843817787418655,0.18458197611292074,0.18478260869565216,0.18498367791077258,0.18518518518518517,0.18538713195201745,0.185589519650655,0.18579234972677597,0.18599562363238512,0.18619934282584885,0.18640350877192982,0.18660812294182216,0.18681318681318682,0.18701870187018702,0.18722466960352424,0.1874310915104741,0.18763796909492272,0.1878453038674033,0.18805309734513273,0.18826135105204872,0.188470066518847,0.18867924528301888,0.18888888888888888,0.18909899888765294,0.18930957683741648,0.189520624303233,0.18973214285714285,0.18994413407821228,0.19015659955257272,0.19036954087346025,0.1905829596412556,0.19079685746352412,0.19101123595505617,0.19122609673790777,0.19144144144144143,0.19165727170236754,0.19187358916478556,0.19096045197740114,0.19117647058823528,0.1913929784824462,0.19160997732426305,0.19182746878547105,0.19204545454545455,0.19226393629124006,0.19248291571753987,0.19270239452679588,0.19292237442922375,0.19314285714285714,0.19336384439359267,0.19358533791523483,0.1938073394495413,0.19402985074626866,0.19425287356321838,0.19447640966628307,0.19470046082949308,0.19377162629757785,0.19399538106235567,0.1930635838150289,0.19328703703703703,0.19351100811123986,0.1937354988399072,0.19396051103368175,0.1941860465116279,0.19441210710128057,0.19463869463869463,0.19369894982497082,0.1939252336448598,0.19415204678362574,0.19437939110070257,0.19460726846424384,0.19483568075117372,0.19506462984723855,0.1952941176470588,0.1955241460541814,0.1957547169811321,0.1959858323494687,0.19621749408983452,0.19644970414201182,0.1966824644549763,0.19691577698695137,0.19714964370546317,0.19738406658739596,0.1976190476190476,0.19785458879618595,0.19809069212410502,0.1983273596176822,0.19856459330143542,0.19880239520958085,0.19904076738609114,0.1992797118847539,0.19951923076923078,0.19975932611311673,0.2,0.20024125452352232,0.20048309178743962,0.2007255139056832,0.2009685230024213,0.2012121212121212,0.20145631067961164,0.20170109356014582,0.20194647201946472,0.20219244823386115,0.20243902439024392,0.2026862026862027,0.20293398533007334,0.20318237454100369,0.2034313725490196,0.20368098159509201,0.20393120393120392,0.20418204182041821,0.2044334975369458,0.20468557336621454,0.20493827160493827,0.20519159456118666,0.20544554455445543,0.2057001239157373,0.20595533498759305,0.2062111801242236,0.2064676616915423,0.20672478206724781,0.20698254364089774,0.20724094881398253,0.2075,0.20775969962453067,0.20802005012531327,0.20828105395232122,0.20728643216080403,0.20754716981132076,0.20780856423173805,0.2080706179066835,0.20833333333333334,0.20859671302149177,0.2088607594936709,0.20912547528517111,0.20812182741116753,0.20838627700127066,0.20865139949109415,0.2089171974522293,0.20918367346938777,0.20945083014048532,0.20971867007672634,0.20998719590268886,0.21025641025641026,0.21052631578947367,0.21079691516709512,0.21106821106821108,0.211340206185567,0.21161290322580645,0.21188630490956073,0.21216041397153945,0.21243523316062177,0.2127107652399481,0.21298701298701297,0.21326397919375814,0.21354166666666666,0.2138200782268579,0.21409921671018275,0.21437908496732025,0.21465968586387435,0.21494102228047182,0.2152230971128609,0.21550591327201052,0.21578947368421053,0.2160737812911726,0.21635883905013192,0.2166446499339498,0.21693121693121692,0.21721854304635763,0.21750663129973474,0.21779548472775564,0.21808510638297873,0.21837549933422104,0.21866666666666668,0.21895861148197596,0.2192513368983957,0.21954484605087016,0.21983914209115282,0.22013422818791947,0.22043010752688172,0.22072678331090176,0.2210242587601078,0.2213225371120108,0.22162162162162163,0.2219215155615697,0.2222222222222222,0.2225237449118046,0.22282608695652173,0.22312925170068026,0.22343324250681199,0.22237380627557982,0.2226775956284153,0.22298221614227087,0.2219178082191781,0.2222222222222222,0.22252747252747251,0.22283356258596973,0.2231404958677686,0.22344827586206897,0.22375690607734808,0.22406639004149378,0.22437673130193905,0.22468793342579751,0.225,0.2239221140472879,0.22423398328690808,0.22454672245467225,0.22486033519553073,0.22517482517482518,0.22549019607843138,0.2244039270687237,0.2247191011235955,0.2250351617440225,0.22535211267605634,0.22566995768688294,0.22598870056497175,0.2263083451202263,0.22662889518413598,0.22695035460992907,0.22727272727272727,0.22617354196301565,0.2264957264957265,0.2268188302425107,0.22714285714285715,0.22603719599427755,0.22636103151862463,0.2266857962697274,0.22701149425287356,0.2273381294964029,0.2276657060518732,0.227994227994228,0.22832369942196531,0.22865412445730826,0.2289855072463768,0.22931785195936139,0.22965116279069767,0.22998544395924309,0.2303206997084548,0.23065693430656933,0.2309941520467836,0.23133235724743778,0.2316715542521994,0.23201174743024963,0.2323529411764706,0.23269513991163476,0.23303834808259588,0.23338257016248154,0.23372781065088757,0.23407407407407407,0.2344213649851632,0.23476968796433878,0.23511904761904762,0.23546944858420268,0.23582089552238805,0.23617339312406577,0.23652694610778444,0.2368815592203898,0.23723723723723725,0.23759398496240602,0.23795180722891565,0.2383107088989442,0.23867069486404835,0.23903177004538578,0.23939393939393938,0.23975720789074356,0.24012158054711247,0.2404870624048706,0.24085365853658536,0.24122137404580152,0.2415902140672783,0.24196018376722817,0.24233128834355827,0.24270353302611367,0.24307692307692308,0.24345146379044685,0.24382716049382716,0.24420401854714066,0.24458204334365324,0.24496124031007752,0.2453416149068323,0.24572317262830481,0.24610591900311526,0.24648985959438377,0.246875,0.24726134585289514,0.2476489028213166,0.24803767660910517,0.24842767295597484,0.24881889763779527,0.24921135646687698,0.24960505529225907,0.25,0.25039619651347067,0.2492063492063492,0.24960254372019078,0.25,0.2503987240829346,0.2507987220447284,0.2512,0.2516025641025641,0.2520064205457464,0.25241157556270094,0.25120772946859904,0.25161290322580643,0.2520193861066236,0.2524271844660194,0.25283630470016205,0.2532467532467532,0.25203252032520324,0.252442996742671,0.2528548123980424,0.25326797385620914,0.25368248772504093,0.2540983606557377,0.2545155993431856,0.25493421052631576,0.2553542009884679,0.25577557755775576,0.256198347107438,0.25662251655629137,0.2570480928689884,0.2558139534883721,0.2562396006655574,0.25666666666666665,0.2570951585976628,0.25752508361204013,0.25795644891122277,0.25838926174496646,0.25882352941176473,0.25925925925925924,0.2596964586846543,0.26013513513513514,0.26057529610829105,0.26101694915254237,0.2614601018675722,0.2619047619047619,0.262350936967632,0.26109215017064846,0.26153846153846155,0.261986301369863,0.2624356775300172,0.26288659793814434,0.2633390705679862,0.2637931034482759,0.26424870466321243,0.2647058823529412,0.2651646447140381,0.265625,0.26608695652173914,0.2665505226480836,0.2670157068062827,0.2674825174825175,0.2679509632224168,0.26842105263157895,0.2688927943760984,0.26936619718309857,0.2698412698412698,0.2703180212014134,0.26902654867256637,0.2695035460992908,0.2682060390763766,0.26868327402135234,0.26916221033868093,0.26964285714285713,0.2701252236135957,0.27060931899641577,0.2710951526032316,0.2715827338129496,0.27207207207207207,0.27256317689530685,0.2730560578661845,0.27355072463768115,0.27404718693284935,0.27454545454545454,0.2750455373406193,0.2755474452554745,0.2760511882998172,0.2765567765567766,0.27522935779816515,0.2757352941176471,0.27440147329650094,0.27490774907749077,0.2754158964879852,0.2759259259259259,0.2764378478664193,0.27695167286245354,0.2774674115456238,0.27798507462686567,0.27850467289719627,0.27902621722846443,0.2795497185741088,0.2800751879699248,0.2806026365348399,0.2811320754716981,0.28166351606805295,0.2821969696969697,0.2827324478178368,0.2832699619771863,0.2838095238095238,0.28435114503816794,0.28489483747609945,0.28544061302681994,0.28598848368522073,0.2865384615384615,0.28709055876685935,0.2876447876447876,0.28820116054158607,0.28875968992248063,0.28932038834951457,0.2898832684824903,0.290448343079922,0.291015625,0.29158512720156554,0.29215686274509806,0.29273084479371314,0.2933070866141732,0.2938856015779093,0.2924901185770751,0.29306930693069305,0.29365079365079366,0.294234592445328,0.2948207171314741,0.2954091816367265,0.296,0.2965931863727455,0.2971887550200803,0.2977867203219316,0.29838709677419356,0.298989898989899,0.29959514170040485,0.3002028397565923,0.3008130081300813,0.3014256619144603,0.3020408163265306,0.30265848670756645,0.30327868852459017,0.30390143737166325,0.3045267489711934,0.30515463917525776,0.30578512396694213,0.3064182194616977,0.3070539419087137,0.3076923076923077,0.30833333333333335,0.3089770354906054,0.30962343096234307,0.31027253668763105,0.31092436974789917,0.31157894736842107,0.31223628691983124,0.3128964059196617,0.3135593220338983,0.31422505307855625,0.3148936170212766,0.31556503198294245,0.3162393162393162,0.3169164882226981,0.31759656652360513,0.31827956989247314,0.31896551724137934,0.31965442764578833,0.3203463203463203,0.3210412147505423,0.3217391304347826,0.3224400871459695,0.3231441048034934,0.3238512035010941,0.32456140350877194,0.3252747252747253,0.32599118942731276,0.32671081677704195,0.3274336283185841,0.328159645232816,0.3288888888888889,0.32962138084632514,0.33035714285714285,0.33109619686800895,0.33183856502242154,0.3325842696629214,0.3333333333333333,0.3340857787810384,0.334841628959276,0.3356009070294785,0.3340909090909091,0.3348519362186788,0.3356164383561644,0.33638443935926776,0.33715596330275227,0.335632183908046,0.33640552995391704,0.3371824480369515,0.33796296296296297,0.33874709976798145,0.3395348837209302,0.34032634032634035,0.338785046728972,0.3395784543325527,0.3403755868544601,0.3411764705882353,0.3419811320754717,0.34278959810874704,0.34360189573459715,0.34441805225653205,0.34523809523809523,0.3436754176610978,0.3444976076555024,0.34532374100719426,0.34615384615384615,0.3469879518072289,0.34782608695652173,0.3486682808716707,0.34951456310679613,0.34793187347931875,0.348780487804878,0.34963325183374083,0.35049019607843135,0.35135135135135137,0.3497536945812808,0.3506172839506173,0.35148514851485146,0.3523573200992556,0.35074626865671643,0.3516209476309227,0.35,0.3508771929824561,0.35175879396984927,0.3526448362720403,0.35353535353535354,0.35443037974683544,0.3553299492385787,0.356234096692112,0.35714285714285715,0.35805626598465473,0.358974358974359,0.35989717223650386,0.36082474226804123,0.36175710594315247,0.3626943005181347,0.36363636363636365,0.3645833333333333,0.36553524804177545,0.36649214659685864,0.3674540682414698,0.3684210526315789,0.36939313984168864,0.37037037037037035,0.3713527851458886,0.3723404255319149,0.37333333333333335,0.3716577540106952,0.3726541554959786,0.3736559139784946,0.3746630727762803,0.37567567567567567,0.37669376693766937,0.375,0.3760217983651226,0.3770491803278688,0.3780821917808219,0.3791208791208791,0.38016528925619836,0.3812154696132597,0.38227146814404434,0.38333333333333336,0.38440111420612816,0.3854748603351955,0.38375350140056025,0.3848314606741573,0.38591549295774646,0.3870056497175141,0.3881019830028329,0.38920454545454547,0.3903133903133903,0.3914285714285714,0.38968481375358166,0.39080459770114945,0.3919308357348703,0.3930635838150289,0.39420289855072466,0.3953488372093023,0.3965014577259475,0.39766081871345027,0.39882697947214074,0.4,0.40117994100294985,0.40236686390532544,0.4035608308605341,0.40476190476190477,0.4059701492537313,0.40718562874251496,0.40540540540540543,0.4036144578313253,0.40483383685800606,0.40606060606060607,0.4072948328267477,0.4054878048780488,0.40672782874617736,0.40797546012269936,0.40923076923076923,0.4104938271604938,0.4117647058823529,0.41304347826086957,0.4143302180685358,0.415625,0.4169278996865204,0.41823899371069184,0.4195583596214511,0.4208860759493671,0.4222222222222222,0.42356687898089174,0.4249201277955272,0.42628205128205127,0.42765273311897106,0.4258064516129032,0.42718446601941745,0.42857142857142855,0.42996742671009774,0.43137254901960786,0.43278688524590164,0.4342105263157895,0.43564356435643564,0.4370860927152318,0.43853820598006643,0.44,0.4414715719063545,0.4429530201342282,0.4444444444444444,0.44594594594594594,0.44745762711864406,0.4489795918367347,0.45051194539249145,0.4520547945205479,0.4536082474226804,0.45517241379310347,0.45674740484429066,0.4583333333333333,0.45993031358885017,0.46153846153846156,0.4631578947368421,0.4612676056338028,0.4628975265017668,0.4645390070921986,0.46619217081850534,0.4642857142857143,0.4659498207885305,0.4676258992805755,0.4693140794223827,0.4673913043478261,0.46545454545454545,0.46715328467153283,0.46886446886446886,0.47058823529411764,0.47232472324723246,0.4740740740740741,0.4758364312267658,0.47761194029850745,0.4794007490636704,0.48120300751879697,0.47924528301886793,0.4810606060606061,0.4828897338403042,0.4847328244274809,0.48659003831417624,0.48846153846153845,0.49034749034749037,0.49224806201550386,0.49416342412451364,0.4921875,0.49411764705882355,0.49606299212598426,0.4980237154150198,0.5,0.50199203187251,0.5,0.5020080321285141,0.5,0.5020242914979757,0.5040650406504065,0.5061224489795918,0.5040983606557377,0.5061728395061729,0.5082644628099173,0.5103734439834025,0.5125,0.5146443514644351,0.5168067226890757,0.5189873417721519,0.5211864406779662,0.5234042553191489,0.5256410256410257,0.5278969957081545,0.5301724137931034,0.5324675324675324,0.5304347826086957,0.5327510917030568,0.5350877192982456,0.5374449339207048,0.5398230088495575,0.5422222222222223,0.5446428571428571,0.547085201793722,0.5495495495495496,0.5520361990950227,0.55,0.5525114155251142,0.5504587155963303,0.5529953917050692,0.5555555555555556,0.5534883720930233,0.5560747663551402,0.5586854460093896,0.5613207547169812,0.5592417061611374,0.5571428571428572,0.5598086124401914,0.5625,0.5603864734299517,0.5631067961165048,0.5658536585365853,0.5686274509803921,0.5714285714285714,0.5693069306930693,0.572139303482587,0.57,0.5728643216080402,0.5707070707070707,0.5736040609137056,0.5714285714285714,0.5692307692307692,0.5670103092783505,0.5699481865284974,0.5729166666666666,0.5759162303664922,0.5789473684210527,0.582010582010582,0.5797872340425532,0.5828877005347594,0.5860215053763441,0.5891891891891892,0.592391304347826,0.5956284153005464,0.5989010989010989,0.5966850828729282,0.6,0.6033519553072626,0.6067415730337079,0.6101694915254238,0.6136363636363636,0.6171428571428571,0.6206896551724138,0.6184971098265896,0.622093023255814,0.6198830409356725,0.6235294117647059,0.621301775147929,0.625,0.6287425149700598,0.6325301204819277,0.6363636363636364,0.6402439024390244,0.6441717791411042,0.6481481481481481,0.6521739130434783,0.65625,0.6540880503144654,0.6582278481012658,0.6560509554140127,0.6538461538461539,0.6580645161290323,0.6623376623376623,0.6666666666666666,0.6644736842105263,0.6622516556291391,0.6666666666666666,0.6644295302013423,0.668918918918919,0.673469387755102,0.6712328767123288,0.6758620689655173,0.6805555555555556,0.6853146853146853,0.6901408450704225,0.6950354609929078,0.7,0.697841726618705,0.7028985507246377,0.708029197080292,0.7132352941176471,0.7185185185185186,0.7238805970149254,0.7218045112781954,0.7196969696969697,0.7251908396946565,0.7307692307692307,0.7286821705426356,0.7265625,0.7244094488188977,0.7301587301587301,0.736,0.7419354838709677,0.7398373983739838,0.7459016393442623,0.743801652892562,0.7416666666666667,0.7394957983193278,0.7372881355932204,0.7435897435897436,0.75,0.7565217391304347,0.7631578947368421,0.7699115044247787,0.7767857142857143,0.7837837837837838,0.7818181818181819,0.7798165137614679,0.7870370370370371,0.794392523364486,0.7924528301886793,0.8,0.7980769230769231,0.8058252427184466,0.803921568627451,0.801980198019802,0.8,0.797979797979798,0.7959183673469388,0.7938144329896907,0.8020833333333334,0.8,0.7978723404255319,0.8064516129032258,0.8152173913043478,0.8131868131868132,0.8111111111111111,0.8089887640449438,0.8068181818181818,0.8160919540229885,0.8255813953488372,0.8235294117647058,0.8333333333333334,0.8313253012048193,0.8292682926829268,0.8271604938271605,0.8375,0.8354430379746836,0.8333333333333334,0.8311688311688312,0.8421052631578947,0.84,0.8378378378378378,0.8493150684931506,0.8472222222222222,0.8450704225352113,0.8571428571428571,0.855072463768116,0.8529411764705882,0.8656716417910447,0.8787878787878788,0.8769230769230769,0.875,0.873015873015873,0.8709677419354839,0.8688524590163934,0.8666666666666667,0.864406779661017,0.8620689655172413,0.8596491228070176,0.8571428571428571,0.8545454545454545,0.8518518518518519,0.8679245283018868,0.8653846153846154,0.8627450980392157,0.86,0.8571428571428571,0.8541666666666666,0.851063829787234,0.8695652173913043,0.8666666666666667,0.8636363636363636,0.8604651162790697,0.8571428571428571,0.8536585365853658,0.85,0.8717948717948718,0.868421052631579,0.8918918918918919,0.8888888888888888,0.8857142857142857,0.8823529411764706,0.8787878787878788,0.875,0.8709677419354839,0.8666666666666667,0.896551724137931,0.8928571428571429,0.8888888888888888,0.9230769230769231,0.92,0.9166666666666666,0.9130434782608695,0.9545454545454546,0.9523809523809523,0.95,0.9473684210526315,0.9444444444444444,0.9411764705882353,0.9375,0.9333333333333333,0.9285714285714286,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.5643)\"},\"height\":500,\"width\":700,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}]},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test_c, probs)\n",
        "\n",
        "fig = px.area(\n",
        "    x=recall, y=precision,\n",
        "    title=f'Precision-Recall Curve (AUC={auc(recall, precision):.4f})',\n",
        "    labels=dict(x='Recall', y='Precision'),\n",
        "    width=700, height=500\n",
        ")\n",
        "fig.add_shape(\n",
        "    type='line', line=dict(dash='dash'),\n",
        "    x0=0, x1=1, y0=1, y1=0\n",
        ")\n",
        "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
        "fig.update_xaxes(constrain='domain')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "46bd5a70-c485-458e-92a2-c092bdb13fcd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "N_annfTzgylf"
      },
      "source": [
        "We conclude the testing phase by updating our running comparison of model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ec28140b-de62-4116-b4b3-a7d44c8e851e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "z3ukvOYjgylf",
        "outputId": "f51f9f10-0150-49bd-afba-374a2506fdd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>PR AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic</td>\n",
              "      <td>0.831195</td>\n",
              "      <td>0.358209</td>\n",
              "      <td>0.669767</td>\n",
              "      <td>0.548121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Elastic Net</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.382271</td>\n",
              "      <td>0.641860</td>\n",
              "      <td>0.564333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Method  Accuracy  Precision    Recall    PR AUC\n",
              "0     Logistic  0.831195   0.358209  0.669767  0.548121\n",
              "1  Elastic Net  0.846075   0.382271  0.641860  0.564333"
            ]
          },
          "execution_count": 281,
          "metadata": {}
        }
      ],
      "source": [
        "Final_Metrics = pd.DataFrame({'Method':['Logistic','Elastic Net'],'Accuracy':[0.831195,acc_lrc1],'Precision':[0.358209,prec_lrc1],'Recall':[0.669767,sens_lrc1],'PR AUC':[0.548121,auc(recall, precision)]})\n",
        "Final_Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1d774c80-ce89-488c-8e8d-16549f5a5162",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ScAx77YDgylf"
      },
      "source": [
        "These metrics may be utilized in two primary ways\n",
        "\n",
        "  1. As performance metrics we can compare with those from other models to make a final choice on which to deploy.\n",
        "\n",
        "  We see here that the KNN model...\n",
        "  \n",
        "  2. By comparing test set metric values with those from the training set, close values indicate the ability of our model to generalize.\n",
        "\n",
        "  Test metrics are only slightly worse than training metrics, indicating both models are useful for generalizing to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1994bc8b-7398-46d6-886f-3dfdb36fe431",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "keA7W8Eugylf"
      },
      "source": [
        "#VIDEO 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c30a20d1-17ce-46bf-88da-b1f420bd879c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cOpnLar_wpjJ"
      },
      "source": [
        "##Deploying the Data for Impact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "58b60c5f-e962-4955-8339-1d886ed84fd5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sSs0EbVyEXdR"
      },
      "source": [
        "###Predicting Outcomes for new Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e494fd19-9a97-420d-b1dc-714c1714fc66",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "g8UO4pbBgylg"
      },
      "source": [
        "With the trained, tested and validated model in hand, Shontelle is ready to utilize it to glean insights concerning a random sample of students who have shown various signs of struggling in their university experience. She then downloads a spreadsheet entitled **predict.csv**. Let's take a look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9b15ce84-4d8e-4d03-98f7-920434cf18e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "H-dvvH4Sgylg",
        "outputId": "981b7bae-2327-4399-85ba-c16368b76cbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JHPSY555D</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.97</td>\n",
              "      <td>4.09</td>\n",
              "      <td>Business</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.666667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9KC4NM2YV</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.77</td>\n",
              "      <td>Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33M8O2J01</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.78</td>\n",
              "      <td>3.19</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AMX4WP4A0</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>4.02</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Science</td>\n",
              "      <td>13.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.615385</td>\n",
              "      <td>2.266667</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R32ET2VTA</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.58</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14359</th>\n",
              "      <td>BXXXVTVLJ</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.98</td>\n",
              "      <td>3.84</td>\n",
              "      <td>Health</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14360</th>\n",
              "      <td>WPFBYCAF2</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.79</td>\n",
              "      <td>3.61</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14361</th>\n",
              "      <td>9DB0QFR11</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.54</td>\n",
              "      <td>3.89</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.923077</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14362</th>\n",
              "      <td>F7Z5NC62T</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.11</td>\n",
              "      <td>3.37</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.416667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14363</th>\n",
              "      <td>KDYYXRZTL</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.43</td>\n",
              "      <td>3.66</td>\n",
              "      <td>Liberal Arts</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NR</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5725 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             SID     COHORT   RACE_ETHNICITY  GENDER       FIRST_GEN_STATUS  \\\n",
              "0      JHPSY555D  Fall 2023  Hispanic/Latino  Female                Unknown   \n",
              "1      9KC4NM2YV  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "2      33M8O2J01  Fall 2023  Hispanic/Latino    Male       First Generation   \n",
              "3      AMX4WP4A0  Fall 2023  Hispanic/Latino  Female  Continuing Generation   \n",
              "4      R32ET2VTA  Fall 2023  Hispanic/Latino    Male  Continuing Generation   \n",
              "...          ...        ...              ...     ...                    ...   \n",
              "14359  BXXXVTVLJ  Fall 2023            White    Male       First Generation   \n",
              "14360  WPFBYCAF2  Fall 2023  Hispanic/Latino  Female       First Generation   \n",
              "14361  9DB0QFR11  Fall 2023  Hispanic/Latino  Female       First Generation   \n",
              "14362  F7Z5NC62T  Fall 2023            Asian    Male  Continuing Generation   \n",
              "14363  KDYYXRZTL  Fall 2023            White    Male  Continuing Generation   \n",
              "\n",
              "       HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  UNITS_ATTEMPTED_1  \\\n",
              "0             3.97         4.09             Business               12.0   \n",
              "1             3.67         3.77                 Arts               15.0   \n",
              "2             2.78         3.19  University Programs               13.0   \n",
              "3             4.02         4.15              Science               13.0   \n",
              "4             3.08         3.58  University Programs                6.0   \n",
              "...            ...          ...                  ...                ...   \n",
              "14359         3.98         3.84               Health               14.0   \n",
              "14360         3.79         3.61         Liberal Arts               15.0   \n",
              "14361         3.54         3.89          Engineering               14.0   \n",
              "14362         3.11         3.37          Engineering               12.0   \n",
              "14363         3.43         3.66         Liberal Arts                6.0   \n",
              "\n",
              "       UNITS_ATTEMPTED_2  UNITS_COMPLETED_1  UNITS_COMPLETED_2     GPA_1  \\\n",
              "0                    NaN                6.0                NaN  1.666667   \n",
              "1                   12.0                6.0                0.0  2.000000   \n",
              "2                    NaN               10.0                NaN  2.000000   \n",
              "3                   15.0               13.0               11.0  2.615385   \n",
              "4                    6.0                3.0                3.0  1.000000   \n",
              "...                  ...                ...                ...       ...   \n",
              "14359                NaN                0.0                NaN       NaN   \n",
              "14360                NaN                0.0                NaN  0.000000   \n",
              "14361                NaN               14.0                NaN  2.923077   \n",
              "14362                NaN                5.0                NaN  1.416667   \n",
              "14363                NaN                3.0                NaN  3.000000   \n",
              "\n",
              "          GPA_2 SEM_1_STATUS  \n",
              "0           NaN           NR  \n",
              "1      0.000000            C  \n",
              "2           NaN           NR  \n",
              "3      2.266667            C  \n",
              "4      1.500000            C  \n",
              "...         ...          ...  \n",
              "14359       NaN           NR  \n",
              "14360       NaN           NR  \n",
              "14361       NaN           NR  \n",
              "14362       NaN           NR  \n",
              "14363       NaN           NR  \n",
              "\n",
              "[5725 rows x 15 columns]"
            ]
          },
          "execution_count": 282,
          "metadata": {}
        }
      ],
      "source": [
        "predict_copy = predict.copy()\n",
        "\n",
        "predict_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f7b8f571-99bf-4a13-bcc5-7afbf8897545",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "X56KKMF1gylg"
      },
      "outputs": [],
      "source": [
        "predict_copy['DFW_RATE_1'] = (predict_copy['UNITS_ATTEMPTED_1']-predict_copy['UNITS_COMPLETED_1'])/predict_copy['UNITS_ATTEMPTED_1']\n",
        "\n",
        "#DFW Rate Term 2\n",
        "predict_copy['DFW_RATE_2'] = (predict_copy['UNITS_ATTEMPTED_2']-predict_copy['UNITS_COMPLETED_2'])/predict_copy['UNITS_ATTEMPTED_2']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ef6d22e9-0daf-4d6f-aa3c-f384f7788ac8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3_Qk9mWdgylg"
      },
      "outputs": [],
      "source": [
        "predict_copy1 = predict_copy.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d90bd060-26ec-42e5-9f54-a5eb75cf7ec8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zCMErZolgylg",
        "outputId": "9abd6677-fb7c-46a2-cc05-cb00498773f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['7JQLAYZLI', 'LXWJ36QZM', '27WYDZMY5', '361CTKCC7', 'OMX2V4SW6',\n",
              "       'NG9LGMEOP', 'K31H5XQA3', '65G2D1P8S', 'GSPDGDVZ8', 'E4YX926EY'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 285,
          "metadata": {}
        }
      ],
      "source": [
        "np.random.seed(rms); pred_sam_ind = np.random.choice(predict_copy1[(predict_copy1['GPA_1']<=3.0) & predict_copy1['GPA_2']<=3.0]['SID'], size=10, replace=False)\n",
        "pred_sam_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "97604d51-ae5f-4e8f-ac1c-dffdde748920",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ulVQMNi4gylg",
        "outputId": "b9659d76-79dc-4d8f-95d5-5963c9326301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>OMX2V4SW6</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.32</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>GSPDGDVZ8</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.93</td>\n",
              "      <td>4.29</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>K31H5XQA3</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.36</td>\n",
              "      <td>3.85</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.692308</td>\n",
              "      <td>C</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2267</th>\n",
              "      <td>27WYDZMY5</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.51</td>\n",
              "      <td>3.60</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>3.533333</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4109</th>\n",
              "      <td>LXWJ36QZM</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.96</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7888</th>\n",
              "      <td>NG9LGMEOP</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.39</td>\n",
              "      <td>4.00</td>\n",
              "      <td>Health</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8093</th>\n",
              "      <td>361CTKCC7</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.59</td>\n",
              "      <td>4.22</td>\n",
              "      <td>Health</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9492</th>\n",
              "      <td>65G2D1P8S</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.44</td>\n",
              "      <td>Education</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>E4YX926EY</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.51</td>\n",
              "      <td>3.22</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2145</th>\n",
              "      <td>7JQLAYZLI</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.92</td>\n",
              "      <td>3.07</td>\n",
              "      <td>Business</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.5000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            SID     COHORT             RACE_ETHNICITY  GENDER  \\\n",
              "210   OMX2V4SW6  Fall 2023            Hispanic/Latino  Female   \n",
              "924   GSPDGDVZ8  Fall 2023            Hispanic/Latino  Female   \n",
              "1129  K31H5XQA3  Fall 2023            Hispanic/Latino  Female   \n",
              "2267  27WYDZMY5  Fall 2023                      Asian  Female   \n",
              "4109  LXWJ36QZM  Fall 2023                      White  Female   \n",
              "7888  NG9LGMEOP  Fall 2023  Black or African American  Female   \n",
              "8093  361CTKCC7  Fall 2023            Hispanic/Latino  Female   \n",
              "9492  65G2D1P8S  Fall 2023            Hispanic/Latino  Female   \n",
              "479   E4YX926EY  Fall 2023            Hispanic/Latino    Male   \n",
              "2145  7JQLAYZLI  Fall 2023            Hispanic/Latino    Male   \n",
              "\n",
              "           FIRST_GEN_STATUS  HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  \\\n",
              "210        First Generation         4.21         4.32          Engineering   \n",
              "924                 Unknown         3.93         4.29          Engineering   \n",
              "1129  Continuing Generation         3.36         3.85  University Programs   \n",
              "2267  Continuing Generation         3.51         3.60          Engineering   \n",
              "4109  Continuing Generation         3.96         4.15                 Arts   \n",
              "7888                Unknown         3.39         4.00               Health   \n",
              "8093       First Generation         3.59         4.22               Health   \n",
              "9492       First Generation         4.32         4.44            Education   \n",
              "479        First Generation         2.51         3.22  University Programs   \n",
              "2145  Continuing Generation         2.92         3.07             Business   \n",
              "\n",
              "      UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  UNITS_COMPLETED_1  \\\n",
              "210                14.0               13.0               14.0   \n",
              "924                14.0               12.0               14.0   \n",
              "1129               15.0               16.0                9.0   \n",
              "2267               14.0               15.0               14.0   \n",
              "4109               17.0               15.0               17.0   \n",
              "7888               14.0               12.0               11.0   \n",
              "8093               12.0               15.0               12.0   \n",
              "9492               14.0               12.0               14.0   \n",
              "479                13.0               12.0               10.0   \n",
              "2145               13.0               12.0               10.0   \n",
              "\n",
              "      UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  DFW_RATE_1  \\\n",
              "210                13.0  4.000000  4.000000            C    0.000000   \n",
              "924                12.0  3.714286  4.000000            C    0.000000   \n",
              "1129               13.0  2.750000  2.692308            C    0.400000   \n",
              "2267               15.0  3.571429  3.533333            C    0.000000   \n",
              "4109               15.0  4.000000  4.000000            C    0.000000   \n",
              "7888               12.0  2.571429  3.250000            C    0.214286   \n",
              "8093               15.0  3.500000  3.600000            C    0.000000   \n",
              "9492               12.0  3.785714  4.000000            C    0.000000   \n",
              "479                12.0  1.333333  3.000000            C    0.230769   \n",
              "2145                6.0  2.500000  1.000000            C    0.230769   \n",
              "\n",
              "      DFW_RATE_2  \n",
              "210       0.0000  \n",
              "924       0.0000  \n",
              "1129      0.1875  \n",
              "2267      0.0000  \n",
              "4109      0.0000  \n",
              "7888      0.0000  \n",
              "8093      0.0000  \n",
              "9492      0.0000  \n",
              "479       0.0000  \n",
              "2145      0.5000  "
            ]
          },
          "execution_count": 286,
          "metadata": {}
        }
      ],
      "source": [
        "esam = predict_copy1[predict_copy1['SID'].isin(pred_sam_ind)].sort_values('GENDER')\n",
        "esam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "690d3605-e72f-4a54-a15c-aca529b1578f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "tGb4CUQTgylg"
      },
      "source": [
        "Let's ask AI to generate 10 random names matching the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5a059ccf-6569-4f25-9986-a1e07c31cdb1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "c1rMLlr9gylh"
      },
      "outputs": [],
      "source": [
        "esam['NAME'] = [\"Sophia\", \"Emma\", \"Olivia\", \"Ava\", \"Isabella\", \"Maya\", \"Liz\", \"Noelle\", \"Elijah\", \"James\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "865fdbfd-3e64-4968-b414-5b14a2fee29a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CIBq5uU8gylh",
        "outputId": "08028bde-ba67-4b6e-b597-c38a12792b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>OMX2V4SW6</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.32</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Sophia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>GSPDGDVZ8</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.93</td>\n",
              "      <td>4.29</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Emma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>K31H5XQA3</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.36</td>\n",
              "      <td>3.85</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.692308</td>\n",
              "      <td>C</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>Olivia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2267</th>\n",
              "      <td>27WYDZMY5</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.51</td>\n",
              "      <td>3.60</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>3.533333</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Ava</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4109</th>\n",
              "      <td>LXWJ36QZM</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.96</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Isabella</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7888</th>\n",
              "      <td>NG9LGMEOP</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.39</td>\n",
              "      <td>4.00</td>\n",
              "      <td>Health</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Maya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8093</th>\n",
              "      <td>361CTKCC7</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.59</td>\n",
              "      <td>4.22</td>\n",
              "      <td>Health</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Liz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9492</th>\n",
              "      <td>65G2D1P8S</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.44</td>\n",
              "      <td>Education</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Noelle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>E4YX926EY</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.51</td>\n",
              "      <td>3.22</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Elijah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2145</th>\n",
              "      <td>7JQLAYZLI</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.92</td>\n",
              "      <td>3.07</td>\n",
              "      <td>Business</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>James</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            SID     COHORT             RACE_ETHNICITY  GENDER  \\\n",
              "210   OMX2V4SW6  Fall 2023            Hispanic/Latino  Female   \n",
              "924   GSPDGDVZ8  Fall 2023            Hispanic/Latino  Female   \n",
              "1129  K31H5XQA3  Fall 2023            Hispanic/Latino  Female   \n",
              "2267  27WYDZMY5  Fall 2023                      Asian  Female   \n",
              "4109  LXWJ36QZM  Fall 2023                      White  Female   \n",
              "7888  NG9LGMEOP  Fall 2023  Black or African American  Female   \n",
              "8093  361CTKCC7  Fall 2023            Hispanic/Latino  Female   \n",
              "9492  65G2D1P8S  Fall 2023            Hispanic/Latino  Female   \n",
              "479   E4YX926EY  Fall 2023            Hispanic/Latino    Male   \n",
              "2145  7JQLAYZLI  Fall 2023            Hispanic/Latino    Male   \n",
              "\n",
              "           FIRST_GEN_STATUS  HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  \\\n",
              "210        First Generation         4.21         4.32          Engineering   \n",
              "924                 Unknown         3.93         4.29          Engineering   \n",
              "1129  Continuing Generation         3.36         3.85  University Programs   \n",
              "2267  Continuing Generation         3.51         3.60          Engineering   \n",
              "4109  Continuing Generation         3.96         4.15                 Arts   \n",
              "7888                Unknown         3.39         4.00               Health   \n",
              "8093       First Generation         3.59         4.22               Health   \n",
              "9492       First Generation         4.32         4.44            Education   \n",
              "479        First Generation         2.51         3.22  University Programs   \n",
              "2145  Continuing Generation         2.92         3.07             Business   \n",
              "\n",
              "      UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  UNITS_COMPLETED_1  \\\n",
              "210                14.0               13.0               14.0   \n",
              "924                14.0               12.0               14.0   \n",
              "1129               15.0               16.0                9.0   \n",
              "2267               14.0               15.0               14.0   \n",
              "4109               17.0               15.0               17.0   \n",
              "7888               14.0               12.0               11.0   \n",
              "8093               12.0               15.0               12.0   \n",
              "9492               14.0               12.0               14.0   \n",
              "479                13.0               12.0               10.0   \n",
              "2145               13.0               12.0               10.0   \n",
              "\n",
              "      UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  DFW_RATE_1  \\\n",
              "210                13.0  4.000000  4.000000            C    0.000000   \n",
              "924                12.0  3.714286  4.000000            C    0.000000   \n",
              "1129               13.0  2.750000  2.692308            C    0.400000   \n",
              "2267               15.0  3.571429  3.533333            C    0.000000   \n",
              "4109               15.0  4.000000  4.000000            C    0.000000   \n",
              "7888               12.0  2.571429  3.250000            C    0.214286   \n",
              "8093               15.0  3.500000  3.600000            C    0.000000   \n",
              "9492               12.0  3.785714  4.000000            C    0.000000   \n",
              "479                12.0  1.333333  3.000000            C    0.230769   \n",
              "2145                6.0  2.500000  1.000000            C    0.230769   \n",
              "\n",
              "      DFW_RATE_2      NAME  \n",
              "210       0.0000    Sophia  \n",
              "924       0.0000      Emma  \n",
              "1129      0.1875    Olivia  \n",
              "2267      0.0000       Ava  \n",
              "4109      0.0000  Isabella  \n",
              "7888      0.0000      Maya  \n",
              "8093      0.0000       Liz  \n",
              "9492      0.0000    Noelle  \n",
              "479       0.0000    Elijah  \n",
              "2145      0.5000     James  "
            ]
          },
          "execution_count": 288,
          "metadata": {}
        }
      ],
      "source": [
        "esam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f9a1e71c-532e-49e4-b33b-f51d7a1f96e1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "PBdO39VQgylh"
      },
      "source": [
        "Shontelle is happy with the sample, as a quick glance indicates ethnic and academic diversity, a reflection of her university. She notices that the data indicates that Marisol, Alex and Ana are currently in academc warning due to their CUM_GPA_2 being less than 2.0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "94b56879-8e9f-4946-bdca-5ee8e1946b94",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "uc23pV91gylh",
        "outputId": "6da5ec32-5591-4fe7-bb19-f35c6a0c8b17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479     Elijah\n",
              "2145     James\n",
              "Name: NAME, dtype: object"
            ]
          },
          "execution_count": 289,
          "metadata": {}
        }
      ],
      "source": [
        "esam['NAME'][(esam['GPA_1']<2.0) | (esam['GPA_2']<2.0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "350d9393-db18-45bf-aca1-89d021d01427",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_ZAyP_6Pgylh"
      },
      "source": [
        "As academic disqualification is not completely determined until the end of term 3, she aims to use our validated model to predict status at the beginning of semester 3.\n",
        "\n",
        "Based on our approach above she realizes she has some feature engineering to do, and so she then calculates DFW rates and grade points for each term:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0cc4e2c8-5132-4403-b0cf-0cda6ab2a2a3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "at0UKXz6gylh"
      },
      "source": [
        "To prepare her dataframe for the pipelines used in this notebook, she removes variables not included in the modeling process, and verifies that they match our work above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "249e8073-2780-4ab4-b76d-fa8ac1d23368",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "SkXtcfDYgylh",
        "outputId": "9e0e3859-3924-494f-eb0a-fa0dce291510"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>4.32</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>4.29</td>\n",
              "      <td>3.93</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>3.85</td>\n",
              "      <td>3.36</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.692308</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2267</th>\n",
              "      <td>3.60</td>\n",
              "      <td>3.51</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.533333</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4109</th>\n",
              "      <td>4.15</td>\n",
              "      <td>3.96</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7888</th>\n",
              "      <td>4.00</td>\n",
              "      <td>3.39</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8093</th>\n",
              "      <td>4.22</td>\n",
              "      <td>3.59</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9492</th>\n",
              "      <td>4.44</td>\n",
              "      <td>4.32</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>3.22</td>\n",
              "      <td>2.51</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2145</th>\n",
              "      <td>3.07</td>\n",
              "      <td>2.92</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      HS_ENGL_GPA  HS_MATH_GPA     GPA_1  UNITS_ATTEMPTED_1  DFW_RATE_1  \\\n",
              "210          4.32         4.21  4.000000               14.0    0.000000   \n",
              "924          4.29         3.93  3.714286               14.0    0.000000   \n",
              "1129         3.85         3.36  2.750000               15.0    0.400000   \n",
              "2267         3.60         3.51  3.571429               14.0    0.000000   \n",
              "4109         4.15         3.96  4.000000               17.0    0.000000   \n",
              "7888         4.00         3.39  2.571429               14.0    0.214286   \n",
              "8093         4.22         3.59  3.500000               12.0    0.000000   \n",
              "9492         4.44         4.32  3.785714               14.0    0.000000   \n",
              "479          3.22         2.51  1.333333               13.0    0.230769   \n",
              "2145         3.07         2.92  2.500000               13.0    0.230769   \n",
              "\n",
              "      UNITS_ATTEMPTED_2     GPA_2  DFW_RATE_2             RACE_ETHNICITY  \\\n",
              "210                13.0  4.000000      0.0000            Hispanic/Latino   \n",
              "924                12.0  4.000000      0.0000            Hispanic/Latino   \n",
              "1129               16.0  2.692308      0.1875            Hispanic/Latino   \n",
              "2267               15.0  3.533333      0.0000                      Asian   \n",
              "4109               15.0  4.000000      0.0000                      White   \n",
              "7888               12.0  3.250000      0.0000  Black or African American   \n",
              "8093               15.0  3.600000      0.0000            Hispanic/Latino   \n",
              "9492               12.0  4.000000      0.0000            Hispanic/Latino   \n",
              "479                12.0  3.000000      0.0000            Hispanic/Latino   \n",
              "2145               12.0  1.000000      0.5000            Hispanic/Latino   \n",
              "\n",
              "      GENDER       FIRST_GEN_STATUS  \n",
              "210   Female       First Generation  \n",
              "924   Female                Unknown  \n",
              "1129  Female  Continuing Generation  \n",
              "2267  Female  Continuing Generation  \n",
              "4109  Female  Continuing Generation  \n",
              "7888  Female                Unknown  \n",
              "8093  Female       First Generation  \n",
              "9492  Female       First Generation  \n",
              "479     Male       First Generation  \n",
              "2145    Male  Continuing Generation  "
            ]
          },
          "execution_count": 290,
          "metadata": {}
        }
      ],
      "source": [
        "esam2 = esam[selected_columns]\n",
        "esam2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "28a43cf0-c23e-4ef8-ad52-6e033ed9fdcc",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "KnF2YafHgyli"
      },
      "source": [
        "Next she runs her risk data through the feature engineering pipeline for qualitative and quantitative variables. Finally she uses our final, \"best\" model validated and selected above to predict probability of departure for each student, and uses a 0.72 threshold to classify 0's and 1's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5ba65532-ccfe-4d5d-8e2b-85fec1edc238",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zfmkMU2fgyli",
        "outputId": "4fa1d983-e35f-4221-9b02-d996e908fcc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SID</th>\n",
              "      <th>COHORT</th>\n",
              "      <th>RACE_ETHNICITY</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>FIRST_GEN_STATUS</th>\n",
              "      <th>HS_MATH_GPA</th>\n",
              "      <th>HS_ENGL_GPA</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>UNITS_ATTEMPTED_1</th>\n",
              "      <th>UNITS_ATTEMPTED_2</th>\n",
              "      <th>UNITS_COMPLETED_1</th>\n",
              "      <th>UNITS_COMPLETED_2</th>\n",
              "      <th>GPA_1</th>\n",
              "      <th>GPA_2</th>\n",
              "      <th>SEM_1_STATUS</th>\n",
              "      <th>DFW_RATE_1</th>\n",
              "      <th>DFW_RATE_2</th>\n",
              "      <th>NAME</th>\n",
              "      <th>Pred_SEM_3_STATUS_prob</th>\n",
              "      <th>Pred_SEM_3_STATUS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>OMX2V4SW6</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.21</td>\n",
              "      <td>4.32</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Sophia</td>\n",
              "      <td>0.021535</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9492</th>\n",
              "      <td>65G2D1P8S</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.44</td>\n",
              "      <td>Education</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.785714</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Noelle</td>\n",
              "      <td>0.023018</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>GSPDGDVZ8</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.93</td>\n",
              "      <td>4.29</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Emma</td>\n",
              "      <td>0.024273</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8093</th>\n",
              "      <td>361CTKCC7</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>3.59</td>\n",
              "      <td>4.22</td>\n",
              "      <td>Health</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>3.600000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Liz</td>\n",
              "      <td>0.026358</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4109</th>\n",
              "      <td>LXWJ36QZM</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.96</td>\n",
              "      <td>4.15</td>\n",
              "      <td>Arts</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Isabella</td>\n",
              "      <td>0.026854</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2267</th>\n",
              "      <td>27WYDZMY5</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Asian</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.51</td>\n",
              "      <td>3.60</td>\n",
              "      <td>Engineering</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>3.533333</td>\n",
              "      <td>C</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Ava</td>\n",
              "      <td>0.027613</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7888</th>\n",
              "      <td>NG9LGMEOP</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Black or African American</td>\n",
              "      <td>Female</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>3.39</td>\n",
              "      <td>4.00</td>\n",
              "      <td>Health</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.571429</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Maya</td>\n",
              "      <td>0.070542</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>E4YX926EY</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>First Generation</td>\n",
              "      <td>2.51</td>\n",
              "      <td>3.22</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Elijah</td>\n",
              "      <td>0.075567</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>K31H5XQA3</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Female</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>3.36</td>\n",
              "      <td>3.85</td>\n",
              "      <td>University Programs</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.750000</td>\n",
              "      <td>2.692308</td>\n",
              "      <td>C</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>Olivia</td>\n",
              "      <td>0.126443</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2145</th>\n",
              "      <td>7JQLAYZLI</td>\n",
              "      <td>Fall 2023</td>\n",
              "      <td>Hispanic/Latino</td>\n",
              "      <td>Male</td>\n",
              "      <td>Continuing Generation</td>\n",
              "      <td>2.92</td>\n",
              "      <td>3.07</td>\n",
              "      <td>Business</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>C</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>James</td>\n",
              "      <td>0.202741</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            SID     COHORT             RACE_ETHNICITY  GENDER  \\\n",
              "210   OMX2V4SW6  Fall 2023            Hispanic/Latino  Female   \n",
              "9492  65G2D1P8S  Fall 2023            Hispanic/Latino  Female   \n",
              "924   GSPDGDVZ8  Fall 2023            Hispanic/Latino  Female   \n",
              "8093  361CTKCC7  Fall 2023            Hispanic/Latino  Female   \n",
              "4109  LXWJ36QZM  Fall 2023                      White  Female   \n",
              "2267  27WYDZMY5  Fall 2023                      Asian  Female   \n",
              "7888  NG9LGMEOP  Fall 2023  Black or African American  Female   \n",
              "479   E4YX926EY  Fall 2023            Hispanic/Latino    Male   \n",
              "1129  K31H5XQA3  Fall 2023            Hispanic/Latino  Female   \n",
              "2145  7JQLAYZLI  Fall 2023            Hispanic/Latino    Male   \n",
              "\n",
              "           FIRST_GEN_STATUS  HS_MATH_GPA  HS_ENGL_GPA              COLLEGE  \\\n",
              "210        First Generation         4.21         4.32          Engineering   \n",
              "9492       First Generation         4.32         4.44            Education   \n",
              "924                 Unknown         3.93         4.29          Engineering   \n",
              "8093       First Generation         3.59         4.22               Health   \n",
              "4109  Continuing Generation         3.96         4.15                 Arts   \n",
              "2267  Continuing Generation         3.51         3.60          Engineering   \n",
              "7888                Unknown         3.39         4.00               Health   \n",
              "479        First Generation         2.51         3.22  University Programs   \n",
              "1129  Continuing Generation         3.36         3.85  University Programs   \n",
              "2145  Continuing Generation         2.92         3.07             Business   \n",
              "\n",
              "      UNITS_ATTEMPTED_1  UNITS_ATTEMPTED_2  UNITS_COMPLETED_1  \\\n",
              "210                14.0               13.0               14.0   \n",
              "9492               14.0               12.0               14.0   \n",
              "924                14.0               12.0               14.0   \n",
              "8093               12.0               15.0               12.0   \n",
              "4109               17.0               15.0               17.0   \n",
              "2267               14.0               15.0               14.0   \n",
              "7888               14.0               12.0               11.0   \n",
              "479                13.0               12.0               10.0   \n",
              "1129               15.0               16.0                9.0   \n",
              "2145               13.0               12.0               10.0   \n",
              "\n",
              "      UNITS_COMPLETED_2     GPA_1     GPA_2 SEM_1_STATUS  DFW_RATE_1  \\\n",
              "210                13.0  4.000000  4.000000            C    0.000000   \n",
              "9492               12.0  3.785714  4.000000            C    0.000000   \n",
              "924                12.0  3.714286  4.000000            C    0.000000   \n",
              "8093               15.0  3.500000  3.600000            C    0.000000   \n",
              "4109               15.0  4.000000  4.000000            C    0.000000   \n",
              "2267               15.0  3.571429  3.533333            C    0.000000   \n",
              "7888               12.0  2.571429  3.250000            C    0.214286   \n",
              "479                12.0  1.333333  3.000000            C    0.230769   \n",
              "1129               13.0  2.750000  2.692308            C    0.400000   \n",
              "2145                6.0  2.500000  1.000000            C    0.230769   \n",
              "\n",
              "      DFW_RATE_2      NAME  Pred_SEM_3_STATUS_prob  Pred_SEM_3_STATUS  \n",
              "210       0.0000    Sophia                0.021535                  0  \n",
              "9492      0.0000    Noelle                0.023018                  0  \n",
              "924       0.0000      Emma                0.024273                  0  \n",
              "8093      0.0000       Liz                0.026358                  0  \n",
              "4109      0.0000  Isabella                0.026854                  0  \n",
              "2267      0.0000       Ava                0.027613                  0  \n",
              "7888      0.0000      Maya                0.070542                  0  \n",
              "479       0.0000    Elijah                0.075567                  0  \n",
              "1129      0.1875    Olivia                0.126443                  0  \n",
              "2145      0.5000     James                0.202741                  1  "
            ]
          },
          "execution_count": 291,
          "metadata": {}
        }
      ],
      "source": [
        "#Initialize the new dataframe\n",
        "esam1 = esam.copy()\n",
        "\n",
        "#\n",
        "#esam3 = pd.DataFrame(preprocessor.fit_transform(esam2),columns=all_column_names)\n",
        "#esam4 = esam3[selected_features]\n",
        "\n",
        "# Get predicted probabilities\n",
        "esam1['Pred_SEM_3_STATUS_prob'] = pipeline_lrc1_train.predict_proba(esam1)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "threshold = 0.13\n",
        "esam1['Pred_SEM_3_STATUS'] = (esam1['Pred_SEM_3_STATUS_prob']>=threshold).astype(int)\n",
        "esam1.sort_values(by='Pred_SEM_3_STATUS_prob')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4aaf43be-dea6-4e9b-8747-96d00d4ff846",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "3drSPWuxgyli"
      },
      "source": [
        "Note here that we are using the model fit using the training set. This means the test data is left on the table for prediction. In theory, in the same way we combined the build and validate sets when validation was completed and it was time for testing, we can at this point combine the training and test sets now that it is time for prediction. This will be left as an exercise for you to perform, and compare model predictions with those generated by the present final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1638a2f4-736f-4ba5-9aa7-177b3d112b49",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "t6NVdcS9gyli"
      },
      "source": [
        "###Addressing the Higher Ed Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "417217d1-1d0e-4aa2-a35e-85239b1b85de",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "oplKX2xugyli"
      },
      "source": [
        "The model predicts Olivia and James to be in danger of leaving the university before the start of the next semester. Given that our test precision is about 34%, Shontelle understands that not all students predicted to be 1 will actually depart; and not all students predicted to stay will. So she utilizes the predictions as a starting point, synthesizes it with her findings from the focus group and literature, and identifies further that James and Olivia as at particular risk for dropping out. She arranges time to meet with them to ensure that they have the academic and socio-emotional support they need.\n",
        "\n",
        "Note that this model was \"smarter\" than the logistic model when it comes to Elijah. It did not classify him as at-risk for dropout in term 3. He made significant improvement from term 1 to term 2, and s encouraged to continue this positive trajectory.\n",
        "\n",
        "Finally, we see that the model predicts a 8/10 = 80% *retention rate* for this group of students. Thus predictions made for individual students have implications for selected subgroups and cohorts. This gives Shontelle the information she needs to recommend broad action and report wide reaching predictive aggregate metrics to stakehlders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9c9ca485-79ca-409c-add4-7cfa198bd6b0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "0izYXLlPgyli"
      },
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "4.4. Your Second Scikit-Learn Classification Model: Predict Dropout at 3rd Semester using L1-L2 Penalty (Elastic Net)",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}